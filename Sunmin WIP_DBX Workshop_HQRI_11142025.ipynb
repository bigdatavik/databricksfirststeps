{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676565f7-bc51-4b21-8fba-603008447446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Updated date: November 11, 2025_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks Workshop: HQRI Risk Adjustment & Analytics\n",
    "**Healthcare Quality Reporting & Improvement - DBSQL Analytics Workshop**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Build **Medallion Architecture** pipelines for risk adjustment data\n",
    "2. ‚úÖ Calculate **HCC (Hierarchical Condition Category) risk scores** for Medicare Advantage\n",
    "3. ‚úÖ Create **encounter datamart** tables for CMS submissions\n",
    "4. ‚úÖ Perform **data quality audits** for regulatory compliance\n",
    "5. ‚úÖ Build **Gold layer analytics** for revenue impact and Star Ratings\n",
    "6. ‚úÖ Forecast risk scores using **Databricks SQL (DBSQL)**\n",
    "\n",
    "---\n",
    "\n",
    "## üè• HQRI Use Case: Medicare Risk Adjustment\n",
    "\n",
    "**HQRI (Healthcare Quality Reporting & Improvement)** manages Humana's risk adjustment and quality reporting for Medicare Advantage:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Determine CMS payments based on member health status\n",
    "- üìä **Encounter Datamart**: Aggregate and validate claims/encounters for CMS\n",
    "- ‚≠ê **Star Ratings**: Drive quality metrics that impact bonus payments\n",
    "- ‚úÖ **Compliance & Audits**: Ensure federal regulatory compliance\n",
    "- üìà **Revenue Optimization**: Accurate coding and forecasting maximize financial performance\n",
    "\n",
    "### üóÇÔ∏è Dataset Overview\n",
    "\n",
    "We'll work with **Medicare risk adjustment data**:\n",
    "- **Members**: Medicare Advantage enrollees with demographics\n",
    "- **Claims**: Medical encounters with diagnosis codes\n",
    "- **Diagnoses**: ICD-10 codes mapped to HCC categories\n",
    "- **Providers**: Healthcare providers submitting encounters\n",
    "- **Procedures**: Services rendered and billed\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is a lakehouse?\n",
    "\n",
    "1. **Hybrid Architecture:**  \n",
    "   A lakehouse combines the best of data lakes (flexible, cheap storage) and data warehouses (structured, fast analytics), providing transactional and governance features on top of open cloud storage.\n",
    "\n",
    "2. **ACID Transactions and Schema Governance:**  \n",
    "   Lakehouses support ACID transactions for consistent concurrent data access and enforce schema management, which is essential for data integrity and compliance.\n",
    "\n",
    "3. **Open and Decoupled:**  \n",
    "   They use open file formats (like Parquet), decouple compute from storage for flexible scalability, and allow access by a variety of analytics, BI, and machine learning tools.\n",
    "\n",
    "4. **Supports All Workloads and Data Types:**  \n",
    "   The architecture enables SQL analytics, data science, machine learning, and can handle structured, semi-structured, and unstructured data (including images, text, video).\n",
    "\n",
    "5. **Single Platform, Enterprise Ready:**  \n",
    "   With features like real-time streaming, end-to-end governance, access control, and data discovery tools, lakehouses reduce complexity‚Äîallowing enterprises to manage all data and analytics needs in one unified system.\n",
    "![](https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog\n",
    "\n",
    "[Unified and open governance for data and AI in the Lakehouse](https://www.databricks.com/product/unity-catalog#features)\n",
    "\n",
    "Eliminate silos, simplify governance and accelerate insights at scale:\n",
    "\n",
    "- Centralizes governance, access control, auditing, and data discovery for all data and AI assets across Databricks workspaces.\n",
    "- Enables fine-grained, consistent data access policies (row- and column-level), defined once and applied everywhere.\n",
    "- Provides comprehensive data lineage and audit logs, showing how and by whom data is accessed and transformed.\n",
    "- Supports data discovery, tagging, and documentation, making it easier to find and understand datasets and models.\n",
    "- Works across multiple clouds and supports open formats (Delta, Parquet, etc.), avoiding vendor lock-in and enabling broad interoperability.\n",
    "- Allows secure data and AI sharing within and outside the organization, including clean rooms and partner collaborations.\n",
    "- Provides built-in monitoring for data quality, freshness, and usage, helping ensure compliance and rapid troubleshooting.\n",
    "- Integrates tightly with the catalog/schema/object model, enhancing organization and security for all managed data assets.\n",
    "\n",
    "![](https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086)\n",
    "\n",
    "[Unity Catalog Search & Data Explorer](https://app.getreprise.com/launch/96mpAqy/)\n",
    "\n",
    "[Exploring Lineage and Governance with Unity Catalog](https://app.getreprise.com/launch/MnqjQDX/)\n",
    "\n",
    "[A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2024-08/comprehensive-guide-to-data-and-ai-governance.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Medallion lakehouse architecture\n",
    "\n",
    "In this example, we will be following the **medallion lakehouse architecture**. The medallion architecture is a data design pattern to organize data in a lakehouse. The goal is to progressively improve the quality and structure of the data as it flows through each layer (Bronze [**raw**] ‚Üí Silver [**staging**] ‚Üí Gold [**main**]).\n",
    "\n",
    "1. **Bronze layer**: the raw, unvalidated data\n",
    "2. **Silver**: cleansed and conformed data\n",
    "3. **Gold**: curated business-level tables\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8292153-2b0a-47dd-b6c6-b1cf4c45cdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managed tables\n",
    "\n",
    "[How Unity Catalog Managed Tables Automate Performance at Scale](https://www.databricks.com/blog/how-unity-catalog-managed-tables-automate-performance-scale) with [Predictive Optimization](https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization)\n",
    "\n",
    "\n",
    "<!-- ![](https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384) -->\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9115bf6-333a-4cb8-965c-5e73a2d204d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Faster Queries: 20X query latency reduction](https://www.databricks.com/blog/predictive-optimization-automatically-delivers-faster-queries-and-lower-tco)\n",
    "\n",
    "**Predictive Optimization** in Databricks automates table management by leveraging Unity Catalog and the Data Intelligence Platform. This innovative feature currently runs the following optimizations for Unity Catalog managed tables:\n",
    "\n",
    "* **OPTIMIZE** - Triggers incremental clustering for enabled tables. Improves query performance by optimizing file sizes.\n",
    "* **VACUUM** - Reduces storage costs by deleting data files no longer referenced by the table.\n",
    "* **ANALYZE** - Triggers incremental update of statistics to improve query performance. \n",
    "\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/styles/max_1000x1000/public/2024-05/db-976-blog-img-og.png?itok=qWBT8VA-&v=1717158571\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n",
    "\n",
    "**Compaction** - This enhances query performance by optimizing file sizes, ensuring that data retrieval is efficient.\n",
    "\n",
    "**Liquid Clustering** - This technique incrementally clusters incoming data, enabling optimal data layout and efficient data skipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce4ebf7-dc51-44bc-8bf2-804fec60b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Medallion Pipeline for a Healthcare Payer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Risk Adjustment & HCC Modeling Concepts\n",
    "\n",
    "### üè• Medicare Risk Adjustment Overview\n",
    "\n",
    "CMS uses **HCC (Hierarchical Condition Category)** models to adjust capitation payments based on beneficiary health status:\n",
    "\n",
    "1. **ICD-10 Diagnosis Codes** ‚Üí mapped to **HCC categories**\n",
    "2. **HCC categories** ‚Üí assigned **coefficient weights**\n",
    "3. **Risk Score** = Sum of HCC weights + demographic factors (age, sex, disability status)\n",
    "4. **Payment** = Base rate √ó Risk Score\n",
    "\n",
    "**Example:**\n",
    "- Member with diabetes (HCC 19, weight 0.318) and heart failure (HCC 85, weight 0.368)\n",
    "- Base demographic score: 0.500\n",
    "- **Total Risk Score = 0.500 + 0.318 + 0.368 = 1.186**\n",
    "- If base payment = $10,000, **CMS pays: $10,000 √ó 1.186 = $11,860**\n",
    "\n",
    "### ‚≠ê CMS Star Ratings Impact\n",
    "\n",
    "Star Ratings (1-5 stars) impact bonus payments:\n",
    "- **5 stars**: Up to 5% bonus payment\n",
    "- **4+ stars**: Quality Bonus Payments (QBP)\n",
    "- Ratings based on quality measures (clinical outcomes, patient experience, access)\n",
    "\n",
    "### üìä HQRI Data Model\n",
    "\n",
    "For risk adjustment, key tables include:\n",
    "- **Encounters/Claims**: Medical services with diagnosis codes\n",
    "- **Diagnosis-to-HCC Mapping**: ICD-10 ‚Üí HCC crosswalk\n",
    "- **HCC Coefficients**: CMS model weights by year\n",
    "- **Members**: Demographics and enrollment status\n",
    "- **Risk Scores**: Calculated member-level risk scores\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Dimensional Model\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Data Vault\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "**Resources:**\n",
    "- [CMS Risk Adjustment Model](https://www.cms.gov/medicare/payment/medicare-advantage-rates-statistics/risk-adjustment)\n",
    "- [Implementing Dimensional Modeling on Databricks](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP\n",
    "\n",
    "Just run next couple of cells for setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"sunmin_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\"\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")\n",
    "\n",
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n",
    "\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze/Silver Layers ‚Äì Streamlined Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Overview: Simplified Bronze & Silver\n",
    "\n",
    "For HQRI analytics, we'll **streamline** Bronze and Silver layers to quickly get to Gold layer insights:\n",
    "\n",
    "### Bronze Layer (Raw Data Landing)\n",
    "- üìÇ Ingest encounter data \"as-is\" using `COPY INTO`\n",
    "- üíæ Store in Delta Lake for audit trails\n",
    "- ‚è±Ô∏è Maintain full history for compliance\n",
    "\n",
    "### Silver Layer (Clean & Validate)\n",
    "- üßπ Remove duplicates and validate data quality\n",
    "- üîÑ Map ICD-10 codes to HCC categories\n",
    "- ‚úÖ Apply business rules for CMS submission eligibility\n",
    "\n",
    "> **üí° Focus**: We'll execute Bronze/Silver steps efficiently so we can spend more time on **Gold layer analytics** that drive business value!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/sunmin_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load Claims Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files.\n",
    "\n",
    "\n",
    "-- Load Diagnosis Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Members Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Procedures Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Providers Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/sunmin_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer ‚Äì Transforms for Risk Adjustment\n",
    "\n",
    "Now let's clean the data and prepare it for HCC risk scoring:\n",
    "\n",
    "**Key Silver Transformations:**\n",
    "- ‚úÖ Deduplicate encounters\n",
    "- ‚úÖ Validate diagnosis codes (ICD-10 format)\n",
    "- ‚úÖ Filter to eligible members (Medicare Advantage, active enrollment)\n",
    "- ‚úÖ Map diagnoses to HCC categories\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Speed Focus**: We'll run these transforms to move to Gold layer analytics!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14bb21bd-564b-4bd0-a6f8-c956f766924f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf77187a-4958-4353-87d2-544486a45ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üéØ YOUR TURN! (3 mins)\n",
    "Ask Databricks Assistant: \"How do I calculate the total claims by specialty in SQL?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068e2f90-20d7-434b-bdf2-9d290f735df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    claim_status AS specialty,\n",
    "    SUM(total_charge) AS total_claims\n",
    "FROM my_catalog.payer_silver.claims\n",
    "GROUP BY claim_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì HQRI Risk Adjustment Analytics\n",
    "\n",
    "---\n",
    "\n",
    "## What is the HQRI Gold Layer?\n",
    "\n",
    "The **Gold Layer** is where we deliver **business value** for HQRI. Here we create analytics tables that directly support:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Member-level HCC risk scores for CMS payments\n",
    "- üìä **Encounter Datamart**: CMS submission-ready data with quality validations\n",
    "- ‚≠ê **Star Ratings**: Quality metrics that impact bonus payments\n",
    "- üìà **Revenue Forecasting**: Projected payments based on risk scores\n",
    "- ‚úÖ **Compliance Audits**: Data quality metrics for regulatory requirements\n",
    "- üë• **Member Stratification**: High-risk vs. low-risk population segmentation\n",
    "- üè• **Provider Performance**: Risk capture rates by provider\n",
    "\n",
    "> **üéØ HQRI Value**: Each Gold table directly answers a business question that impacts revenue, compliance, or quality!\n",
    "\n",
    "---\n",
    "\n",
    "## Gold Tables We'll Build\n",
    "\n",
    "| Table | Business Question |\n",
    "|-------|-------------------|\n",
    "| **member_risk_scores** | What is each member's HCC risk score and expected CMS payment? |\n",
    "| **encounter_datamart** | Is encounter data complete and valid for CMS submission? |\n",
    "| **revenue_forecast** | What is our projected revenue by plan, region, and risk level? |\n",
    "| **hcc_distribution** | Which HCC categories drive the most revenue? |\n",
    "| **data_quality_audit** | Are we meeting compliance standards for encounter completeness? |\n",
    "| **member_risk_stratification** | Who are our high-risk members requiring care management? |\n",
    "| **provider_risk_capture** | Which providers excel at risk documentation? |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b0153-df99-4566-982a-745e97ff0888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: HCC Risk Score Calculation\n",
    "\n",
    "### üéØ Business Goal\n",
    "Calculate member-level risk scores based on HCC categories to determine CMS payments.\n",
    "\n",
    "**Risk Score Formula:**\n",
    "```\n",
    "Risk Score = Demographic Score + Œ£(HCC Coefficients)\n",
    "```\n",
    "\n",
    "Let's build this step-by-step using **Databricks SQL**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da30fc9-0891-4c8a-8bbd-aae8687f1372",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Enriched Claims, Members, and Providers Summary Tables"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create gold schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_gold;\n",
    "\n",
    "-- Step 1a: Create HCC Mapping Reference Table (simulated for demo)\n",
    "CREATE OR REPLACE TABLE payer_gold.hcc_reference (\n",
    "  icd10_code STRING,\n",
    "  diagnosis_desc STRING,\n",
    "  hcc_category INT,\n",
    "  hcc_coefficient DOUBLE\n",
    ");\n",
    "\n",
    "INSERT INTO payer_gold.hcc_reference VALUES\n",
    "('E11.9', 'Type 2 Diabetes', 19, 0.318),\n",
    "('I50.9', 'Heart Failure', 85, 0.368),\n",
    "('I10', 'Hypertension', 0, 0.000),\n",
    "('J44.9', 'COPD', 111, 0.328),\n",
    "('N18.3', 'CKD Stage 3', 138, 0.237),\n",
    "('F32.9', 'Depression', 59, 0.309),\n",
    "('E78.5', 'Hyperlipidemia', 0, 0.000),\n",
    "('I25.10', 'CAD', 88, 0.184);\n",
    "\n",
    "-- Step 1b: Join Diagnoses to HCC Categories\n",
    "CREATE OR REPLACE TABLE payer_gold.diagnosis_with_hcc AS\n",
    "SELECT\n",
    "  d.claim_id,\n",
    "  d.diagnosis_code,\n",
    "  d.diagnosis_desc,\n",
    "  h.hcc_category,\n",
    "  h.hcc_coefficient,\n",
    "  CASE WHEN h.hcc_category IS NOT NULL AND h.hcc_category > 0 THEN 1 ELSE 0 END as is_hcc\n",
    "FROM payer_bronze.diagnosis_raw d\n",
    "LEFT JOIN payer_gold.hcc_reference h \n",
    "  ON UPPER(TRIM(d.diagnosis_code)) = UPPER(TRIM(h.icd10_code));\n",
    "\n",
    "-- Step 1c: Calculate Member-Level HCC Risk Scores\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_scores AS\n",
    "WITH member_hccs AS (\n",
    "  SELECT DISTINCT\n",
    "    c.member_id,\n",
    "    m.first_name,\n",
    "    m.last_name,\n",
    "    m.birth_date,\n",
    "    m.gender,\n",
    "    m.plan_id,\n",
    "    dh.hcc_category,\n",
    "    dh.hcc_coefficient,\n",
    "    dh.diagnosis_code,\n",
    "    YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as age\n",
    "  FROM payer_silver.claims c\n",
    "  INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "  INNER JOIN payer_gold.diagnosis_with_hcc dh ON c.claim_id = dh.claim_id\n",
    "  WHERE dh.is_hcc = 1\n",
    "),\n",
    "member_scores AS (\n",
    "  SELECT\n",
    "    member_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    birth_date,\n",
    "    gender,\n",
    "    plan_id,\n",
    "    age,\n",
    "    CASE \n",
    "      WHEN age < 65 THEN 0.350\n",
    "      WHEN age BETWEEN 65 AND 69 THEN 0.450\n",
    "      WHEN age BETWEEN 70 AND 74 THEN 0.550\n",
    "      WHEN age BETWEEN 75 AND 79 THEN 0.650\n",
    "      ELSE 0.750\n",
    "    END as demographic_score,\n",
    "    SUM(hcc_coefficient) as hcc_score,\n",
    "    COUNT(DISTINCT hcc_category) as hcc_count,\n",
    "    COLLECT_SET(hcc_category) as hcc_categories,\n",
    "    COLLECT_SET(diagnosis_code) as diagnosis_codes\n",
    "  FROM member_hccs\n",
    "  GROUP BY member_id, first_name, last_name, birth_date, gender, plan_id, age\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  demographic_score + hcc_score as total_risk_score,\n",
    "  ROUND((demographic_score + hcc_score) * 10000, 2) as projected_annual_payment\n",
    "FROM member_scores\n",
    "ORDER BY total_risk_score DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76565f6d-ea54-4a63-a566-c9d4b5c13b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä View Risk Score Results\n",
    "\n",
    "Let's see the calculated risk scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4271f19b-4ce9-401e-adff-4e6590e98cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View top 20 members by risk score\n",
    "SELECT \n",
    "  member_id,\n",
    "  CONCAT(first_name, ' ', last_name) as member_name,\n",
    "  age,\n",
    "  gender,\n",
    "  hcc_count,\n",
    "  ROUND(demographic_score, 3) as demo_score,\n",
    "  ROUND(hcc_score, 3) as hcc_score,\n",
    "  ROUND(total_risk_score, 3) as risk_score,\n",
    "  projected_annual_payment\n",
    "FROM payer_gold.member_risk_scores\n",
    "ORDER BY total_risk_score DESC\n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab5f7a1-4b7e-44d2-9a6f-53f06724be6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 2: Revenue Forecast & Impact Analysis\n",
    "\n",
    "### üí∞ Business Goal\n",
    "Project total CMS revenue based on risk scores to support financial planning.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Total member population\n",
    "- Average risk score\n",
    "- Projected annual revenue\n",
    "- Revenue by plan and risk tier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fdacb9-d31e-4121-a986-f40b754d9d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Revenue Forecast by Plan\n",
    "CREATE OR REPLACE TABLE payer_gold.revenue_forecast AS\n",
    "SELECT\n",
    "  plan_id,\n",
    "  COUNT(DISTINCT member_id) as total_members,\n",
    "  ROUND(AVG(total_risk_score), 3) as avg_risk_score,\n",
    "  ROUND(MIN(total_risk_score), 3) as min_risk_score,\n",
    "  ROUND(MAX(total_risk_score), 3) as max_risk_score,\n",
    "  SUM(projected_annual_payment) as total_projected_revenue,\n",
    "  ROUND(AVG(projected_annual_payment), 2) as avg_payment_per_member,\n",
    "  SUM(CASE WHEN total_risk_score >= 1.5 THEN 1 ELSE 0 END) as high_risk_members,\n",
    "  SUM(CASE WHEN total_risk_score < 1.0 THEN 1 ELSE 0 END) as low_risk_members\n",
    "FROM payer_gold.member_risk_scores\n",
    "GROUP BY plan_id\n",
    "ORDER BY total_projected_revenue DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f93350-2e88-4a92-9a76-ba5f16527568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Display revenue forecast\n",
    "SELECT * FROM payer_gold.revenue_forecast;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c1af5f-5c26-4fb6-8883-0ee0abd51e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: HCC Distribution Analysis\n",
    "\n",
    "### üìà Business Goal\n",
    "Understand which HCC categories drive the most revenue and identify coding opportunities.\n",
    "\n",
    "This helps HQRI:\n",
    "- Identify high-value diagnoses for provider education\n",
    "- Monitor HCC capture rates\n",
    "- Find gaps in documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039c2881-714f-4efb-84fd-da41099699a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- HCC Category Distribution and Revenue Impact\n",
    "CREATE OR REPLACE TABLE payer_gold.hcc_distribution AS\n",
    "WITH hcc_exploded AS (\n",
    "  SELECT\n",
    "    member_id,\n",
    "    plan_id,\n",
    "    EXPLODE(hcc_categories) as hcc_category\n",
    "  FROM payer_gold.member_risk_scores\n",
    ")\n",
    "SELECT\n",
    "  he.hcc_category,\n",
    "  r.diagnosis_desc,\n",
    "  r.hcc_coefficient,\n",
    "  COUNT(DISTINCT he.member_id) as member_count,\n",
    "  COUNT(DISTINCT he.plan_id) as plan_count,\n",
    "  ROUND(r.hcc_coefficient * COUNT(DISTINCT he.member_id) * 10000, 2) as total_revenue_impact,\n",
    "  ROUND(r.hcc_coefficient * 10000, 2) as revenue_per_member\n",
    "FROM hcc_exploded he\n",
    "INNER JOIN payer_gold.hcc_reference r ON he.hcc_category = r.hcc_category\n",
    "GROUP BY he.hcc_category, r.diagnosis_desc, r.hcc_coefficient\n",
    "ORDER BY total_revenue_impact DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02aeb28f-1a1c-498b-9943-59d75884fa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View HCC distribution\n",
    "SELECT * FROM payer_gold.hcc_distribution;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2231a1-375a-4c6c-a787-9c159e15f97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 4: Data Quality & Compliance Audit\n",
    "\n",
    "### ‚úÖ Business Goal\n",
    "Ensure encounter data meets CMS submission standards and identify data quality issues.\n",
    "\n",
    "**CMS Requirements:**\n",
    "- Valid diagnosis codes (ICD-10 format)\n",
    "- Complete member demographics\n",
    "- Valid provider NPIs\n",
    "- Service dates within coverage period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838cab28-b5ff-4ac0-b676-a4563be46845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Data Quality Audit for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.data_quality_audit AS\n",
    "WITH encounter_checks AS (\n",
    "  SELECT\n",
    "    'Total Encounters' as metric,\n",
    "    COUNT(*) as record_count,\n",
    "    NULL as pct_of_total,\n",
    "    'INFO' as severity\n",
    "  FROM payer_silver.claims\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Encounters Missing Diagnosis',\n",
    "    COUNT(DISTINCT c.claim_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT c.claim_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.claims c\n",
    "  LEFT JOIN payer_bronze.diagnosis_raw d ON c.claim_id = d.claim_id\n",
    "  WHERE d.claim_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Diagnoses Mapped to HCC',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_bronze.diagnosis_raw),\n",
    "      2\n",
    "    ),\n",
    "    'INFO'\n",
    "  FROM payer_gold.diagnosis_with_hcc\n",
    "  WHERE is_hcc = 1\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Members Without Risk Scores',\n",
    "    COUNT(DISTINCT m.member_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT m.member_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.members),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.members m\n",
    "  LEFT JOIN payer_gold.member_risk_scores r ON m.member_id = r.member_id\n",
    "  WHERE r.member_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Providers Missing NPI',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.providers),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.providers\n",
    "  WHERE npi IS NULL OR TRIM(npi) = ''\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Claims with Invalid Status',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.claims\n",
    "  WHERE claim_status NOT IN ('approved', 'paid', 'pending')\n",
    ")\n",
    "SELECT\n",
    "  metric,\n",
    "  record_count,\n",
    "  COALESCE(pct_of_total, 0.0) as pct_of_total,\n",
    "  severity,\n",
    "  CASE\n",
    "    WHEN severity = 'ERROR' AND record_count > 0 THEN 'FAIL'\n",
    "    WHEN severity = 'WARNING' AND pct_of_total > 5 THEN 'REVIEW'\n",
    "    ELSE 'PASS'\n",
    "  END as status\n",
    "FROM encounter_checks\n",
    "ORDER BY\n",
    "  CASE severity WHEN 'ERROR' THEN 1 WHEN 'WARNING' THEN 2 ELSE 3 END,\n",
    "  pct_of_total DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f520f3f-5bd9-43cf-891f-d3849359f485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View data quality audit results\n",
    "SELECT * FROM payer_gold.data_quality_audit;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9961268c-b1f3-4604-bc54-0f427c2db0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 5: Member Risk Stratification\n",
    "\n",
    "### üë• Business Goal\n",
    "Segment members by risk level to support care management and intervention programs.\n",
    "\n",
    "**Risk Tiers:**\n",
    "- **Very High Risk** (Score > 2.0): Intensive care management\n",
    "- **High Risk** (Score 1.5-2.0): Enhanced monitoring\n",
    "- **Moderate Risk** (Score 1.0-1.5): Standard care\n",
    "- **Low Risk** (Score < 1.0): Preventive care focus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7396318d-098e-4644-b0bc-102527210d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Member Risk Stratification\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_stratification AS\n",
    "SELECT\n",
    "  member_id,\n",
    "  CONCAT(first_name, ' ', last_name) as member_name,\n",
    "  age,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  total_risk_score,\n",
    "  hcc_count,\n",
    "  projected_annual_payment,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Very High Risk'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'High Risk'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Moderate Risk'\n",
    "    ELSE 'Low Risk'\n",
    "  END as risk_tier,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Intensive Care Management Required'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'Enhanced Monitoring Recommended'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Standard Care Protocol'\n",
    "    ELSE 'Preventive Care Focus'\n",
    "  END as care_recommendation,\n",
    "  hcc_categories as active_hcc_list\n",
    "FROM payer_gold.member_risk_scores;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae8461-2e2e-4566-a63d-d6e941b147dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Summary statistics by risk tier\n",
    "SELECT\n",
    "  risk_tier,\n",
    "  COUNT(*) as member_count,\n",
    "  ROUND(AVG(age), 1) as avg_age,\n",
    "  ROUND(AVG(total_risk_score), 3) as avg_risk_score,\n",
    "  ROUND(AVG(hcc_count), 1) as avg_hcc_count,\n",
    "  SUM(projected_annual_payment) as total_revenue,\n",
    "  ROUND(AVG(projected_annual_payment), 2) as avg_payment_per_member\n",
    "FROM payer_gold.member_risk_stratification\n",
    "GROUP BY risk_tier\n",
    "ORDER BY avg_risk_score DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291bd675-7d9a-4f47-9cb4-6d1d93148a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 6: Provider Performance on Risk Capture\n",
    "\n",
    "### üè• Business Goal\n",
    "Identify which providers excel at documenting HCC conditions to guide provider education.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Members per provider\n",
    "- Average risk score of provider's panel\n",
    "- HCC capture rate\n",
    "- Revenue attributed to provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3661be1-6e9a-4ba1-9334-8aea8f02f3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, avg, sum, round as spark_round, col\n",
    "\n",
    "# Provider Performance Analysis\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "members = spark.table(\"payer_gold.member_risk_scores\")\n",
    "diagnosis_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# Join claims with risk scores\n",
    "provider_performance = claims \\\n",
    "    .join(providers, \"provider_id\") \\\n",
    "    .join(members, \"member_id\") \\\n",
    "    .join(diagnosis_hcc, \"claim_id\") \\\n",
    "    .groupBy(\n",
    "        \"provider_id\",\n",
    "        \"provider_name\",\n",
    "        \"specialty\",\n",
    "        \"city\",\n",
    "        \"state\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        countDistinct(\"member_id\").alias(\"unique_members\"),\n",
    "        count(\"claim_id\").alias(\"total_encounters\"),\n",
    "        spark_round(avg(\"total_risk_score\"), 3).alias(\"avg_member_risk_score\"),\n",
    "        sum(col(\"hcc_coefficient\")).alias(\"total_hcc_value\"),\n",
    "        countDistinct(col(\"hcc_category\")).alias(\"unique_hccs_captured\"),\n",
    "        spark_round(sum(\"projected_annual_payment\"), 2).alias(\"attributed_revenue\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"hcc_capture_rate\",\n",
    "        spark_round(col(\"unique_hccs_captured\") / col(\"unique_members\"), 2)\n",
    "    ) \\\n",
    "    .orderBy(col(\"attributed_revenue\").desc())\n",
    "\n",
    "# Display top providers\n",
    "print(\"üè• Top 20 Providers by Risk Capture Performance:\")\n",
    "display(provider_performance.limit(20))\n",
    "\n",
    "# Save to Gold table\n",
    "provider_performance.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.provider_risk_capture_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d7d1eb-2e5f-494a-9bea-b64353481adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 7: Encounter Datamart for CMS Submission\n",
    "\n",
    "### üìä Business Goal\n",
    "Create a CMS-ready encounter datamart with all required fields and validations.\n",
    "\n",
    "**CMS Submission Requirements:**\n",
    "- Valid member enrollment\n",
    "- Complete encounter details (dates, provider, diagnosis)\n",
    "- Proper diagnosis code formatting (ICD-10)\n",
    "- Service within coverage period\n",
    "- Provider has valid NPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169b5140-87ca-486b-8256-1b5edac71e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Encounter Datamart for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.encounter_datamart_cms AS\n",
    "SELECT\n",
    "  -- Encounter identifiers\n",
    "  c.claim_id as encounter_id,\n",
    "  c.claim_date as encounter_date,\n",
    "  c.claim_status as encounter_status,\n",
    "  \n",
    "  -- Member information\n",
    "  m.member_id,\n",
    "  m.first_name,\n",
    "  m.last_name,\n",
    "  m.birth_date,\n",
    "  m.gender,\n",
    "  m.plan_id,\n",
    "  YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as member_age,\n",
    "  \n",
    "  -- Provider information\n",
    "  p.provider_id,\n",
    "  p.npi as provider_npi,\n",
    "  p.provider_name,\n",
    "  p.specialty as provider_specialty,\n",
    "  p.state as provider_state,\n",
    "  \n",
    "  -- Diagnosis information\n",
    "  d.diagnosis_code as icd10_code,\n",
    "  d.diagnosis_desc,\n",
    "  d.hcc_category,\n",
    "  d.hcc_coefficient,\n",
    "  d.is_hcc,\n",
    "  \n",
    "  -- Claim financial\n",
    "  c.total_charge,\n",
    "  \n",
    "  -- Data quality flags\n",
    "  CASE \n",
    "    WHEN m.member_id IS NULL THEN 'FAIL: Missing Member'\n",
    "    WHEN p.npi IS NULL OR TRIM(p.npi) = '' THEN 'FAIL: Missing Provider NPI'\n",
    "    WHEN d.diagnosis_code IS NULL THEN 'FAIL: Missing Diagnosis'\n",
    "    WHEN c.claim_date < m.effective_date THEN 'FAIL: Service Before Coverage'\n",
    "    WHEN c.claim_status NOT IN ('approved', 'paid') THEN 'WARNING: Invalid Status'\n",
    "    ELSE 'PASS'\n",
    "  END as submission_validation_status,\n",
    "  \n",
    "  -- Submission flag\n",
    "  CASE \n",
    "    WHEN m.member_id IS NOT NULL \n",
    "     AND p.npi IS NOT NULL \n",
    "     AND TRIM(p.npi) != ''\n",
    "     AND d.diagnosis_code IS NOT NULL\n",
    "     AND c.claim_date >= m.effective_date\n",
    "     AND c.claim_status IN ('approved', 'paid')\n",
    "    THEN 1 \n",
    "    ELSE 0 \n",
    "  END as cms_submission_ready,\n",
    "  \n",
    "  CURRENT_TIMESTAMP() as datamart_created_at\n",
    "  \n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "LEFT JOIN payer_gold.diagnosis_with_hcc d ON c.claim_id = d.claim_id;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16171ad-f5b3-4528-9536-9accf5826903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CMS Submission Readiness Summary\n",
    "SELECT\n",
    "  submission_validation_status,\n",
    "  COUNT(*) as encounter_count,\n",
    "  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct_of_total,\n",
    "  SUM(cms_submission_ready) as ready_for_submission\n",
    "FROM payer_gold.encounter_datamart_cms\n",
    "GROUP BY submission_validation_status\n",
    "ORDER BY encounter_count DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67f5872-4eed-41cb-a33d-20b41fa85b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ Gold Layer Summary\n",
    "\n",
    "**Congratulations!** You've created a comprehensive HQRI analytics platform with:\n",
    "\n",
    "‚úÖ **HCC Risk Scores** - Member-level risk calculations for CMS payments  \n",
    "‚úÖ **Revenue Forecasting** - Projected payments by plan and risk tier  \n",
    "‚úÖ **HCC Distribution** - Identify high-value diagnosis categories  \n",
    "‚úÖ **Data Quality Audits** - Ensure CMS submission compliance  \n",
    "‚úÖ **Member Stratification** - Segment members for care management  \n",
    "‚úÖ **Provider Performance** - Track risk capture excellence  \n",
    "‚úÖ **Encounter Datamart** - CMS-ready submission data  \n",
    "\n",
    "---\n",
    "\n",
    "Now let's visualize these insights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd070c5e-2128-43c3-bb65-cb5fec3566f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìä DBSQL Analytics & Dashboards\n",
    "\n",
    "---\n",
    "\n",
    "## HQRI Visualizations with Databricks SQL\n",
    "\n",
    "Now let's create **interactive visualizations** for HQRI stakeholders using Databricks' native visualization capabilities.\n",
    "\n",
    "> **üí° Tip**: Use the `display()` function to automatically generate charts, or use Databricks SQL dashboards!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fc395e-70a3-43f4-a0e4-0698fcf46d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Visualization 1: Risk Score Distribution\n",
    "\n",
    "# Display risk score distribution across all members\n",
    "risk_scores = spark.table(\"payer_gold.member_risk_scores\")\n",
    "display(risk_scores.select(\"member_id\", \"age\", \"gender\", \"total_risk_score\", \"hcc_count\", \"projected_annual_payment\"))\n",
    "\n",
    "# üí° Try this: In the visualization panel, create a HISTOGRAM with total_risk_score on X-axis to see the distribution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ff1561-f6e2-43db-91a2-bef5a7e9ccd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization 2: HCC Revenue Impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0060180a-6efb-4de0-bc2f-31e90ba6a9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show which HCC categories drive the most revenue\n",
    "hcc_dist = spark.table(\"payer_gold.hcc_distribution\")\n",
    "display(hcc_dist)\n",
    "\n",
    "# üí° Try this: Create a BAR CHART with diagnosis_desc on X-axis and total_revenue_impact on Y-axis!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dc7637-0cbc-45aa-830a-6eef7f338892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"What kind of aggregations can I do with table payer_gold.claims_enriched?\"\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713ffea-c715-46f9-a267-75093518164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization 3: Member Risk Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d43e2-a24b-4243-9566-3e3d8c733913",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Payer Claims Enriched Table"
    }
   },
   "outputs": [],
   "source": [
    "# Display member stratification by risk tier\n",
    "from pyspark.sql.functions import count, sum, avg, round as spark_round\n",
    "\n",
    "stratification = spark.table(\"payer_gold.member_risk_stratification\") \\\n",
    "    .groupBy(\"risk_tier\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"member_count\"),\n",
    "        spark_round(avg(\"total_risk_score\"), 3).alias(\"avg_risk_score\"),\n",
    "        sum(\"projected_annual_payment\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(\"avg_risk_score\", ascending=False)\n",
    "\n",
    "display(stratification)\n",
    "\n",
    "# üí° Try this: Create a PIE CHART showing member_count by risk_tier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6513d65d-db72-4aa1-89a8-ffb2de886d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization 4: Revenue Forecast by Plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27af84c5-623f-44d6-9a93-019c3f90fb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Show projected revenue by plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55eeef7f-0571-40ba-9269-87c1388958c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges by Claim Status"
    }
   },
   "outputs": [],
   "source": [
    "revenue_forecast = spark.table(\"payer_gold.revenue_forecast\")\n",
    "display(revenue_forecast)\n",
    "\n",
    "# üí° Try this: Create a BAR CHART with plan_id on X-axis and total_projected_revenue on Y-axis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bd3b72-1e38-4861-a90a-b1c0155d02d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization 5: Data Quality Audit Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a772eaa0-abc2-43a6-8185-6bbcb61ba3d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count of Claims Grouped by Gender"
    }
   },
   "outputs": [],
   "source": [
    "# Display data quality metrics\n",
    "audit_results = spark.table(\"payer_gold.data_quality_audit\")\n",
    "display(audit_results)\n",
    "\n",
    "# üí° This shows compliance readiness for CMS submission\n",
    "# Status = PASS means ready for submission, FAIL means action required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fb4408-de01-4392-b809-162cdc6e3cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization 6: Provider Risk Capture Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85dff0f-de3d-4084-bff0-4e8d1c11a22f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges Grouped by Claim Date"
    }
   },
   "outputs": [],
   "source": [
    "# Show provider performance metrics\n",
    "provider_perf = spark.table(\"payer_gold.provider_risk_capture_performance\")\n",
    "display(provider_perf)\n",
    "\n",
    "# üí° Try this: Create a SCATTER PLOT with unique_members on X-axis and avg_member_risk_score on Y-axis\n",
    "# This shows which providers have high-risk patient panels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7439bb5-34bd-47c3-a604-99b8471ab7a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Using Databricks AI/BI and Genie\n",
    "\n",
    "Databricks provides powerful AI-powered analytics tools for HQRI teams:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6923f450-56f2-46da-a356-dbb09b5a5ec1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group Claims by City and Count"
    }
   },
   "source": [
    "### AI/BI: Intelligent Analytics for HQRI\n",
    "\n",
    "Databricks AI/BI provides semantic understanding of your risk adjustment data, enabling self-service analytics.\n",
    "\n",
    "**Use Cases for HQRI:**\n",
    "- Natural language queries: \"What's the average risk score by plan?\"\n",
    "- Automated insights: Detect anomalies in HCC coding patterns\n",
    "- Data lineage: Track how risk scores flow through your pipeline\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"AI/BI\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e0c6dc-e3a0-451a-930f-f39c928a55c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Genie: Talk with Your Risk Adjustment Data\n",
    "\n",
    "Genie lets HQRI analysts ask questions in natural language:\n",
    "\n",
    "**Example Queries:**\n",
    "- \"Show me members with risk scores above 2.0\"\n",
    "- \"Which HCC categories have the highest revenue impact?\"\n",
    "- \"What's the data quality score for last month's encounters?\"\n",
    "- \"Compare risk scores between MA-PD and MA-only plans\"\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Genie\" width=\"600\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ee3c88-20a6-4788-95b2-7bc433d43534",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Total Charges from Payer Claims Table"
    }
   },
   "source": [
    "\n",
    "## üí° Example Prompts for AI Assistant\n",
    "\n",
    "Try asking the Databricks AI Assistant these HQRI-specific questions:\n",
    "\n",
    "1. **Risk Scoring**: \"How do I add disease interaction logic to my HCC risk score calculation?\"\n",
    "2. **Data Quality**: \"Show me SQL to find members with duplicate HCC categories\"\n",
    "3. **Performance**: \"How can I optimize queries on the encounter_datamart_cms table?\"\n",
    "4. **Forecasting**: \"Help me build a time series forecast for next year's risk scores\"\n",
    "5. **Compliance**: \"What Delta Lake features help with CMS audit requirements?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c481c9d-e2db-4f9d-bb8d-7a059e0fd3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Best Practices for HQRI on Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b953e3-6eba-43c5-8157-48245b421e22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Claim Date and Total Charge from Payer Claims"
    }
   },
   "source": [
    "\n",
    "## üöÄ Performance Optimization for Encounter Data\n",
    "\n",
    "### 1. Partition Large Tables by Date\n",
    "\n",
    "```python\n",
    "# Partition encounter datamart by month for faster queries\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE payer_gold.encounter_datamart_cms\n",
    "  PARTITION BY (YEAR(encounter_date), MONTH(encounter_date))\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. Z-Order for Common Filter Columns\n",
    "\n",
    "```sql\n",
    "-- Optimize for queries filtering by member_id and HCC category\n",
    "OPTIMIZE payer_gold.member_risk_scores\n",
    "ZORDER BY (member_id, plan_id);\n",
    "\n",
    "OPTIMIZE payer_gold.encounter_datamart_cms\n",
    "ZORDER BY (member_id, encounter_date, hcc_category);\n",
    "```\n",
    "\n",
    "### 3. Enable Predictive Optimization\n",
    "\n",
    "```sql\n",
    "-- Let Databricks automatically optimize your tables\n",
    "ALTER TABLE payer_gold.member_risk_scores\n",
    "SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true',\n",
    "                   'delta.autoOptimize.autoCompact' = 'true');\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b7166b-70ab-4b63-8055-51c5d42cc956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Data Quality & Compliance Best Practices\n",
    "\n",
    "### 1. Implement Data Quality Checks\n",
    "\n",
    "```python\n",
    "# Add constraints to ensure data integrity\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE payer_silver.claims \n",
    "  ADD CONSTRAINT valid_charge CHECK (total_charge > 0)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE payer_silver.providers \n",
    "  ADD CONSTRAINT valid_npi CHECK (LENGTH(npi) = 10)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. Enable Change Data Feed for Auditing\n",
    "\n",
    "```sql\n",
    "-- Track all changes for CMS audit requirements\n",
    "ALTER TABLE payer_gold.member_risk_scores\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "\n",
    "-- Query changes\n",
    "SELECT * FROM table_changes('payer_gold.member_risk_scores', 0);\n",
    "```\n",
    "\n",
    "### 3. Use Delta Lake Time Travel\n",
    "\n",
    "```sql\n",
    "-- Review historical risk scores for audits\n",
    "SELECT * FROM payer_gold.member_risk_scores\n",
    "VERSION AS OF 10;  -- 10 versions ago\n",
    "\n",
    "-- Compare risk scores between versions\n",
    "SELECT current.member_id, \n",
    "       current.total_risk_score as current_score,\n",
    "       previous.total_risk_score as previous_score\n",
    "FROM payer_gold.member_risk_scores current\n",
    "LEFT JOIN payer_gold.member_risk_scores@v5 previous \n",
    "  ON current.member_id = previous.member_id\n",
    "WHERE current.total_risk_score != previous.total_risk_score;\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0836fe4a-ef44-4b1d-bea7-f8458e833451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîê Unity Catalog Governance for HQRI\n",
    "\n",
    "### 1. Row-Level Security\n",
    "\n",
    "```sql\n",
    "-- Restrict access to sensitive member data by region\n",
    "CREATE FUNCTION payer_gold.row_filter_by_region(state STRING)\n",
    "RETURN IF(IS_MEMBER('region_east_users'), state IN ('NY', 'NJ', 'PA'), TRUE);\n",
    "\n",
    "ALTER TABLE payer_gold.member_risk_scores\n",
    "SET ROW FILTER payer_gold.row_filter_by_region ON (state);\n",
    "```\n",
    "\n",
    "### 2. Column-Level Masking\n",
    "\n",
    "```sql\n",
    "-- Mask member names for non-privileged users\n",
    "CREATE FUNCTION payer_gold.mask_member_name(name STRING)\n",
    "RETURN CASE\n",
    "  WHEN IS_MEMBER('hqri_admin') THEN name\n",
    "  ELSE '***MASKED***'\n",
    "END;\n",
    "\n",
    "ALTER TABLE payer_gold.member_risk_scores\n",
    "ALTER COLUMN first_name SET MASK payer_gold.mask_member_name;\n",
    "```\n",
    "\n",
    "### 3. Data Lineage & Tagging\n",
    "\n",
    "```sql\n",
    "-- Tag PHI/PII columns for compliance\n",
    "ALTER TABLE payer_gold.member_risk_scores\n",
    "ALTER COLUMN first_name SET TAGS ('PHI' = 'true', 'PII' = 'true');\n",
    "\n",
    "-- Add business glossary terms\n",
    "COMMENT ON TABLE payer_gold.member_risk_scores IS\n",
    "'CMS-V24 risk scores for Medicare Advantage members. Updated daily.';\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a066c3-a0c9-41e8-bdd4-475282fb0950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Delta Lake Maintenance\n",
    "\n",
    "### Regular Maintenance Schedule\n",
    "\n",
    "```sql\n",
    "-- Run weekly: Compact small files\n",
    "OPTIMIZE payer_gold.encounter_datamart_cms;\n",
    "OPTIMIZE payer_gold.member_risk_scores;\n",
    "\n",
    "-- Run monthly: Remove old file versions (keep 30 days for CMS audits)\n",
    "VACUUM payer_gold.encounter_datamart_cms RETAIN 720 HOURS;\n",
    "\n",
    "-- Run weekly: Update statistics for query optimization\n",
    "ANALYZE TABLE payer_gold.member_risk_scores COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "### Monitor Table Health\n",
    "\n",
    "```python\n",
    "# Check table size and file count\n",
    "display(spark.sql(\"DESCRIBE DETAIL payer_gold.member_risk_scores\"))\n",
    "\n",
    "# View table history\n",
    "display(spark.sql(\"DESCRIBE HISTORY payer_gold.member_risk_scores\"))\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa71c33b-47d1-4b46-9940-2fdfd3cfb439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary: HQRI Value Delivered\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a comprehensive **HQRI Risk Adjustment Analytics Platform** on Databricks!\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "\n",
    "#### 1. **HCC Risk Score Calculations**\n",
    "- Mapped ICD-10 diagnoses to HCC categories\n",
    "- Calculated member-level risk scores (demographic + HCC coefficients)\n",
    "- Projected CMS payments based on risk scores\n",
    "\n",
    "#### 2. **Revenue Impact Analysis**\n",
    "- Forecasted annual revenue by plan and risk tier\n",
    "- Identified high-value HCC categories driving payments\n",
    "- Segmented members by risk level for care management\n",
    "\n",
    "#### 3. **Data Quality & Compliance**\n",
    "- Built data quality audit tables for CMS submission readiness\n",
    "- Implemented validation rules for encounter data\n",
    "- Created audit trails with Delta Lake Time Travel\n",
    "\n",
    "#### 4. **Encounter Datamart**\n",
    "- Created CMS-ready encounter data with all required fields\n",
    "- Added validation flags for submission quality\n",
    "- Tracked data lineage for regulatory compliance\n",
    "\n",
    "#### 5. **Provider Performance**\n",
    "- Analyzed provider risk capture rates\n",
    "- Identified top performers for best practice sharing\n",
    "- Attributed revenue to provider documentation quality\n",
    "\n",
    "#### 6. **Interactive Dashboards**\n",
    "- Built DBSQL visualizations for executive reporting\n",
    "- Created self-service analytics with AI/BI and Genie\n",
    "- Enabled natural language queries for business users\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Business Impact for HQRI\n",
    "\n",
    "**This platform enables HQRI to:**\n",
    "\n",
    "- üìà **Maximize Revenue**: Accurate risk coding increases CMS payments\n",
    "- ‚≠ê **Improve Star Ratings**: Quality metrics tied to bonus payments\n",
    "- ‚úÖ **Ensure Compliance**: Meet federal audit and reporting requirements\n",
    "- üë• **Optimize Care**: Identify high-risk members for interventions\n",
    "- üè• **Educate Providers**: Share best practices for risk documentation\n",
    "- üìä **Forecast Accurately**: Project future revenue based on risk trends\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps for HQRI\n",
    "\n",
    "### Immediate Actions\n",
    "1. ‚úÖ Schedule daily/weekly refresh of risk score tables\n",
    "2. ‚úÖ Set up Databricks SQL dashboards for stakeholders\n",
    "3. ‚úÖ Configure Unity Catalog governance for PHI/PII\n",
    "4. ‚úÖ Implement Delta Live Tables for automated pipelines\n",
    "\n",
    "### Advanced Capabilities\n",
    "1. üìà **Machine Learning**: Predict which members will develop high-cost conditions\n",
    "2. ü§ñ **AI-Powered Coding**: Use LLMs to suggest HCC codes from clinical notes\n",
    "3. üìä **Real-Time Analytics**: Stream encounter data for immediate risk scoring\n",
    "4. üîó **External Integration**: Connect to CMS EDPS for submission validation\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Resources\n",
    "\n",
    "### Databricks Training\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training)\n",
    "- [Healthcare & Life Sciences Solutions](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n",
    "- [Data Engineering Professional Certification](https://www.databricks.com/learn/certification/data-engineer-professional)\n",
    "\n",
    "### CMS Risk Adjustment\n",
    "- [CMS Risk Adjustment Model Documentation](https://www.cms.gov/medicare/payment/medicare-advantage-rates-statistics/risk-adjustment)\n",
    "- [HCC Coding Guidelines](https://www.cms.gov/medicare/payment/medicare-advantage-rates-statistics/risk-adjustment)\n",
    "\n",
    "### Databricks Resources\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [Databricks SQL Best Practices](https://docs.databricks.com/sql/admin/sql-endpoints.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this **HQRI Risk Adjustment Workshop**!\n",
    "\n",
    "You now have the skills to:\n",
    "- Build robust risk adjustment pipelines\n",
    "- Calculate HCC risk scores for CMS payments\n",
    "- Create compliance-ready encounter datamarts\n",
    "- Generate actionable analytics for revenue optimization\n",
    "\n",
    "**Questions?** Reach out to your Databricks account team or visit the [Databricks Community](https://community.databricks.com/).\n",
    "\n",
    "---\n",
    "\n",
    "*Workshop Version: November 14, 2025 - HQRI Humana*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a98a2-db41-4a69-b0ec-9876ee60ccfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Databricks Healthcare Payer Analytics Workshop! Let's review what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished\n",
    "\n",
    "### 1. **Medallion Architecture**\n",
    "- ‚úÖ Built a complete **Bronze ‚Üí Silver ‚Üí Gold** pipeline\n",
    "- ‚úÖ Understood data quality improvement at each layer\n",
    "- ‚úÖ Created analytics-ready datasets\n",
    "\n",
    "### 2. **Data Engineering Skills**\n",
    "- ‚úÖ Loaded data using **COPY INTO**\n",
    "- ‚úÖ Transformed data with **SQL and PySpark**\n",
    "- ‚úÖ Applied data quality checks and validations\n",
    "- ‚úÖ Created aggregations and derived metrics\n",
    "\n",
    "### 3. **Analytics & Visualization**\n",
    "- ‚úÖ Generated business insights from data\n",
    "- ‚úÖ Created interactive visualizations\n",
    "- ‚úÖ Performed statistical analysis\n",
    "- ‚úÖ Built executive dashboards\n",
    "\n",
    "### 4. **Databricks Platform**\n",
    "- ‚úÖ Worked with **Unity Catalog**\n",
    "- ‚úÖ Used **Delta Lake** for reliable data storage\n",
    "- ‚úÖ Leveraged **AI Assistant** for code help\n",
    "- ‚úÖ Applied performance optimization techniques\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. ‚≠ê **Bookmark this notebook** for future reference\n",
    "2. üìñ Complete the hands-on exercises\n",
    "3. üîÑ Try modifying the code with your own logic\n",
    "4. üíæ Export your results and share with your team\n",
    "\n",
    "### Continue Learning\n",
    "\n",
    "#### üìö Advanced Topics to Explore\n",
    "- **Delta Live Tables (DLT)**: Declarative pipeline framework\n",
    "- **Databricks Workflows**: Orchestration and scheduling\n",
    "- **Unity Catalog**: Advanced governance features\n",
    "- **Databricks SQL**: Performance tuning and optimization\n",
    "- **Machine Learning**: MLflow and Feature Store\n",
    "- **Streaming**: Structured Streaming with Delta Lake\n",
    "\n",
    "#### üîó Helpful Resources\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training)\n",
    "- [Community Forums](https://community.databricks.com/)\n",
    "- [Databricks Blog](https://www.databricks.com/blog)\n",
    "\n",
    "#### üéØ Recommended Certifications\n",
    "- **Databricks Lakehouse Platform Fundamentals**\n",
    "- **Databricks Certified Data Engineer Associate**\n",
    "- **Databricks Certified Data Analyst Associate**\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Applications\n",
    "\n",
    "Apply these skills to:\n",
    "- üè• **Healthcare**: Claims processing, patient analytics, risk scoring\n",
    "- üè¶ **Finance**: Fraud detection, transaction analysis, risk management\n",
    "- üõí **Retail**: Customer analytics, inventory optimization, sales forecasting\n",
    "- üì± **Technology**: User behavior analysis, product metrics, A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Get Help & Share\n",
    "\n",
    "### Need Help?\n",
    "- üí¨ Ask the **Databricks AI Assistant**\n",
    "- üåê Visit [Databricks Community](https://community.databricks.com/)\n",
    "- üìß Contact your Databricks account team\n",
    "- üìñ Check [Stack Overflow](https://stackoverflow.com/questions/tagged/databricks)\n",
    "\n",
    "### Share Your Success\n",
    "- ‚≠ê Share insights with your colleagues\n",
    "- üìä Create dashboards for stakeholders\n",
    "- üé§ Present your work at team meetings\n",
    "- üèÜ Contribute to the Databricks community\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: October 7, 2025*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f241a39f-e379-43c2-bf07-a69604fe8fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ Quick Reference Guide\n",
    "\n",
    "## Common PySpark Operations\n",
    "\n",
    "### Reading Data\n",
    "```python\n",
    "# From Delta table\n",
    "df = spark.table(\"catalog.schema.table\")\n",
    "\n",
    "# From CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/file.csv\")\n",
    "\n",
    "# From JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# From Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Writing Data\n",
    "```python\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "# Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"table_name\")\n",
    "\n",
    "# With partitioning\n",
    "df.write.format(\"delta\").partitionBy(\"date_col\").saveAsTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Select columns\n",
    "df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Filter rows\n",
    "df.filter(col(\"amount\") > 100)\n",
    "df.where(\"amount > 100\")\n",
    "\n",
    "# Add new column\n",
    "df.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# Rename column\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "# Drop column\n",
    "df.drop(\"col_name\")\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    avg(\"amount\").alias(\"average\")\n",
    ")\n",
    "\n",
    "# Join tables\n",
    "df1.join(df2, \"key_column\")\n",
    "df1.join(df2, df1.key == df2.key, \"left\")\n",
    "\n",
    "# Sort\n",
    "df.orderBy(\"col_name\")\n",
    "df.orderBy(col(\"col_name\").desc())\n",
    "\n",
    "# Remove duplicates\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "### Common Functions\n",
    "```python\n",
    "# String functions\n",
    "upper(\"col_name\")\n",
    "lower(\"col_name\")\n",
    "trim(\"col_name\")\n",
    "concat(\"col1\", \"col2\")\n",
    "substring(\"col_name\", 1, 5)\n",
    "\n",
    "# Date functions\n",
    "current_date()\n",
    "current_timestamp()\n",
    "date_format(\"date_col\", \"yyyy-MM-dd\")\n",
    "year(\"date_col\")\n",
    "month(\"date_col\")\n",
    "datediff(\"date1\", \"date2\")\n",
    "\n",
    "# Math functions\n",
    "round(\"col_name\", 2)\n",
    "abs(\"col_name\")\n",
    "ceil(\"col_name\")\n",
    "floor(\"col_name\")\n",
    "\n",
    "# Conditional logic\n",
    "when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    "\n",
    "# Null handling\n",
    "col(\"col_name\").isNull()\n",
    "col(\"col_name\").isNotNull()\n",
    "coalesce(\"col1\", \"col2\", lit(0))\n",
    "```\n",
    "\n",
    "## Common SQL Operations\n",
    "\n",
    "### DDL Commands\n",
    "```sql\n",
    "-- Create database\n",
    "CREATE DATABASE IF NOT EXISTS database_name;\n",
    "\n",
    "-- Drop database\n",
    "DROP DATABASE IF EXISTS database_name CASCADE;\n",
    "\n",
    "-- Create table\n",
    "CREATE TABLE table_name (\n",
    "    id STRING,\n",
    "    amount DOUBLE,\n",
    "    date DATE\n",
    ");\n",
    "\n",
    "-- Drop table\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "-- Describe table\n",
    "DESCRIBE EXTENDED table_name;\n",
    "SHOW COLUMNS FROM table_name;\n",
    "```\n",
    "\n",
    "### DML Commands\n",
    "```sql\n",
    "-- Insert data\n",
    "INSERT INTO table_name VALUES (1, 'value1', 100);\n",
    "\n",
    "-- Update data (Delta Lake)\n",
    "UPDATE table_name SET amount = 200 WHERE id = 1;\n",
    "\n",
    "-- Delete data (Delta Lake)\n",
    "DELETE FROM table_name WHERE id = 1;\n",
    "\n",
    "-- Merge (Upsert)\n",
    "MERGE INTO target_table\n",
    "USING source_table\n",
    "ON target_table.id = source_table.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "### Query Commands\n",
    "```sql\n",
    "-- Basic SELECT\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "\n",
    "-- With WHERE clause\n",
    "SELECT * FROM table_name WHERE amount > 100;\n",
    "\n",
    "-- Aggregations\n",
    "SELECT category, COUNT(*), SUM(amount), AVG(amount)\n",
    "FROM table_name\n",
    "GROUP BY category;\n",
    "\n",
    "-- Joins\n",
    "SELECT a.*, b.name\n",
    "FROM table_a a\n",
    "INNER JOIN table_b b ON a.id = b.id;\n",
    "\n",
    "-- Window functions\n",
    "SELECT \n",
    "    *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as rank\n",
    "FROM table_name;\n",
    "\n",
    "-- CTE (Common Table Expression)\n",
    "WITH summary AS (\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM table_name\n",
    "    GROUP BY category\n",
    ")\n",
    "SELECT * FROM summary WHERE total > 1000;\n",
    "```\n",
    "\n",
    "## Databricks Utilities\n",
    "```python\n",
    "# File system operations\n",
    "dbutils.fs.ls(\"path/\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "dbutils.fs.rm(\"path/\", recurse=True)\n",
    "dbutils.fs.mkdirs(\"path/\")\n",
    "\n",
    "# Widgets (parameters)\n",
    "dbutils.widgets.text(\"param_name\", \"default_value\")\n",
    "param_value = dbutils.widgets.get(\"param_name\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, {\"param\": \"value\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this reference handy as you build your data pipelines!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3734562717507439,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Sunmin WIP_DBX Workshop_HQRI_11142025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "sunmin_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sunmin_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sunmin_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
