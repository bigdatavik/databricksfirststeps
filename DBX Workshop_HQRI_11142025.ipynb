{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676565f7-bc51-4b21-8fba-603008447446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Updated date: November 11, 2025_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks Workshop: HQRI Risk Adjustment & Analytics\n",
    "**Healthcare Quality Reporting & Improvement - DBSQL Analytics Workshop**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Build **Medallion Architecture** pipelines for risk adjustment data\n",
    "2. ‚úÖ Calculate **HCC (Hierarchical Condition Category) risk scores** for Medicare Advantage\n",
    "3. ‚úÖ Create **encounter datamart** tables for CMS submissions\n",
    "4. ‚úÖ Perform **data quality audits** for regulatory compliance\n",
    "5. ‚úÖ Build **Gold layer analytics** for revenue impact and Star Ratings\n",
    "6. ‚úÖ Forecast risk scores using **Databricks SQL (DBSQL)**\n",
    "\n",
    "---\n",
    "\n",
    "## üè• HQRI Use Case: Medicare Risk Adjustment\n",
    "\n",
    "**HQRI (Healthcare Quality Reporting & Improvement)** manages Humana's risk adjustment and quality reporting for Medicare Advantage:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Determine CMS payments based on member health status\n",
    "- üìä **Encounter Datamart**: Aggregate and validate claims/encounters for CMS\n",
    "- ‚≠ê **Star Ratings**: Drive quality metrics that impact bonus payments\n",
    "- ‚úÖ **Compliance & Audits**: Ensure federal regulatory compliance\n",
    "- üìà **Revenue Optimization**: Accurate coding and forecasting maximize financial performance\n",
    "\n",
    "### üóÇÔ∏è Dataset Overview\n",
    "\n",
    "We'll work with **Medicare risk adjustment data**:\n",
    "- **Members**: Medicare Advantage enrollees with demographics\n",
    "- **Claims**: Medical encounters with diagnosis codes\n",
    "- **Diagnoses**: ICD-10 codes mapped to HCC categories\n",
    "- **Providers**: Healthcare providers submitting encounters\n",
    "- **Procedures**: Services rendered and billed\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is a lakehouse?\n",
    "\n",
    "1. **Hybrid Architecture:**  \n",
    "   A lakehouse combines the best of data lakes (flexible, cheap storage) and data warehouses (structured, fast analytics), providing transactional and governance features on top of open cloud storage.\n",
    "\n",
    "2. **ACID Transactions and Schema Governance:**  \n",
    "   Lakehouses support ACID transactions for consistent concurrent data access and enforce schema management, which is essential for data integrity and compliance.\n",
    "\n",
    "3. **Open and Decoupled:**  \n",
    "   They use open file formats (like Parquet), decouple compute from storage for flexible scalability, and allow access by a variety of analytics, BI, and machine learning tools.\n",
    "\n",
    "4. **Supports All Workloads and Data Types:**  \n",
    "   The architecture enables SQL analytics, data science, machine learning, and can handle structured, semi-structured, and unstructured data (including images, text, video).\n",
    "\n",
    "5. **Single Platform, Enterprise Ready:**  \n",
    "   With features like real-time streaming, end-to-end governance, access control, and data discovery tools, lakehouses reduce complexity‚Äîallowing enterprises to manage all data and analytics needs in one unified system.\n",
    "![](https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog\n",
    "\n",
    "[Unified and open governance for data and AI in the Lakehouse](https://www.databricks.com/product/unity-catalog#features)\n",
    "\n",
    "Eliminate silos, simplify governance and accelerate insights at scale:\n",
    "\n",
    "- Centralizes governance, access control, auditing, and data discovery for all data and AI assets across Databricks workspaces.\n",
    "- Enables fine-grained, consistent data access policies (row- and column-level), defined once and applied everywhere.\n",
    "- Provides comprehensive data lineage and audit logs, showing how and by whom data is accessed and transformed.\n",
    "- Supports data discovery, tagging, and documentation, making it easier to find and understand datasets and models.\n",
    "- Works across multiple clouds and supports open formats (Delta, Parquet, etc.), avoiding vendor lock-in and enabling broad interoperability.\n",
    "- Allows secure data and AI sharing within and outside the organization, including clean rooms and partner collaborations.\n",
    "- Provides built-in monitoring for data quality, freshness, and usage, helping ensure compliance and rapid troubleshooting.\n",
    "- Integrates tightly with the catalog/schema/object model, enhancing organization and security for all managed data assets.\n",
    "\n",
    "![](https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086)\n",
    "\n",
    "[Unity Catalog Search & Data Explorer](https://app.getreprise.com/launch/96mpAqy/)\n",
    "\n",
    "[Exploring Lineage and Governance with Unity Catalog](https://app.getreprise.com/launch/MnqjQDX/)\n",
    "\n",
    "[A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2024-08/comprehensive-guide-to-data-and-ai-governance.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Medallion lakehouse architecture\n",
    "\n",
    "In this example, we will be following the **medallion lakehouse architecture**. The medallion architecture is a data design pattern to organize data in a lakehouse. The goal is to progressively improve the quality and structure of the data as it flows through each layer (Bronze [**raw**] ‚Üí Silver [**staging**] ‚Üí Gold [**main**]).\n",
    "\n",
    "1. **Bronze layer**: the raw, unvalidated data\n",
    "2. **Silver**: cleansed and conformed data\n",
    "3. **Gold**: curated business-level tables\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8292153-2b0a-47dd-b6c6-b1cf4c45cdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managed tables\n",
    "\n",
    "[How Unity Catalog Managed Tables Automate Performance at Scale](https://www.databricks.com/blog/how-unity-catalog-managed-tables-automate-performance-scale) with [Predictive Optimization](https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization)\n",
    "\n",
    "\n",
    "<!-- ![](https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384) -->\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9115bf6-333a-4cb8-965c-5e73a2d204d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Faster Queries: 20X query latency reduction](https://www.databricks.com/blog/predictive-optimization-automatically-delivers-faster-queries-and-lower-tco)\n",
    "\n",
    "**Predictive Optimization** in Databricks automates table management by leveraging Unity Catalog and the Data Intelligence Platform. This innovative feature currently runs the following optimizations for Unity Catalog managed tables:\n",
    "\n",
    "* **OPTIMIZE** - Triggers incremental clustering for enabled tables. Improves query performance by optimizing file sizes.\n",
    "* **VACUUM** - Reduces storage costs by deleting data files no longer referenced by the table.\n",
    "* **ANALYZE** - Triggers incremental update of statistics to improve query performance. \n",
    "\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/styles/max_1000x1000/public/2024-05/db-976-blog-img-og.png?itok=qWBT8VA-&v=1717158571\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n",
    "\n",
    "**Compaction** - This enhances query performance by optimizing file sizes, ensuring that data retrieval is efficient.\n",
    "\n",
    "**Liquid Clustering** - This technique incrementally clusters incoming data, enabling optimal data layout and efficient data skipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce4ebf7-dc51-44bc-8bf2-804fec60b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Medallion Pipeline for a Healthcare Payer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Risk Adjustment & HCC Modeling Concepts\n",
    "\n",
    "### üè• Medicare Risk Adjustment Overview\n",
    "\n",
    "CMS uses **HCC (Hierarchical Condition Category)** models to adjust capitation payments based on beneficiary health status:\n",
    "\n",
    "1. **ICD-10 Diagnosis Codes** ‚Üí mapped to **HCC categories**\n",
    "2. **HCC categories** ‚Üí assigned **coefficient weights**\n",
    "3. **Risk Score** = Sum of HCC weights + demographic factors (age, sex, disability status)\n",
    "4. **Payment** = Base rate √ó Risk Score\n",
    "\n",
    "**Example:**\n",
    "- Member with diabetes (HCC 19, weight 0.318) and heart failure (HCC 85, weight 0.368)\n",
    "- Base demographic score: 0.500\n",
    "- **Total Risk Score = 0.500 + 0.318 + 0.368 = 1.186**\n",
    "- If base payment = $10,000, **CMS pays: $10,000 √ó 1.186 = $11,860**\n",
    "\n",
    "### ‚≠ê CMS Star Ratings Impact\n",
    "\n",
    "Star Ratings (1-5 stars) impact bonus payments:\n",
    "- **5 stars**: Up to 5% bonus payment\n",
    "- **4+ stars**: Quality Bonus Payments (QBP)\n",
    "- Ratings based on quality measures (clinical outcomes, patient experience, access)\n",
    "\n",
    "### üìä HQRI Data Model\n",
    "\n",
    "For risk adjustment, key tables include:\n",
    "- **Encounters/Claims**: Medical services with diagnosis codes\n",
    "- **Diagnosis-to-HCC Mapping**: ICD-10 ‚Üí HCC crosswalk\n",
    "- **HCC Coefficients**: CMS model weights by year\n",
    "- **Members**: Demographics and enrollment status\n",
    "- **Risk Scores**: Calculated member-level risk scores\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Dimensional Model\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Data Vault\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "**Resources:**\n",
    "- [CMS Risk Adjustment Model](https://www.cms.gov/medicare/payment/medicare-advantage-rates-statistics/risk-adjustment)\n",
    "- [Implementing Dimensional Modeling on Databricks](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP\n",
    "\n",
    "Just run next couple of cells for setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"sunmin_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\"\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")\n",
    "\n",
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n",
    "\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze/Silver Layers ‚Äì Streamlined Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Overview: Simplified Bronze & Silver\n",
    "\n",
    "For HQRI analytics, we'll **streamline** Bronze and Silver layers to quickly get to Gold layer insights:\n",
    "\n",
    "### Bronze Layer (Raw Data Landing)\n",
    "- üìÇ Ingest encounter data \"as-is\" using `COPY INTO`\n",
    "- üíæ Store in Delta Lake for audit trails\n",
    "- ‚è±Ô∏è Maintain full history for compliance\n",
    "\n",
    "### Silver Layer (Clean & Validate)\n",
    "- üßπ Remove duplicates and validate data quality\n",
    "- üîÑ Map ICD-10 codes to HCC categories\n",
    "- ‚úÖ Apply business rules for CMS submission eligibility\n",
    "\n",
    "> **üí° Focus**: We'll execute Bronze/Silver steps efficiently so we can spend more time on **Gold layer analytics** that drive business value!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/sunmin_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load Claims Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/sunmin_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files.\n",
    "\n",
    "\n",
    "-- Load Diagnosis Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/sunmin_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Members Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/sunmin_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Procedures Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/sunmin_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Providers Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/sunmin_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/sunmin_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer ‚Äì Transforms for Risk Adjustment\n",
    "\n",
    "Now let's clean the data and prepare it for HCC risk scoring:\n",
    "\n",
    "**Key Silver Transformations:**\n",
    "- ‚úÖ Deduplicate encounters\n",
    "- ‚úÖ Validate diagnosis codes (ICD-10 format)\n",
    "- ‚úÖ Filter to eligible members (Medicare Advantage, active enrollment)\n",
    "- ‚úÖ Map diagnoses to HCC categories\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Speed Focus**: We'll run these transforms to move to Gold layer analytics!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bb21bd-564b-4bd0-a6f8-c956f766924f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf77187a-4958-4353-87d2-544486a45ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üéØ YOUR TURN! (3 mins)\n",
    "Ask Databricks Assistant: \"How do I calculate the total claims by specialty in SQL?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068e2f90-20d7-434b-bdf2-9d290f735df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    claim_status AS specialty,\n",
    "    SUM(total_charge) AS total_claims\n",
    "FROM sunmin_catalog.payer_silver.claims\n",
    "GROUP BY claim_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5a43d0-3c60-4504-b35e-b614e2ba0566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## üîÑ Why Migrate from SAS to Databricks?\n",
    "\n",
    "### Common HQRI Challenges with SAS\n",
    "\n",
    "Healthcare organizations using SAS for risk adjustment analytics face several limitations:\n",
    "\n",
    "1. **üêå Performance Bottlenecks**\n",
    "   - Single-server processing limits scalability\n",
    "   - Large member populations (millions of records) cause memory issues\n",
    "   - Batch processing takes hours or overnight\n",
    "\n",
    "2. **üí∞ Cost Concerns**\n",
    "   - Expensive annual licensing fees\n",
    "   - Additional costs for SAS/ACCESS, SAS Enterprise Guide, SAS Visual Analytics\n",
    "   - Hardware upgrades needed for growing data volumes\n",
    "\n",
    "3. **üîß Development Complexity**\n",
    "   - Multiple PROC steps required for simple operations\n",
    "   - Limited modern SQL features (no EXPLODE, limited window functions)\n",
    "   - SAS macros difficult to maintain and debug\n",
    "   - Separate tools needed for visualization and dashboards\n",
    "\n",
    "4. **‚òÅÔ∏è Cloud Migration Challenges**\n",
    "   - Legacy on-premises architecture\n",
    "   - Difficult integration with cloud data lakes\n",
    "   - Limited real-time analytics capabilities\n",
    "\n",
    "### Databricks Advantages for HQRI\n",
    "\n",
    "| **Capability** | **Impact** |\n",
    "|----------------|------------|\n",
    "| **Distributed Processing** | Handle billions of encounters with sub-second queries |\n",
    "| **Modern SQL** | Window functions, CTEs, EXPLODE - cleaner, more maintainable code |\n",
    "| **Unified Platform** | SQL, Python, R, dashboards, ML - all in one place |\n",
    "| **Delta Lake** | ACID transactions + time travel for audit compliance |\n",
    "| **Unity Catalog** | Row/column-level security for PHI/PII protection |\n",
    "| **Real-time Analytics** | Streaming + batch unified for immediate insights |\n",
    "| **Cost Efficiency** | Pay-per-use vs. fixed licensing, auto-scaling |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì HQRI Risk Adjustment Analytics\n",
    "\n",
    "---\n",
    "\n",
    "## What is the HQRI Gold Layer?\n",
    "\n",
    "The **Gold Layer** is where we deliver **business value** for HQRI. Here we create analytics tables that directly support:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Member-level HCC risk scores for CMS payments\n",
    "- üìä **Encounter Datamart**: CMS submission-ready data with quality validations\n",
    "- ‚≠ê **Star Ratings**: Quality metrics that impact bonus payments\n",
    "- üìà **Revenue Forecasting**: Projected payments based on risk scores\n",
    "- ‚úÖ **Compliance Audits**: Data quality metrics for regulatory requirements\n",
    "- üë• **Member Stratification**: High-risk vs. low-risk population segmentation\n",
    "- üè• **Provider Performance**: Risk capture rates by provider\n",
    "\n",
    "> **üéØ HQRI Value**: Each Gold table directly answers a business question that impacts revenue, compliance, or quality!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4b2b34d-0094-4ee7-829b-a22fd6250925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sunmin TODO: Add hands-on examples from Gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b0153-df99-4566-982a-745e97ff0888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 1: HCC Risk Score Calculation\n",
    "\n",
    "### üéØ Business Goal\n",
    "Calculate member-level risk scores based on HCC categories to determine CMS payments.\n",
    "\n",
    "**Risk Score Formula:**\n",
    "```\n",
    "Risk Score = Demographic Score + Œ£(HCC Coefficients)\n",
    "```\n",
    "\n",
    "### üìä SAS vs. Databricks Comparison\n",
    "\n",
    "Let's compare how this is traditionally done in **SAS** vs. **Databricks SQL** and **PySpark**:\n",
    "\n",
    "---\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: HCC Risk Score Calculation */\n",
    "/* Step 1: Create HCC reference table */\n",
    "DATA work.hcc_reference;\n",
    "    INPUT icd10_code $ 1-10 diagnosis_desc $ 12-50 hcc_category hcc_coefficient;\n",
    "    DATALINES;\n",
    "E11.9      Type 2 Diabetes                    19  0.318\n",
    "I50.9      Heart Failure                      85  0.368\n",
    "I10        Hypertension                       0   0.000\n",
    "J44.9      COPD                              111  0.328\n",
    "N18.3      CKD Stage 3                       138  0.237\n",
    ";\n",
    "RUN;\n",
    "\n",
    "/* Step 2: Merge claims with diagnoses and HCC mapping */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.diagnosis_with_hcc AS\n",
    "    SELECT \n",
    "        d.claim_id,\n",
    "        d.diagnosis_code,\n",
    "        d.diagnosis_desc,\n",
    "        h.hcc_category,\n",
    "        h.hcc_coefficient,\n",
    "        CASE \n",
    "            WHEN h.hcc_category IS NOT NULL AND h.hcc_category > 0 \n",
    "            THEN 1 ELSE 0 \n",
    "        END AS is_hcc\n",
    "    FROM work.diagnosis_raw AS d\n",
    "    LEFT JOIN work.hcc_reference AS h\n",
    "        ON UPCASE(STRIP(d.diagnosis_code)) = UPCASE(STRIP(h.icd10_code));\n",
    "QUIT;\n",
    "\n",
    "/* Step 3: Calculate member-level risk scores */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.member_hccs AS\n",
    "    SELECT DISTINCT\n",
    "        c.member_id,\n",
    "        m.first_name,\n",
    "        m.last_name,\n",
    "        m.birth_date,\n",
    "        m.gender,\n",
    "        m.plan_id,\n",
    "        dh.hcc_category,\n",
    "        dh.hcc_coefficient,\n",
    "        dh.diagnosis_code,\n",
    "        YEAR(TODAY()) - YEAR(m.birth_date) AS age\n",
    "    FROM work.claims AS c\n",
    "    INNER JOIN work.members AS m \n",
    "        ON c.member_id = m.member_id\n",
    "    INNER JOIN work.diagnosis_with_hcc AS dh \n",
    "        ON c.claim_id = dh.claim_id\n",
    "    WHERE dh.is_hcc = 1;\n",
    "QUIT;\n",
    "\n",
    "/* Step 4: Aggregate and calculate final risk scores */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.member_risk_scores AS\n",
    "    SELECT \n",
    "        member_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        birth_date,\n",
    "        gender,\n",
    "        plan_id,\n",
    "        age,\n",
    "        CASE \n",
    "            WHEN age < 65 THEN 0.350\n",
    "            WHEN age BETWEEN 65 AND 69 THEN 0.450\n",
    "            WHEN age BETWEEN 70 AND 74 THEN 0.550\n",
    "            WHEN age BETWEEN 75 AND 79 THEN 0.650\n",
    "            ELSE 0.750\n",
    "        END AS demographic_score,\n",
    "        SUM(hcc_coefficient) AS hcc_score,\n",
    "        COUNT(DISTINCT hcc_category) AS hcc_count,\n",
    "        CALCULATED demographic_score + CALCULATED hcc_score AS total_risk_score,\n",
    "        ROUND((CALCULATED demographic_score + CALCULATED hcc_score) * 10000, 0.01) \n",
    "            AS projected_annual_payment\n",
    "    FROM work.member_hccs\n",
    "    GROUP BY member_id, first_name, last_name, birth_date, gender, plan_id, age\n",
    "    ORDER BY total_risk_score DESC;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå Multiple PROC SQL steps required\n",
    "- ‚ùå Intermediate tables clutter WORK library\n",
    "- ‚ùå Limited scalability with large datasets\n",
    "- ‚ùå No automatic optimization or parallelization\n",
    "- ‚ùå Complex syntax for array aggregations (HCC categories list)\n",
    "\n",
    "---\n",
    "\n",
    "Now let's see how **Databricks SQL** simplifies this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da30fc9-0891-4c8a-8bbd-aae8687f1372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create gold schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_gold;\n",
    "\n",
    "-- Step 1a: Create HCC Mapping Reference Table (simulated for demo)\n",
    "CREATE OR REPLACE TABLE payer_gold.hcc_reference (\n",
    "  icd10_code STRING,\n",
    "  diagnosis_desc STRING,\n",
    "  hcc_category INT,\n",
    "  hcc_coefficient DOUBLE\n",
    ");\n",
    "\n",
    "INSERT INTO payer_gold.hcc_reference VALUES\n",
    "('E11.9', 'Type 2 Diabetes', 19, 0.318),\n",
    "('I50.9', 'Heart Failure', 85, 0.368),\n",
    "('I10', 'Hypertension', 0, 0.000),\n",
    "('J44.9', 'COPD', 111, 0.328),\n",
    "('N18.3', 'CKD Stage 3', 138, 0.237),\n",
    "('F32.9', 'Depression', 59, 0.309),\n",
    "('E78.5', 'Hyperlipidemia', 0, 0.000),\n",
    "('I25.10', 'CAD', 88, 0.184);\n",
    "\n",
    "-- Step 1b: Join Diagnoses to HCC Categories\n",
    "CREATE OR REPLACE TABLE payer_gold.diagnosis_with_hcc AS\n",
    "SELECT\n",
    "  d.claim_id,\n",
    "  d.diagnosis_code,\n",
    "  d.diagnosis_desc,\n",
    "  h.hcc_category,\n",
    "  h.hcc_coefficient,\n",
    "  CASE WHEN h.hcc_category IS NOT NULL AND h.hcc_category > 0 THEN 1 ELSE 0 END as is_hcc\n",
    "FROM payer_bronze.diagnosis_raw d\n",
    "LEFT JOIN payer_gold.hcc_reference h \n",
    "  ON UPPER(TRIM(d.diagnosis_code)) = UPPER(TRIM(h.icd10_code));\n",
    "\n",
    "-- Step 1c: Calculate Member-Level HCC Risk Scores\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_scores AS\n",
    "WITH member_hccs AS (\n",
    "  SELECT DISTINCT\n",
    "    c.member_id,\n",
    "    m.first_name,\n",
    "    m.last_name,\n",
    "    m.birth_date,\n",
    "    m.gender,\n",
    "    m.plan_id,\n",
    "    dh.hcc_category,\n",
    "    dh.hcc_coefficient,\n",
    "    dh.diagnosis_code,\n",
    "    YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as age\n",
    "  FROM payer_silver.claims c\n",
    "  INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "  INNER JOIN payer_gold.diagnosis_with_hcc dh ON c.claim_id = dh.claim_id\n",
    "  WHERE dh.is_hcc = 1\n",
    "),\n",
    "member_scores AS (\n",
    "  SELECT\n",
    "    member_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    birth_date,\n",
    "    gender,\n",
    "    plan_id,\n",
    "    age,\n",
    "    CASE \n",
    "      WHEN age < 65 THEN 0.350\n",
    "      WHEN age BETWEEN 65 AND 69 THEN 0.450\n",
    "      WHEN age BETWEEN 70 AND 74 THEN 0.550\n",
    "      WHEN age BETWEEN 75 AND 79 THEN 0.650\n",
    "      ELSE 0.750\n",
    "    END as demographic_score,\n",
    "    SUM(hcc_coefficient) as hcc_score,\n",
    "    COUNT(DISTINCT hcc_category) as hcc_count,\n",
    "    COLLECT_SET(hcc_category) as hcc_categories,\n",
    "    COLLECT_SET(diagnosis_code) as diagnosis_codes\n",
    "  FROM member_hccs\n",
    "  GROUP BY member_id, first_name, last_name, birth_date, gender, plan_id, age\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  demographic_score + hcc_score as total_risk_score,\n",
    "  ROUND((demographic_score + hcc_score) * 10000, 2) as projected_annual_payment\n",
    "FROM member_scores\n",
    "ORDER BY total_risk_score DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdd27aa-70ac-4293-ad4e-25c55b2cf9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üöÄ Databricks SQL Advantages\n",
    "\n",
    "**Databricks SQL Benefits:**\n",
    "- ‚úÖ **Single SQL Statement**: All logic in one CREATE TABLE AS with CTEs\n",
    "- ‚úÖ **Advanced Functions**: COLLECT_SET() for array aggregation (no SAS equivalent)\n",
    "- ‚úÖ **Automatic Optimization**: Query engine optimizes joins and aggregations\n",
    "- ‚úÖ **Scalability**: Distributed processing handles billions of rows\n",
    "- ‚úÖ **Unity Catalog**: Built-in governance, lineage tracking, and access control\n",
    "- ‚úÖ **Delta Lake**: ACID transactions, time travel, schema evolution\n",
    "- ‚úÖ **Real-time Refresh**: Can be scheduled or triggered automatically\n",
    "\n",
    "### üí° PySpark Alternative\n",
    "\n",
    "For complex business logic or programmatic control, use PySpark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0e827d-baf5-4ce5-a350-2b002d4ae251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark Example: HCC Risk Score Calculation\n",
    "from pyspark.sql.functions import (\n",
    "    col, year, current_date, sum as _sum, count, countDistinct,\n",
    "    collect_set, when, round as spark_round, lit\n",
    ")\n",
    "\n",
    "# Read tables\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "diagnosis_with_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# Join and calculate member HCCs\n",
    "member_hccs = claims \\\n",
    "    .join(members, \"member_id\") \\\n",
    "    .join(diagnosis_with_hcc, \"claim_id\") \\\n",
    "    .filter(col(\"is_hcc\") == 1) \\\n",
    "    .withColumn(\"age\", year(current_date()) - year(col(\"birth_date\"))) \\\n",
    "    .select(\n",
    "        \"member_id\", \"first_name\", \"last_name\", \"birth_date\", \"gender\", \n",
    "        \"plan_id\", \"age\", \"hcc_category\", \"hcc_coefficient\", \"diagnosis_code\"\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "# Calculate demographic scores and aggregate HCC scores\n",
    "member_risk_scores_pyspark = member_hccs \\\n",
    "    .withColumn(\"demographic_score\",\n",
    "        when(col(\"age\") < 65, 0.350)\n",
    "        .when(col(\"age\") <= 69, 0.450)\n",
    "        .when(col(\"age\") <= 74, 0.550)\n",
    "        .when(col(\"age\") <= 79, 0.650)\n",
    "        .otherwise(0.750)\n",
    "    ) \\\n",
    "    .groupBy(\"member_id\", \"first_name\", \"last_name\", \"birth_date\", \n",
    "             \"gender\", \"plan_id\", \"age\", \"demographic_score\") \\\n",
    "    .agg(\n",
    "        _sum(\"hcc_coefficient\").alias(\"hcc_score\"),\n",
    "        countDistinct(\"hcc_category\").alias(\"hcc_count\"),\n",
    "        collect_set(\"hcc_category\").alias(\"hcc_categories\"),\n",
    "        collect_set(\"diagnosis_code\").alias(\"diagnosis_codes\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_risk_score\", col(\"demographic_score\") + col(\"hcc_score\")) \\\n",
    "    .withColumn(\"projected_annual_payment\", \n",
    "                spark_round(col(\"total_risk_score\") * 10000, 2)) \\\n",
    "    .orderBy(col(\"total_risk_score\").desc())\n",
    "\n",
    "# Display results\n",
    "print(\"üìä PySpark Risk Score Calculation - Top 10 Members:\")\n",
    "display(member_risk_scores_pyspark.limit(10))\n",
    "\n",
    "# Optionally write to table\n",
    "# member_risk_scores_pyspark.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_gold.member_risk_scores_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76565f6d-ea54-4a63-a566-c9d4b5c13b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä View Risk Score Results\n",
    "\n",
    "Let's see the calculated risk scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4271f19b-4ce9-401e-adff-4e6590e98cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View top 20 members by risk score\n",
    "SELECT \n",
    "  member_id,\n",
    "  CONCAT(first_name, ' ', last_name) as member_name,\n",
    "  age,\n",
    "  gender,\n",
    "  hcc_count,\n",
    "  ROUND(demographic_score, 3) as demo_score,\n",
    "  ROUND(hcc_score, 3) as hcc_score,\n",
    "  ROUND(total_risk_score, 3) as risk_score,\n",
    "  projected_annual_payment\n",
    "FROM payer_gold.member_risk_scores\n",
    "ORDER BY total_risk_score DESC\n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c86dda2-5b70-4d4c-a7d9-d00af5376afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Revenue Forecast by Plan */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.revenue_forecast AS\n",
    "    SELECT \n",
    "        plan_id,\n",
    "        COUNT(DISTINCT member_id) AS total_members,\n",
    "        ROUND(AVG(total_risk_score), 0.001) AS avg_risk_score,\n",
    "        ROUND(MIN(total_risk_score), 0.001) AS min_risk_score,\n",
    "        ROUND(MAX(total_risk_score), 0.001) AS max_risk_score,\n",
    "        SUM(projected_annual_payment) AS total_projected_revenue,\n",
    "        ROUND(AVG(projected_annual_payment), 0.01) AS avg_payment_per_member,\n",
    "        SUM(CASE WHEN total_risk_score >= 1.5 THEN 1 ELSE 0 END) AS high_risk_members,\n",
    "        SUM(CASE WHEN total_risk_score < 1.0 THEN 1 ELSE 0 END) AS low_risk_members\n",
    "    FROM work.member_risk_scores\n",
    "    GROUP BY plan_id\n",
    "    ORDER BY total_projected_revenue DESC;\n",
    "QUIT;\n",
    "\n",
    "/* Export to Excel for reporting */\n",
    "PROC EXPORT DATA=work.revenue_forecast\n",
    "    OUTFILE='/path/to/revenue_forecast.xlsx'\n",
    "    DBMS=XLSX REPLACE;\n",
    "RUN;\n",
    "\n",
    "/* Generate summary report */\n",
    "PROC PRINT DATA=work.revenue_forecast;\n",
    "    TITLE 'Revenue Forecast by Plan';\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Limitations:**\n",
    "- ‚ùå Manual export steps for reporting\n",
    "- ‚ùå No real-time dashboard integration\n",
    "- ‚ùå Limited to single-server processing\n",
    "- ‚ùå Requires additional tools for visualization\n",
    "- ‚ùå Static reports need manual refresh\n",
    "\n",
    "#### Databricks SQL Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab5f7a1-4b7e-44d2-9a6f-53f06724be6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 2: Revenue Forecast & Impact Analysis\n",
    "\n",
    "### üí∞ Business Goal\n",
    "Project total CMS revenue based on risk scores to support financial planning.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Total member population\n",
    "- Average risk score\n",
    "- Projected annual revenue\n",
    "- Revenue by plan and risk tier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fdacb9-d31e-4121-a986-f40b754d9d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Revenue Forecast by Plan\n",
    "CREATE OR REPLACE TABLE payer_gold.revenue_forecast AS\n",
    "SELECT\n",
    "  plan_id,\n",
    "  COUNT(DISTINCT member_id) as total_members,\n",
    "  ROUND(AVG(total_risk_score), 3) as avg_risk_score,\n",
    "  ROUND(MIN(total_risk_score), 3) as min_risk_score,\n",
    "  ROUND(MAX(total_risk_score), 3) as max_risk_score,\n",
    "  SUM(projected_annual_payment) as total_projected_revenue,\n",
    "  ROUND(AVG(projected_annual_payment), 2) as avg_payment_per_member,\n",
    "  SUM(CASE WHEN total_risk_score >= 1.5 THEN 1 ELSE 0 END) as high_risk_members,\n",
    "  SUM(CASE WHEN total_risk_score < 1.0 THEN 1 ELSE 0 END) as low_risk_members\n",
    "FROM payer_gold.member_risk_scores\n",
    "GROUP BY plan_id\n",
    "ORDER BY total_projected_revenue DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f93350-2e88-4a92-9a76-ba5f16527568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Display revenue forecast\n",
    "SELECT * FROM payer_gold.revenue_forecast;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a03cfc-dc0a-4ee0-882d-12d58d31d264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: HCC Distribution Analysis */\n",
    "/* Step 1: Unnest HCC categories from member_risk_scores */\n",
    "/* Note: SAS doesn't have native array explosion like SQL EXPLODE */\n",
    "/* Must use DATA step with ARRAY processing */\n",
    "\n",
    "DATA work.hcc_exploded;\n",
    "    SET work.member_risk_scores;\n",
    "    ARRAY hccs hcc_cat1-hcc_cat10;  /* Assumes max 10 HCCs per member */\n",
    "    \n",
    "    DO i = 1 TO DIM(hccs);\n",
    "        IF hccs[i] NE . THEN DO;\n",
    "            hcc_category = hccs[i];\n",
    "            OUTPUT;\n",
    "        END;\n",
    "    END;\n",
    "    DROP hcc_cat1-hcc_cat10 i;\n",
    "RUN;\n",
    "\n",
    "/* Step 2: Join with HCC reference and aggregate */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.hcc_distribution AS\n",
    "    SELECT \n",
    "        he.hcc_category,\n",
    "        r.diagnosis_desc,\n",
    "        r.hcc_coefficient,\n",
    "        COUNT(DISTINCT he.member_id) AS member_count,\n",
    "        COUNT(DISTINCT he.plan_id) AS plan_count,\n",
    "        ROUND(r.hcc_coefficient * COUNT(DISTINCT he.member_id) * 10000, 0.01) \n",
    "            AS total_revenue_impact,\n",
    "        ROUND(r.hcc_coefficient * 10000, 0.01) AS revenue_per_member\n",
    "    FROM work.hcc_exploded AS he\n",
    "    INNER JOIN work.hcc_reference AS r \n",
    "        ON he.hcc_category = r.hcc_category\n",
    "    GROUP BY he.hcc_category, r.diagnosis_desc, r.hcc_coefficient\n",
    "    ORDER BY total_revenue_impact DESC;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå No native EXPLODE function - requires manual array processing\n",
    "- ‚ùå Must pre-define array size (max HCCs per member)\n",
    "- ‚ùå Two-step process: DATA step + PROC SQL\n",
    "- ‚ùå Complex logic for dynamic array sizes\n",
    "- ‚ùå Performance issues with large datasets\n",
    "\n",
    "#### Databricks SQL Approach - Single Statement!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c1af5f-5c26-4fb6-8883-0ee0abd51e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 3: HCC Distribution Analysis\n",
    "\n",
    "### üìà Business Goal\n",
    "Understand which HCC categories drive the most revenue and identify coding opportunities.\n",
    "\n",
    "This helps HQRI:\n",
    "- Identify high-value diagnoses for provider education\n",
    "- Monitor HCC capture rates\n",
    "- Find gaps in documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039c2881-714f-4efb-84fd-da41099699a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- HCC Category Distribution and Revenue Impact\n",
    "CREATE OR REPLACE TABLE payer_gold.hcc_distribution AS\n",
    "WITH hcc_exploded AS (\n",
    "  SELECT\n",
    "    member_id,\n",
    "    plan_id,\n",
    "    EXPLODE(hcc_categories) as hcc_category\n",
    "  FROM payer_gold.member_risk_scores\n",
    ")\n",
    "SELECT\n",
    "  he.hcc_category,\n",
    "  r.diagnosis_desc,\n",
    "  r.hcc_coefficient,\n",
    "  COUNT(DISTINCT he.member_id) as member_count,\n",
    "  COUNT(DISTINCT he.plan_id) as plan_count,\n",
    "  ROUND(r.hcc_coefficient * COUNT(DISTINCT he.member_id) * 10000, 2) as total_revenue_impact,\n",
    "  ROUND(r.hcc_coefficient * 10000, 2) as revenue_per_member\n",
    "FROM hcc_exploded he\n",
    "INNER JOIN payer_gold.hcc_reference r ON he.hcc_category = r.hcc_category\n",
    "GROUP BY he.hcc_category, r.diagnosis_desc, r.hcc_coefficient\n",
    "ORDER BY total_revenue_impact DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02aeb28f-1a1c-498b-9943-59d75884fa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View HCC distribution\n",
    "SELECT * FROM payer_gold.hcc_distribution;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2231a1-375a-4c6c-a787-9c159e15f97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 4: Data Quality & Compliance Audit\n",
    "\n",
    "### ‚úÖ Business Goal\n",
    "Ensure encounter data meets CMS submission standards and identify data quality issues.\n",
    "\n",
    "**CMS Requirements:**\n",
    "- Valid diagnosis codes (ICD-10 format)\n",
    "- Complete member demographics\n",
    "- Valid provider NPIs\n",
    "- Service dates within coverage period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6bc87b-c322-48f5-a718-bc91ec71a615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Data Quality Audit - Requires Multiple Queries */\n",
    "\n",
    "/* Query 1: Total Encounters */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_total AS\n",
    "    SELECT \n",
    "        'Total Encounters' AS metric,\n",
    "        COUNT(*) AS record_count,\n",
    "        . AS pct_of_total,\n",
    "        'INFO' AS severity\n",
    "    FROM work.claims;\n",
    "QUIT;\n",
    "\n",
    "/* Query 2: Encounters Missing Diagnosis */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_missing_dx AS\n",
    "    SELECT \n",
    "        'Encounters Missing Diagnosis' AS metric,\n",
    "        COUNT(DISTINCT c.claim_id) AS record_count,\n",
    "        CALCULATED record_count * 100.0 / \n",
    "            (SELECT COUNT(*) FROM work.claims) AS pct_of_total,\n",
    "        'ERROR' AS severity\n",
    "    FROM work.claims AS c\n",
    "    LEFT JOIN work.diagnosis_raw AS d \n",
    "        ON c.claim_id = d.claim_id\n",
    "    WHERE d.claim_id IS NULL;\n",
    "QUIT;\n",
    "\n",
    "/* Query 3: HCC Mapping Rate */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_hcc_mapped AS\n",
    "    SELECT \n",
    "        'Diagnoses Mapped to HCC' AS metric,\n",
    "        COUNT(*) AS record_count,\n",
    "        CALCULATED record_count * 100.0 / \n",
    "            (SELECT COUNT(*) FROM work.diagnosis_raw) AS pct_of_total,\n",
    "        'INFO' AS severity\n",
    "    FROM work.diagnosis_with_hcc\n",
    "    WHERE is_hcc = 1;\n",
    "QUIT;\n",
    "\n",
    "/* Repeat for remaining checks... */\n",
    "\n",
    "/* Combine all audit results */\n",
    "DATA work.data_quality_audit;\n",
    "    SET work.audit_total\n",
    "        work.audit_missing_dx\n",
    "        work.audit_hcc_mapped\n",
    "        /* ... other audit tables ... */;\n",
    "        \n",
    "    /* Add status flag */\n",
    "    IF severity = 'ERROR' AND record_count > 0 THEN status = 'FAIL';\n",
    "    ELSE IF severity = 'WARNING' AND pct_of_total > 5 THEN status = 'REVIEW';\n",
    "    ELSE status = 'PASS';\n",
    "RUN;\n",
    "\n",
    "/* Sort and display */\n",
    "PROC SORT DATA=work.data_quality_audit;\n",
    "    BY severity DESCENDING pct_of_total;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå Must create separate queries for each audit check\n",
    "- ‚ùå Manual combination with DATA step\n",
    "- ‚ùå Multiple intermediate tables clutter workspace\n",
    "- ‚ùå Difficult to maintain as audit rules grow\n",
    "- ‚ùå No UNION ALL equivalent in single PROC SQL\n",
    "- ‚ùå Subquery limitations in SELECT clause\n",
    "\n",
    "#### Databricks SQL Approach - Elegant & Maintainable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838cab28-b5ff-4ac0-b676-a4563be46845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Data Quality Audit for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.data_quality_audit AS\n",
    "WITH encounter_checks AS (\n",
    "  SELECT\n",
    "    'Total Encounters' as metric,\n",
    "    COUNT(*) as record_count,\n",
    "    NULL as pct_of_total,\n",
    "    'INFO' as severity\n",
    "  FROM payer_silver.claims\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Encounters Missing Diagnosis',\n",
    "    COUNT(DISTINCT c.claim_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT c.claim_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.claims c\n",
    "  LEFT JOIN payer_bronze.diagnosis_raw d ON c.claim_id = d.claim_id\n",
    "  WHERE d.claim_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Diagnoses Mapped to HCC',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_bronze.diagnosis_raw),\n",
    "      2\n",
    "    ),\n",
    "    'INFO'\n",
    "  FROM payer_gold.diagnosis_with_hcc\n",
    "  WHERE is_hcc = 1\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Members Without Risk Scores',\n",
    "    COUNT(DISTINCT m.member_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT m.member_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.members),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.members m\n",
    "  LEFT JOIN payer_gold.member_risk_scores r ON m.member_id = r.member_id\n",
    "  WHERE r.member_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Providers Missing NPI',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.providers),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.providers\n",
    "  WHERE npi IS NULL OR TRIM(npi) = ''\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Claims with Invalid Status',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.claims\n",
    "  WHERE claim_status NOT IN ('approved', 'paid', 'pending')\n",
    ")\n",
    "SELECT\n",
    "  metric,\n",
    "  record_count,\n",
    "  COALESCE(pct_of_total, 0.0) as pct_of_total,\n",
    "  severity,\n",
    "  CASE\n",
    "    WHEN severity = 'ERROR' AND record_count > 0 THEN 'FAIL'\n",
    "    WHEN severity = 'WARNING' AND pct_of_total > 5 THEN 'REVIEW'\n",
    "    ELSE 'PASS'\n",
    "  END as status\n",
    "FROM encounter_checks\n",
    "ORDER BY\n",
    "  CASE severity WHEN 'ERROR' THEN 1 WHEN 'WARNING' THEN 2 ELSE 3 END,\n",
    "  pct_of_total DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f520f3f-5bd9-43cf-891f-d3849359f485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View data quality audit results\n",
    "SELECT * FROM payer_gold.data_quality_audit;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9961268c-b1f3-4604-bc54-0f427c2db0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 5: Member Risk Stratification\n",
    "\n",
    "### üë• Business Goal\n",
    "Segment members by risk level to support care management and intervention programs.\n",
    "\n",
    "**Risk Tiers:**\n",
    "- **Very High Risk** (Score > 2.0): Intensive care management\n",
    "- **High Risk** (Score 1.5-2.0): Enhanced monitoring\n",
    "- **Moderate Risk** (Score 1.0-1.5): Standard care\n",
    "- **Low Risk** (Score < 1.0): Preventive care focus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7396318d-098e-4644-b0bc-102527210d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Member Risk Stratification\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_stratification AS\n",
    "SELECT\n",
    "  member_id,\n",
    "  CONCAT(first_name, ' ', last_name) as member_name,\n",
    "  age,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  total_risk_score,\n",
    "  hcc_count,\n",
    "  projected_annual_payment,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Very High Risk'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'High Risk'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Moderate Risk'\n",
    "    ELSE 'Low Risk'\n",
    "  END as risk_tier,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Intensive Care Management Required'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'Enhanced Monitoring Recommended'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Standard Care Protocol'\n",
    "    ELSE 'Preventive Care Focus'\n",
    "  END as care_recommendation,\n",
    "  hcc_categories as active_hcc_list\n",
    "FROM payer_gold.member_risk_scores;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae8461-2e2e-4566-a63d-d6e941b147dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Summary statistics by risk tier\n",
    "SELECT\n",
    "  risk_tier,\n",
    "  COUNT(*) as member_count,\n",
    "  ROUND(AVG(age), 1) as avg_age,\n",
    "  ROUND(AVG(total_risk_score), 3) as avg_risk_score,\n",
    "  ROUND(AVG(hcc_count), 1) as avg_hcc_count,\n",
    "  SUM(projected_annual_payment) as total_revenue,\n",
    "  ROUND(AVG(projected_annual_payment), 2) as avg_payment_per_member\n",
    "FROM payer_gold.member_risk_stratification\n",
    "GROUP BY risk_tier\n",
    "ORDER BY avg_risk_score DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9548ad6-8c71-45aa-9c7f-28e5075553e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. PySpark Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Provider Performance on Risk Capture */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.provider_performance AS\n",
    "    SELECT \n",
    "        p.provider_id,\n",
    "        p.provider_name,\n",
    "        p.specialty,\n",
    "        p.city,\n",
    "        p.state,\n",
    "        COUNT(DISTINCT c.member_id) AS unique_members,\n",
    "        COUNT(c.claim_id) AS total_encounters,\n",
    "        ROUND(AVG(m.total_risk_score), 0.001) AS avg_member_risk_score,\n",
    "        SUM(dh.hcc_coefficient) AS total_hcc_value,\n",
    "        COUNT(DISTINCT dh.hcc_category) AS unique_hccs_captured,\n",
    "        ROUND(SUM(m.projected_annual_payment), 0.01) AS attributed_revenue,\n",
    "        ROUND(COUNT(DISTINCT dh.hcc_category) / COUNT(DISTINCT c.member_id), 0.01) \n",
    "            AS hcc_capture_rate\n",
    "    FROM work.claims AS c\n",
    "    INNER JOIN work.providers AS p \n",
    "        ON c.provider_id = p.provider_id\n",
    "    INNER JOIN work.member_risk_scores AS m \n",
    "        ON c.member_id = m.member_id\n",
    "    INNER JOIN work.diagnosis_with_hcc AS dh \n",
    "        ON c.claim_id = dh.claim_id\n",
    "    GROUP BY p.provider_id, p.provider_name, p.specialty, p.city, p.state\n",
    "    ORDER BY attributed_revenue DESC;\n",
    "QUIT;\n",
    "\n",
    "/* Create top providers report */\n",
    "PROC PRINT DATA=work.provider_performance(OBS=20);\n",
    "    TITLE 'Top 20 Providers by Risk Capture Performance';\n",
    "    VAR provider_name specialty unique_members avg_member_risk_score \n",
    "        unique_hccs_captured attributed_revenue hcc_capture_rate;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Limitations:**\n",
    "- ‚ùå Must calculate derived metrics (hcc_capture_rate) in the same SELECT\n",
    "- ‚ùå No withColumn() equivalent for cleaner syntax\n",
    "- ‚ùå Limited to single-server memory for large joins\n",
    "- ‚ùå Static output - no interactive display\n",
    "- ‚ùå Requires separate PROC PRINT for visualization\n",
    "\n",
    "#### PySpark Approach - More Flexible & Scalable!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291bd675-7d9a-4f47-9cb4-6d1d93148a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 6: Provider Performance on Risk Capture\n",
    "\n",
    "### üè• Business Goal\n",
    "Identify which providers excel at documenting HCC conditions to guide provider education.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Members per provider\n",
    "- Average risk score of provider's panel\n",
    "- HCC capture rate\n",
    "- Revenue attributed to provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3661be1-6e9a-4ba1-9334-8aea8f02f3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, avg, sum, round as spark_round, col\n",
    "\n",
    "# Provider Performance Analysis\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "members = spark.table(\"payer_gold.member_risk_scores\")\n",
    "diagnosis_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# Join claims with risk scores\n",
    "provider_performance = claims \\\n",
    "    .join(providers, \"provider_id\") \\\n",
    "    .join(members, \"member_id\") \\\n",
    "    .join(diagnosis_hcc, \"claim_id\") \\\n",
    "    .groupBy(\n",
    "        \"provider_id\",\n",
    "        \"provider_name\",\n",
    "        \"specialty\",\n",
    "        \"city\",\n",
    "        \"state\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        countDistinct(\"member_id\").alias(\"unique_members\"),\n",
    "        count(\"claim_id\").alias(\"total_encounters\"),\n",
    "        spark_round(avg(\"total_risk_score\"), 3).alias(\"avg_member_risk_score\"),\n",
    "        sum(col(\"hcc_coefficient\")).alias(\"total_hcc_value\"),\n",
    "        countDistinct(col(\"hcc_category\")).alias(\"unique_hccs_captured\"),\n",
    "        spark_round(sum(\"projected_annual_payment\"), 2).alias(\"attributed_revenue\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"hcc_capture_rate\",\n",
    "        spark_round(col(\"unique_hccs_captured\") / col(\"unique_members\"), 2)\n",
    "    ) \\\n",
    "    .orderBy(col(\"attributed_revenue\").desc())\n",
    "\n",
    "# Display top providers\n",
    "print(\"üè• Top 20 Providers by Risk Capture Performance:\")\n",
    "display(provider_performance.limit(20))\n",
    "\n",
    "# Save to Gold table\n",
    "provider_performance.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.provider_risk_capture_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d7d1eb-2e5f-494a-9bea-b64353481adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 7: Encounter Datamart for CMS Submission\n",
    "\n",
    "### üìä Business Goal\n",
    "Create a CMS-ready encounter datamart with all required fields and validations.\n",
    "\n",
    "**CMS Submission Requirements:**\n",
    "- Valid member enrollment\n",
    "- Complete encounter details (dates, provider, diagnosis)\n",
    "- Proper diagnosis code formatting (ICD-10)\n",
    "- Service within coverage period\n",
    "- Provider has valid NPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d226a329-1f6a-438f-b250-6f4c57b17097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üîÑ Migration Summary: SAS to Databricks\n",
    "\n",
    "### Key Advantages of Databricks Over SAS for Gold Layer Analytics\n",
    "\n",
    "| Feature | SAS | Databricks SQL/PySpark |\n",
    "|---------|-----|------------------------|\n",
    "| **Array Operations** | Manual ARRAY processing | Native COLLECT_SET(), EXPLODE() |\n",
    "| **Query Complexity** | Multiple PROC SQL steps | Single CTE-based queries |\n",
    "| **Scalability** | Single-server memory limits | Distributed processing (petabyte scale) |\n",
    "| **Modern Functions** | Limited window functions | Full SQL:2016 compliance |\n",
    "| **Real-time Analytics** | Batch-only | Streaming + batch unified |\n",
    "| **Visualization** | Separate tools (PROC GPLOT, SAS VA) | Built-in interactive dashboards |\n",
    "| **Governance** | Manual security setup | Unity Catalog (row/column security) |\n",
    "| **Version Control** | Limited | Full Delta Lake time travel |\n",
    "| **Cloud Native** | Legacy architecture | Modern cloud-optimized |\n",
    "| **Cost** | Expensive licensing | Pay-per-use consumption |\n",
    "| **Programming** | SAS language only | SQL, Python, R, Scala, Java |\n",
    "\n",
    "### üí° Migration Best Practices\n",
    "\n",
    "1. **Start with Simple Queries**: Begin with straightforward PROC SQL ‚Üí Databricks SQL translations\n",
    "2. **Leverage CTEs**: Replace multi-step SAS DATA/PROC steps with Common Table Expressions\n",
    "3. **Use PySpark for Complex Logic**: When business rules are complex, PySpark offers more flexibility than SAS macros\n",
    "4. **Embrace Modern Functions**: COLLECT_SET, EXPLODE, and window functions simplify code\n",
    "5. **Unity Catalog = SAS Library**: Map SAS libraries to Unity Catalog schemas\n",
    "6. **Delta Lake = SAS Datasets**: Delta tables provide ACID transactions like SAS datasets, but at scale\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169b5140-87ca-486b-8256-1b5edac71e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Sunmin: what is this?\n",
    "\n",
    "-- Encounter Datamart for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.encounter_datamart_cms AS\n",
    "SELECT\n",
    "  -- Encounter identifiers\n",
    "  c.claim_id as encounter_id,\n",
    "  c.claim_date as encounter_date,\n",
    "  c.claim_status as encounter_status,\n",
    "  \n",
    "  -- Member information\n",
    "  m.member_id,\n",
    "  m.first_name,\n",
    "  m.last_name,\n",
    "  m.birth_date,\n",
    "  m.gender,\n",
    "  m.plan_id,\n",
    "  YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as member_age,\n",
    "  \n",
    "  -- Provider information\n",
    "  p.provider_id,\n",
    "  p.npi as provider_npi,\n",
    "  p.provider_name,\n",
    "  p.specialty as provider_specialty,\n",
    "  p.state as provider_state,\n",
    "  \n",
    "  -- Diagnosis information\n",
    "  d.diagnosis_code as icd10_code,\n",
    "  d.diagnosis_desc,\n",
    "  d.hcc_category,\n",
    "  d.hcc_coefficient,\n",
    "  d.is_hcc,\n",
    "  \n",
    "  -- Claim financial\n",
    "  c.total_charge,\n",
    "  \n",
    "  -- Data quality flags\n",
    "  CASE \n",
    "    WHEN m.member_id IS NULL THEN 'FAIL: Missing Member'\n",
    "    WHEN p.npi IS NULL OR TRIM(p.npi) = '' THEN 'FAIL: Missing Provider NPI'\n",
    "    WHEN d.diagnosis_code IS NULL THEN 'FAIL: Missing Diagnosis'\n",
    "    WHEN c.claim_date < m.effective_date THEN 'FAIL: Service Before Coverage'\n",
    "    WHEN c.claim_status NOT IN ('approved', 'paid') THEN 'WARNING: Invalid Status'\n",
    "    ELSE 'PASS'\n",
    "  END as submission_validation_status,\n",
    "  \n",
    "  -- Submission flag\n",
    "  CASE \n",
    "    WHEN m.member_id IS NOT NULL \n",
    "     AND p.npi IS NOT NULL \n",
    "     AND TRIM(p.npi) != ''\n",
    "     AND d.diagnosis_code IS NOT NULL\n",
    "     AND c.claim_date >= m.effective_date\n",
    "     AND c.claim_status IN ('approved', 'paid')\n",
    "    THEN 1 \n",
    "    ELSE 0 \n",
    "  END as cms_submission_ready,\n",
    "  \n",
    "  CURRENT_TIMESTAMP() as datamart_created_at\n",
    "  \n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "LEFT JOIN payer_gold.diagnosis_with_hcc d ON c.claim_id = d.claim_id;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16171ad-f5b3-4528-9536-9accf5826903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Sunmin: what is this?\n",
    "\n",
    "-- CMS Submission Readiness Summary\n",
    "SELECT\n",
    "  submission_validation_status,\n",
    "  COUNT(*) as encounter_count,\n",
    "  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct_of_total,\n",
    "  SUM(cms_submission_ready) as ready_for_submission\n",
    "FROM payer_gold.encounter_datamart_cms\n",
    "GROUP BY submission_validation_status\n",
    "ORDER BY encounter_count DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67c5a5c9-a41d-45fe-8ec2-6895f8a9512c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcd33ec-d92b-4ce0-b1db-d0738d9fb90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a98a2-db41-4a69-b0ec-9876ee60ccfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Databricks Healthcare Payer Analytics Workshop! Let's review what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished\n",
    "\n",
    "### 1. **Medallion Architecture**\n",
    "- ‚úÖ Built a complete **Bronze ‚Üí Silver ‚Üí Gold** pipeline\n",
    "- ‚úÖ Understood data quality improvement at each layer\n",
    "- ‚úÖ Created analytics-ready datasets\n",
    "\n",
    "### 2. **Data Engineering Skills**\n",
    "- ‚úÖ Loaded data using **COPY INTO**\n",
    "- ‚úÖ Transformed data with **SQL and PySpark**\n",
    "- ‚úÖ Applied data quality checks and validations\n",
    "- ‚úÖ Created aggregations and derived metrics\n",
    "\n",
    "### 3. **Analytics & Visualization**\n",
    "- ‚úÖ Generated business insights from data\n",
    "- ‚úÖ Created interactive visualizations\n",
    "- ‚úÖ Performed statistical analysis\n",
    "- ‚úÖ Built executive dashboards\n",
    "\n",
    "### 4. **Databricks Platform**\n",
    "- ‚úÖ Worked with **Unity Catalog**\n",
    "- ‚úÖ Used **Delta Lake** for reliable data storage\n",
    "- ‚úÖ Leveraged **AI Assistant** for code help\n",
    "- ‚úÖ Applied performance optimization techniques\n",
    "\n",
    "### 5. **Refer to the Best Practices!**\n",
    "Best practices notebook: _**[Reference] Best Practices**_\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3734562717510899,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBX Workshop_HQRI_11142025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "sunmin_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sunmin_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sunmin_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
