{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676565f7-bc51-4b21-8fba-603008447446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Updated date: November 14, 2025_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks Workshop: HQRI Risk Adjustment & Analytics\n",
    "**Healthcare Quality Reporting & Improvement - DBSQL Analytics Workshop**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Build **Medallion Architecture** pipelines for risk adjustment data\n",
    "2. ‚úÖ Calculate **HCC (Hierarchical Condition Category) risk scores** for Medicare Advantage\n",
    "3. ‚úÖ Create **encounter datamart** tables for CMS submissions\n",
    "4. ‚úÖ Perform **data quality audits** for regulatory compliance\n",
    "5. ‚úÖ Build **Gold layer analytics** for revenue impact and Star Ratings\n",
    "6. ‚úÖ Forecast risk scores using **Databricks SQL (DBSQL)**\n",
    "\n",
    "---\n",
    "\n",
    "## üè• HQRI Use Case: Medicare Risk Adjustment\n",
    "\n",
    "**HQRI (Healthcare Quality Reporting & Improvement)** manages Humana's risk adjustment and quality reporting for Medicare Advantage:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Determine CMS payments based on member health status\n",
    "- üìä **Encounter Datamart**: Aggregate and validate claims/encounters for CMS\n",
    "- ‚≠ê **Star Ratings**: Drive quality metrics that impact bonus payments\n",
    "- ‚úÖ **Compliance & Audits**: Ensure federal regulatory compliance\n",
    "- üìà **Revenue Optimization**: Accurate coding and forecasting maximize financial performance\n",
    "\n",
    "### üóÇÔ∏è Dataset Overview\n",
    "\n",
    "We'll work with **Medicare risk adjustment data**:\n",
    "- **Members**: Medicare Advantage enrollees with demographics\n",
    "- **Claims**: Medical encounters with diagnosis codes\n",
    "- **Diagnoses**: ICD-10 codes mapped to HCC categories\n",
    "- **Providers**: Healthcare providers submitting encounters\n",
    "- **Procedures**: Services rendered and billed\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce4ebf7-dc51-44bc-8bf2-804fec60b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Medallion Pipeline for a Healthcare Payer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Risk Adjustment & HCC Modeling Concepts\n",
    "\n",
    "### üè• Medicare Risk Adjustment Overview\n",
    "\n",
    "CMS uses **HCC (Hierarchical Condition Category)** models to adjust capitation payments based on beneficiary health status:\n",
    "\n",
    "1. **ICD-10 Diagnosis Codes** ‚Üí mapped to **HCC categories**\n",
    "2. **HCC categories** ‚Üí assigned **coefficient weights**\n",
    "3. **Risk Score** = Sum of HCC weights + demographic factors (age, sex, disability status)\n",
    "4. **Payment** = Base rate √ó Risk Score\n",
    "\n",
    "**Example:**\n",
    "- Member with diabetes (HCC 19, weight 0.318) and heart failure (HCC 85, weight 0.368)\n",
    "- Base demographic score: 0.500\n",
    "- **Total Risk Score = 0.500 + 0.318 + 0.368 = 1.186**\n",
    "- If base payment = $10,000, **CMS pays: $10,000 √ó 1.186 = $11,860**\n",
    "\n",
    "### ‚≠ê CMS Star Ratings Impact\n",
    "\n",
    "Star Ratings (1-5 stars) impact bonus payments:\n",
    "- **5 stars**: Up to 5% bonus payment\n",
    "- **4+ stars**: Quality Bonus Payments (QBP)\n",
    "- Ratings based on quality measures (clinical outcomes, patient experience, access)\n",
    "\n",
    "### üìä HQRI Data Model\n",
    "\n",
    "For risk adjustment, key tables include:\n",
    "- **Encounters/Claims**: Medical services with diagnosis codes\n",
    "- **Diagnosis-to-HCC Mapping**: ICD-10 ‚Üí HCC crosswalk\n",
    "- **HCC Coefficients**: CMS model weights by year\n",
    "- **Members**: Demographics and enrollment status\n",
    "- **Risk Scores**: Calculated member-level risk scores\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Dimensional Model\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Data Vault\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "**Resources:**\n",
    "- [CMS Risk Adjustment Model](https://www.cms.gov/medicare/payment/medicare-advantage-rates-statistics/risk-adjustment)\n",
    "- [Implementing Dimensional Modeling on Databricks](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e582d5a-8e97-4f72-86ae-3ef684662918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# üîÑ Why Migrate from SAS to Databricks?\n",
    "\n",
    "### Common HQRI Challenges with SAS\n",
    "\n",
    "Healthcare organizations using SAS for risk adjustment analytics face several limitations:\n",
    "\n",
    "1. **üêå Performance Bottlenecks**\n",
    "   - Single-server processing limits scalability\n",
    "   - Large member populations (millions of records) cause memory issues\n",
    "   - Batch processing takes hours or overnight\n",
    "\n",
    "2. **üí∞ Cost Concerns**\n",
    "   - Expensive annual licensing fees\n",
    "   - Additional costs for SAS/ACCESS, SAS Enterprise Guide, SAS Visual Analytics\n",
    "   - Hardware upgrades needed for growing data volumes\n",
    "\n",
    "3. **üîß Development Complexity**\n",
    "   - Multiple PROC steps required for simple operations\n",
    "   - Limited modern SQL features (no EXPLODE, limited window functions)\n",
    "   - SAS macros difficult to maintain and debug\n",
    "   - Separate tools needed for visualization and dashboards\n",
    "\n",
    "4. **‚òÅÔ∏è Cloud Migration Challenges**\n",
    "   - Legacy on-premises architecture\n",
    "   - Difficult integration with cloud data lakes\n",
    "   - Limited real-time analytics capabilities\n",
    "\n",
    "### Databricks Advantages for HQRI\n",
    "\n",
    "| **Capability** | **Impact** |\n",
    "|----------------|------------|\n",
    "| **Distributed Processing** | Handle billions of encounters with sub-second queries |\n",
    "| **Modern SQL** | Window functions, CTEs, EXPLODE - cleaner, more maintainable code |\n",
    "| **Unified Platform** | SQL, Python, R, dashboards, ML - all in one place |\n",
    "| **Delta Lake** | ACID transactions + time travel for audit compliance |\n",
    "| **Unity Catalog** | Row/column-level security for PHI/PII protection |\n",
    "| **Real-time Analytics** | Streaming + batch unified for immediate insights |\n",
    "| **Cost Efficiency** | Pay-per-use vs. fixed licensing, auto-scaling |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP\n",
    "\n",
    "Just run next couple of cells for setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"my_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\"\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")\n",
    "\n",
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n",
    "\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze/Silver Layers ‚Äì Streamlined Data Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Overview: Simplified Bronze & Silver\n",
    "\n",
    "For HQRI analytics, we'll **streamline** Bronze and Silver layers to quickly get to Gold layer insights:\n",
    "\n",
    "### Bronze Layer (Raw Data Landing)\n",
    "- üìÇ Ingest encounter data \"as-is\" using `COPY INTO`\n",
    "- üíæ Store in Delta Lake for audit trails\n",
    "- ‚è±Ô∏è Maintain full history for compliance\n",
    "\n",
    "### Silver Layer (Clean & Validate)\n",
    "- üßπ Remove duplicates and validate data quality\n",
    "- üîÑ Map ICD-10 codes to HCC categories\n",
    "- ‚úÖ Apply business rules for CMS submission eligibility\n",
    "\n",
    "> **üí° Focus**: We'll execute Bronze/Silver steps efficiently so we can spend more time on **Gold layer analytics** that drive business value!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/my_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load Claims Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files.\n",
    "\n",
    "\n",
    "-- Load Diagnosis Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Members Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Procedures Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Providers Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/my_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer ‚Äì Transforms for Risk Adjustment\n",
    "\n",
    "Now let's clean the data and prepare it for HCC risk scoring:\n",
    "\n",
    "**Key Silver Transformations:**\n",
    "- ‚úÖ Deduplicate encounters\n",
    "- ‚úÖ Validate diagnosis codes (ICD-10 format)\n",
    "- ‚úÖ Filter to eligible members (Medicare Advantage, active enrollment)\n",
    "- ‚úÖ Map diagnoses to HCC categories\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Speed Focus**: We'll run these transforms to move to Gold layer analytics!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bb21bd-564b-4bd0-a6f8-c956f766924f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf77187a-4958-4353-87d2-544486a45ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üéØ YOUR TURN! (3 mins)\n",
    "Ask Databricks Assistant: \"How do I calculate the total claims by specialty in SQL?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì HQRI Risk Adjustment Analytics\n",
    "\n",
    "---\n",
    "\n",
    "## What is the HQRI Gold Layer?\n",
    "\n",
    "The **Gold Layer** is where we deliver **business value** for HQRI. Here we create analytics tables that directly support:\n",
    "\n",
    "- üí∞ **Risk Score Calculations**: Member-level HCC risk scores for CMS payments\n",
    "- üìä **Encounter Datamart**: CMS submission-ready data with quality validations\n",
    "- ‚≠ê **Star Ratings**: Quality metrics that impact bonus payments\n",
    "- üìà **Revenue Forecasting**: Projected payments based on risk scores\n",
    "- ‚úÖ **Compliance Audits**: Data quality metrics for regulatory requirements\n",
    "- üë• **Member Stratification**: High-risk vs. low-risk population segmentation\n",
    "- üè• **Provider Performance**: Risk capture rates by provider\n",
    "\n",
    "> **üéØ HQRI Value**: Each Gold table directly answers a business question that impacts revenue, compliance, or quality!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b0153-df99-4566-982a-745e97ff0888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 1: HCC Risk Score Calculation\n",
    "\n",
    "### üéØ Business Goal\n",
    "Calculate member-level risk scores based on HCC categories to determine CMS payments.\n",
    "\n",
    "**Risk Score Formula:**\n",
    "```\n",
    "Risk Score = Demographic Score + Œ£(HCC Coefficients)\n",
    "```\n",
    "\n",
    "### üìä SAS vs. Databricks Comparison\n",
    "\n",
    "Let's compare how this is traditionally done in **SAS** vs. **Databricks SQL** and **PySpark**:\n",
    "\n",
    "---\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: HCC Risk Score Calculation */\n",
    "/* Step 1: Create HCC reference table */\n",
    "DATA work.hcc_reference;\n",
    "    INPUT icd10_code $ 1-10 diagnosis_desc $ 12-50 hcc_category hcc_coefficient;\n",
    "    DATALINES;\n",
    "E11.9      Type 2 Diabetes                    19  0.318\n",
    "I50.9      Heart Failure                      85  0.368\n",
    "I10        Hypertension                       0   0.000\n",
    "J44.9      COPD                              111  0.328\n",
    "N18.3      CKD Stage 3                       138  0.237\n",
    ";\n",
    "RUN;\n",
    "\n",
    "/* Step 2: Merge claims with diagnoses and HCC mapping */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.diagnosis_with_hcc AS\n",
    "    SELECT \n",
    "        d.claim_id,\n",
    "        d.diagnosis_code,\n",
    "        d.diagnosis_desc,\n",
    "        h.hcc_category,\n",
    "        h.hcc_coefficient,\n",
    "        CASE \n",
    "            WHEN h.hcc_category IS NOT NULL AND h.hcc_category > 0 \n",
    "            THEN 1 ELSE 0 \n",
    "        END AS is_hcc\n",
    "    FROM work.diagnosis_raw AS d\n",
    "    LEFT JOIN work.hcc_reference AS h\n",
    "        ON UPCASE(STRIP(d.diagnosis_code)) = UPCASE(STRIP(h.icd10_code));\n",
    "QUIT;\n",
    "\n",
    "/* Step 3: Calculate member-level risk scores */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.member_hccs AS\n",
    "    SELECT DISTINCT\n",
    "        c.member_id,\n",
    "        m.first_name,\n",
    "        m.last_name,\n",
    "        m.birth_date,\n",
    "        m.gender,\n",
    "        m.plan_id,\n",
    "        dh.hcc_category,\n",
    "        dh.hcc_coefficient,\n",
    "        dh.diagnosis_code,\n",
    "        YEAR(TODAY()) - YEAR(m.birth_date) AS age\n",
    "    FROM work.claims AS c\n",
    "    INNER JOIN work.members AS m \n",
    "        ON c.member_id = m.member_id\n",
    "    INNER JOIN work.diagnosis_with_hcc AS dh \n",
    "        ON c.claim_id = dh.claim_id\n",
    "    WHERE dh.is_hcc = 1;\n",
    "QUIT;\n",
    "\n",
    "/* Step 4: Aggregate and calculate final risk scores */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.member_risk_scores AS\n",
    "    SELECT \n",
    "        member_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        birth_date,\n",
    "        gender,\n",
    "        plan_id,\n",
    "        age,\n",
    "        CASE \n",
    "            WHEN age < 65 THEN 0.350\n",
    "            WHEN age BETWEEN 65 AND 69 THEN 0.450\n",
    "            WHEN age BETWEEN 70 AND 74 THEN 0.550\n",
    "            WHEN age BETWEEN 75 AND 79 THEN 0.650\n",
    "            ELSE 0.750\n",
    "        END AS demographic_score,\n",
    "        SUM(hcc_coefficient) AS hcc_score,\n",
    "        COUNT(DISTINCT hcc_category) AS hcc_count,\n",
    "        CALCULATED demographic_score + CALCULATED hcc_score AS total_risk_score,\n",
    "        ROUND((CALCULATED demographic_score + CALCULATED hcc_score) * 10000, 0.01) \n",
    "            AS projected_annual_payment\n",
    "    FROM work.member_hccs\n",
    "    GROUP BY member_id, first_name, last_name, birth_date, gender, plan_id, age\n",
    "    ORDER BY total_risk_score DESC;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå Multiple PROC SQL steps required\n",
    "- ‚ùå Intermediate tables clutter WORK library\n",
    "- ‚ùå Limited scalability with large datasets\n",
    "- ‚ùå No automatic optimization or parallelization\n",
    "- ‚ùå Complex syntax for array aggregations (HCC categories list)\n",
    "\n",
    "---\n",
    "\n",
    "Now let's see how **Databricks SQL** simplifies this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da30fc9-0891-4c8a-8bbd-aae8687f1372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create gold schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_gold;\n",
    "\n",
    "-- Step 1a: Create HCC Mapping Reference Table (simulated for demo)\n",
    "CREATE OR REPLACE TABLE payer_gold.hcc_reference (\n",
    "  icd10_code STRING,\n",
    "  diagnosis_desc STRING,\n",
    "  hcc_category INT,\n",
    "  hcc_coefficient DOUBLE\n",
    ");\n",
    "\n",
    "INSERT INTO payer_gold.hcc_reference VALUES\n",
    "('E11.9', 'Type 2 Diabetes', 19, 0.318),\n",
    "('I50.9', 'Heart Failure', 85, 0.368),\n",
    "('I10', 'Hypertension', 0, 0.000),\n",
    "('J44.9', 'COPD', 111, 0.328),\n",
    "('N18.3', 'CKD Stage 3', 138, 0.237),\n",
    "('F32.9', 'Depression', 59, 0.309),\n",
    "('E78.5', 'Hyperlipidemia', 0, 0.000),\n",
    "('I25.10', 'CAD', 88, 0.184);\n",
    "\n",
    "-- Step 1b: Join Diagnoses to HCC Categories\n",
    "CREATE OR REPLACE TABLE payer_gold.diagnosis_with_hcc AS\n",
    "SELECT\n",
    "  d.claim_id,\n",
    "  d.diagnosis_code,\n",
    "  d.diagnosis_desc,\n",
    "  h.hcc_category,\n",
    "  h.hcc_coefficient,\n",
    "  CASE WHEN h.hcc_category IS NOT NULL AND h.hcc_category > 0 THEN 1 ELSE 0 END as is_hcc\n",
    "FROM payer_bronze.diagnosis_raw d\n",
    "LEFT JOIN payer_gold.hcc_reference h \n",
    "  ON UPPER(TRIM(d.diagnosis_code)) = UPPER(TRIM(h.icd10_code));\n",
    "\n",
    "-- Step 1c: Calculate Member-Level HCC Risk Scores\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_scores AS\n",
    "WITH member_hccs AS (\n",
    "  SELECT DISTINCT\n",
    "    c.member_id,\n",
    "    m.first_name,\n",
    "    m.last_name,\n",
    "    m.birth_date,\n",
    "    m.gender,\n",
    "    m.plan_id,\n",
    "    dh.hcc_category,\n",
    "    dh.hcc_coefficient,\n",
    "    dh.diagnosis_code,\n",
    "    YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as age\n",
    "  FROM payer_silver.claims c\n",
    "  INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "  INNER JOIN payer_gold.diagnosis_with_hcc dh ON c.claim_id = dh.claim_id\n",
    "  WHERE dh.is_hcc = 1\n",
    "),\n",
    "member_scores AS (\n",
    "  SELECT\n",
    "    member_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    birth_date,\n",
    "    gender,\n",
    "    plan_id,\n",
    "    age,\n",
    "    CASE \n",
    "      WHEN age < 65 THEN 0.350\n",
    "      WHEN age BETWEEN 65 AND 69 THEN 0.450\n",
    "      WHEN age BETWEEN 70 AND 74 THEN 0.550\n",
    "      WHEN age BETWEEN 75 AND 79 THEN 0.650\n",
    "      ELSE 0.750\n",
    "    END as demographic_score,\n",
    "    SUM(hcc_coefficient) as hcc_score,\n",
    "    COUNT(DISTINCT hcc_category) as hcc_count,\n",
    "    COLLECT_SET(hcc_category) as hcc_categories,\n",
    "    COLLECT_SET(diagnosis_code) as diagnosis_codes\n",
    "  FROM member_hccs\n",
    "  GROUP BY member_id, first_name, last_name, birth_date, gender, plan_id, age\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  demographic_score + hcc_score as total_risk_score,\n",
    "  ROUND((demographic_score + hcc_score) * 10000, 2) as projected_annual_payment\n",
    "FROM member_scores\n",
    "ORDER BY total_risk_score DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdd27aa-70ac-4293-ad4e-25c55b2cf9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üöÄ Databricks SQL Advantages\n",
    "\n",
    "**Databricks SQL Benefits:**\n",
    "- ‚úÖ **Single SQL Statement**: All logic in one CREATE TABLE AS with CTEs\n",
    "- ‚úÖ **Advanced Functions**: COLLECT_SET() for array aggregation (no SAS equivalent)\n",
    "- ‚úÖ **Automatic Optimization**: Query engine optimizes joins and aggregations\n",
    "- ‚úÖ **Scalability**: Distributed processing handles billions of rows\n",
    "- ‚úÖ **Unity Catalog**: Built-in governance, lineage tracking, and access control\n",
    "- ‚úÖ **Delta Lake**: ACID transactions, time travel, schema evolution\n",
    "- ‚úÖ **Real-time Refresh**: Can be scheduled or triggered automatically\n",
    "\n",
    "### üí° PySpark Alternative\n",
    "\n",
    "For complex business logic or programmatic control, use PySpark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0e827d-baf5-4ce5-a350-2b002d4ae251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark Example: HCC Risk Score Calculation\n",
    "from pyspark.sql.functions import (\n",
    "    col, year, current_date, sum as _sum, count, countDistinct,\n",
    "    collect_set, when, round as spark_round, lit\n",
    ")\n",
    "\n",
    "# Read tables\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "diagnosis_with_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# Join and calculate member HCCs\n",
    "member_hccs = claims \\\n",
    "    .join(members, \"member_id\") \\\n",
    "    .join(diagnosis_with_hcc, \"claim_id\") \\\n",
    "    .filter(col(\"is_hcc\") == 1) \\\n",
    "    .withColumn(\"age\", year(current_date()) - year(col(\"birth_date\"))) \\\n",
    "    .select(\n",
    "        \"member_id\", \"first_name\", \"last_name\", \"birth_date\", \"gender\", \n",
    "        \"plan_id\", \"age\", \"hcc_category\", \"hcc_coefficient\", \"diagnosis_code\"\n",
    "    ) \\\n",
    "    .distinct()\n",
    "\n",
    "# Calculate demographic scores and aggregate HCC scores\n",
    "member_risk_scores_pyspark = member_hccs \\\n",
    "    .withColumn(\"demographic_score\",\n",
    "        when(col(\"age\") < 65, 0.350)\n",
    "        .when(col(\"age\") <= 69, 0.450)\n",
    "        .when(col(\"age\") <= 74, 0.550)\n",
    "        .when(col(\"age\") <= 79, 0.650)\n",
    "        .otherwise(0.750)\n",
    "    ) \\\n",
    "    .groupBy(\"member_id\", \"first_name\", \"last_name\", \"birth_date\", \n",
    "             \"gender\", \"plan_id\", \"age\", \"demographic_score\") \\\n",
    "    .agg(\n",
    "        _sum(\"hcc_coefficient\").alias(\"hcc_score\"),\n",
    "        countDistinct(\"hcc_category\").alias(\"hcc_count\"),\n",
    "        collect_set(\"hcc_category\").alias(\"hcc_categories\"),\n",
    "        collect_set(\"diagnosis_code\").alias(\"diagnosis_codes\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_risk_score\", col(\"demographic_score\") + col(\"hcc_score\")) \\\n",
    "    .withColumn(\"projected_annual_payment\", \n",
    "                spark_round(col(\"total_risk_score\") * 10000, 2)) \\\n",
    "    .orderBy(col(\"total_risk_score\").desc())\n",
    "\n",
    "# Display results\n",
    "print(\"üìä PySpark Risk Score Calculation - Top 10 Members:\")\n",
    "display(member_risk_scores_pyspark.limit(10))\n",
    "\n",
    "# Optionally write to table\n",
    "# member_risk_scores_pyspark.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_gold.member_risk_scores_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd846838-7c45-49ca-bba3-1694a5a1d099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! (3 mins)\n",
    "Ask Databricks Assistant: \"Calculate top 20 members by risk score in SQL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab5f7a1-4b7e-44d2-9a6f-53f06724be6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 2: Revenue Forecast & Impact Analysis\n",
    "\n",
    "### üí∞ Business Goal\n",
    "Project total CMS revenue based on risk scores to support financial planning.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Total member population\n",
    "- Average risk score\n",
    "- Projected annual revenue\n",
    "- Revenue by plan and risk tier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c03deaa-3fe4-419d-b33d-ec487bf9ebed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Revenue Forecast by Plan */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.revenue_forecast AS\n",
    "    SELECT \n",
    "        plan_id,\n",
    "        COUNT(DISTINCT member_id) AS total_members,\n",
    "        ROUND(AVG(total_risk_score), 0.001) AS avg_risk_score,\n",
    "        ROUND(MIN(total_risk_score), 0.001) AS min_risk_score,\n",
    "        ROUND(MAX(total_risk_score), 0.001) AS max_risk_score,\n",
    "        SUM(projected_annual_payment) AS total_projected_revenue,\n",
    "        ROUND(AVG(projected_annual_payment), 0.01) AS avg_payment_per_member,\n",
    "        SUM(CASE WHEN total_risk_score >= 1.5 THEN 1 ELSE 0 END) AS high_risk_members,\n",
    "        SUM(CASE WHEN total_risk_score < 1.0 THEN 1 ELSE 0 END) AS low_risk_members\n",
    "    FROM work.member_risk_scores\n",
    "    GROUP BY plan_id\n",
    "    ORDER BY total_projected_revenue DESC;\n",
    "QUIT;\n",
    "\n",
    "/* Export to Excel for reporting */\n",
    "PROC EXPORT DATA=work.revenue_forecast\n",
    "    OUTFILE='/path/to/revenue_forecast.xlsx'\n",
    "    DBMS=XLSX REPLACE;\n",
    "RUN;\n",
    "\n",
    "/* Generate summary report */\n",
    "PROC PRINT DATA=work.revenue_forecast;\n",
    "    TITLE 'Revenue Forecast by Plan';\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Limitations:**\n",
    "- ‚ùå Manual export steps for reporting\n",
    "- ‚ùå No real-time dashboard integration\n",
    "- ‚ùå Limited to single-server processing\n",
    "- ‚ùå Requires additional tools for visualization\n",
    "- ‚ùå Static reports need manual refresh\n",
    "\n",
    "#### Databricks SQL Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dbb756-8188-473a-b74c-bee114c6abf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üéØ YOUR TURN! (3 mins)\n",
    "Work with Databricks Assistant and convert below SAS code to SQL. Create new table called \"revenue_forecast\" under \"payer_gold\" schema (if you have time, convert to Pyspark as well):\n",
    "\n",
    "```**sas**\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.revenue_forecast AS\n",
    "    SELECT \n",
    "        plan_id,\n",
    "        COUNT(DISTINCT member_id) AS total_members,\n",
    "        ROUND(AVG(total_risk_score), 0.001) AS avg_risk_score,\n",
    "        ROUND(MIN(total_risk_score), 0.001) AS min_risk_score,\n",
    "        ROUND(MAX(total_risk_score), 0.001) AS max_risk_score,\n",
    "        SUM(projected_annual_payment) AS total_projected_revenue,\n",
    "        ROUND(AVG(projected_annual_payment), 0.01) AS avg_payment_per_member,\n",
    "        SUM(CASE WHEN total_risk_score >= 1.5 THEN 1 ELSE 0 END) AS high_risk_members,\n",
    "        SUM(CASE WHEN total_risk_score < 1.0 THEN 1 ELSE 0 END) AS low_risk_members\n",
    "    FROM work.member_risk_scores\n",
    "    GROUP BY plan_id\n",
    "    ORDER BY total_projected_revenue DESC;\n",
    "QUIT;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f93350-2e88-4a92-9a76-ba5f16527568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- -- Display revenue forecast\n",
    "-- SELECT * FROM payer_gold.revenue_forecast;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c1af5f-5c26-4fb6-8883-0ee0abd51e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 3: HCC Distribution Analysis\n",
    "\n",
    "### üìà Business Goal\n",
    "Understand which HCC categories drive the most revenue and identify coding opportunities.\n",
    "\n",
    "This helps HQRI:\n",
    "- Identify high-value diagnoses for provider education\n",
    "- Monitor HCC capture rates\n",
    "- Find gaps in documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a03cfc-dc0a-4ee0-882d-12d58d31d264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: HCC Distribution Analysis */\n",
    "/* Step 1: Unnest HCC categories from member_risk_scores */\n",
    "/* Note: SAS doesn't have native array explosion like SQL EXPLODE */\n",
    "/* Must use DATA step with ARRAY processing */\n",
    "\n",
    "DATA work.hcc_exploded;\n",
    "    SET work.member_risk_scores;\n",
    "    ARRAY hccs hcc_cat1-hcc_cat10;  /* Assumes max 10 HCCs per member */\n",
    "    \n",
    "    DO i = 1 TO DIM(hccs);\n",
    "        IF hccs[i] NE . THEN DO;\n",
    "            hcc_category = hccs[i];\n",
    "            OUTPUT;\n",
    "        END;\n",
    "    END;\n",
    "    DROP hcc_cat1-hcc_cat10 i;\n",
    "RUN;\n",
    "\n",
    "/* Step 2: Join with HCC reference and aggregate */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.hcc_distribution AS\n",
    "    SELECT \n",
    "        he.hcc_category,\n",
    "        r.diagnosis_desc,\n",
    "        r.hcc_coefficient,\n",
    "        COUNT(DISTINCT he.member_id) AS member_count,\n",
    "        COUNT(DISTINCT he.plan_id) AS plan_count,\n",
    "        ROUND(r.hcc_coefficient * COUNT(DISTINCT he.member_id) * 10000, 0.01) \n",
    "            AS total_revenue_impact,\n",
    "        ROUND(r.hcc_coefficient * 10000, 0.01) AS revenue_per_member\n",
    "    FROM work.hcc_exploded AS he\n",
    "    INNER JOIN work.hcc_reference AS r \n",
    "        ON he.hcc_category = r.hcc_category\n",
    "    GROUP BY he.hcc_category, r.diagnosis_desc, r.hcc_coefficient\n",
    "    ORDER BY total_revenue_impact DESC;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå No native EXPLODE function - requires manual array processing\n",
    "- ‚ùå Must pre-define array size (max HCCs per member)\n",
    "- ‚ùå Two-step process: DATA step + PROC SQL\n",
    "- ‚ùå Complex logic for dynamic array sizes\n",
    "- ‚ùå Performance issues with large datasets\n",
    "\n",
    "#### Databricks SQL Approach - Single Statement!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abd85b1-3b28-4b99-8326-4dd695795961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! (3 mins)\n",
    "Work with Databricks Assistant and convert below SAS code to SQL. Create new table called \"hcc_distribution\" under \"payer_gold\" schema (if you have time, convert to Pyspark as well):\n",
    "\n",
    "```sas\n",
    "/* SAS: HCC Distribution Analysis */\n",
    "/* Step 1: Unnest HCC categories from member_risk_scores */\n",
    "/* Note: SAS doesn't have native array explosion like SQL EXPLODE */\n",
    "/* Must use DATA step with ARRAY processing */\n",
    "\n",
    "DATA work.hcc_exploded;\n",
    "    SET work.member_risk_scores;\n",
    "    ARRAY hccs hcc_cat1-hcc_cat10;  /* Assumes max 10 HCCs per member */\n",
    "    \n",
    "    DO i = 1 TO DIM(hccs);\n",
    "        IF hccs[i] NE . THEN DO;\n",
    "            hcc_category = hccs[i];\n",
    "            OUTPUT;\n",
    "        END;\n",
    "    END;\n",
    "    DROP hcc_cat1-hcc_cat10 i;\n",
    "RUN;\n",
    "\n",
    "/* Step 2: Join with HCC reference and aggregate */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.hcc_distribution AS\n",
    "    SELECT \n",
    "        he.hcc_category,\n",
    "        r.diagnosis_desc,\n",
    "        r.hcc_coefficient,\n",
    "        COUNT(DISTINCT he.member_id) AS member_count,\n",
    "        COUNT(DISTINCT he.plan_id) AS plan_count,\n",
    "        ROUND(r.hcc_coefficient * COUNT(DISTINCT he.member_id) * 10000, 0.01) \n",
    "            AS total_revenue_impact,\n",
    "        ROUND(r.hcc_coefficient * 10000, 0.01) AS revenue_per_member\n",
    "    FROM work.hcc_exploded AS he\n",
    "    INNER JOIN work.hcc_reference AS r \n",
    "        ON he.hcc_category = r.hcc_category\n",
    "    GROUP BY he.hcc_category, r.diagnosis_desc, r.hcc_coefficient\n",
    "    ORDER BY total_revenue_impact DESC;\n",
    "QUIT;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02aeb28f-1a1c-498b-9943-59d75884fa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- -- View HCC distribution\n",
    "-- SELECT * FROM payer_gold.hcc_distribution;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2231a1-375a-4c6c-a787-9c159e15f97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 4: Data Quality & Compliance Audit\n",
    "\n",
    "### ‚úÖ Business Goal\n",
    "Ensure encounter data meets CMS submission standards and identify data quality issues.\n",
    "\n",
    "**CMS Requirements:**\n",
    "- Valid diagnosis codes (ICD-10 format)\n",
    "- Complete member demographics\n",
    "- Valid provider NPIs\n",
    "- Service dates within coverage period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6bc87b-c322-48f5-a718-bc91ec71a615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. Databricks SQL Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Data Quality Audit - Requires Multiple Queries */\n",
    "\n",
    "/* Query 1: Total Encounters */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_total AS\n",
    "    SELECT \n",
    "        'Total Encounters' AS metric,\n",
    "        COUNT(*) AS record_count,\n",
    "        . AS pct_of_total,\n",
    "        'INFO' AS severity\n",
    "    FROM work.claims;\n",
    "QUIT;\n",
    "\n",
    "/* Query 2: Encounters Missing Diagnosis */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_missing_dx AS\n",
    "    SELECT \n",
    "        'Encounters Missing Diagnosis' AS metric,\n",
    "        COUNT(DISTINCT c.claim_id) AS record_count,\n",
    "        CALCULATED record_count * 100.0 / \n",
    "            (SELECT COUNT(*) FROM work.claims) AS pct_of_total,\n",
    "        'ERROR' AS severity\n",
    "    FROM work.claims AS c\n",
    "    LEFT JOIN work.diagnosis_raw AS d \n",
    "        ON c.claim_id = d.claim_id\n",
    "    WHERE d.claim_id IS NULL;\n",
    "QUIT;\n",
    "\n",
    "/* Query 3: HCC Mapping Rate */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.audit_hcc_mapped AS\n",
    "    SELECT \n",
    "        'Diagnoses Mapped to HCC' AS metric,\n",
    "        COUNT(*) AS record_count,\n",
    "        CALCULATED record_count * 100.0 / \n",
    "            (SELECT COUNT(*) FROM work.diagnosis_raw) AS pct_of_total,\n",
    "        'INFO' AS severity\n",
    "    FROM work.diagnosis_with_hcc\n",
    "    WHERE is_hcc = 1;\n",
    "QUIT;\n",
    "\n",
    "/* Repeat for remaining checks... */\n",
    "\n",
    "/* Combine all audit results */\n",
    "DATA work.data_quality_audit;\n",
    "    SET work.audit_total\n",
    "        work.audit_missing_dx\n",
    "        work.audit_hcc_mapped\n",
    "        /* ... other audit tables ... */;\n",
    "        \n",
    "    /* Add status flag */\n",
    "    IF severity = 'ERROR' AND record_count > 0 THEN status = 'FAIL';\n",
    "    ELSE IF severity = 'WARNING' AND pct_of_total > 5 THEN status = 'REVIEW';\n",
    "    ELSE status = 'PASS';\n",
    "RUN;\n",
    "\n",
    "/* Sort and display */\n",
    "PROC SORT DATA=work.data_quality_audit;\n",
    "    BY severity DESCENDING pct_of_total;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Challenges:**\n",
    "- ‚ùå Must create separate queries for each audit check\n",
    "- ‚ùå Manual combination with DATA step\n",
    "- ‚ùå Multiple intermediate tables clutter workspace\n",
    "- ‚ùå Difficult to maintain as audit rules grow\n",
    "- ‚ùå No UNION ALL equivalent in single PROC SQL\n",
    "- ‚ùå Subquery limitations in SELECT clause\n",
    "\n",
    "#### Databricks SQL Approach - Elegant & Maintainable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838cab28-b5ff-4ac0-b676-a4563be46845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Data Quality Audit for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.data_quality_audit AS\n",
    "WITH encounter_checks AS (\n",
    "  SELECT\n",
    "    'Total Encounters' as metric,\n",
    "    COUNT(*) as record_count,\n",
    "    NULL as pct_of_total,\n",
    "    'INFO' as severity\n",
    "  FROM payer_silver.claims\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Encounters Missing Diagnosis',\n",
    "    COUNT(DISTINCT c.claim_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT c.claim_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.claims c\n",
    "  LEFT JOIN payer_bronze.diagnosis_raw d ON c.claim_id = d.claim_id\n",
    "  WHERE d.claim_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Diagnoses Mapped to HCC',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_bronze.diagnosis_raw),\n",
    "      2\n",
    "    ),\n",
    "    'INFO'\n",
    "  FROM payer_gold.diagnosis_with_hcc\n",
    "  WHERE is_hcc = 1\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Members Without Risk Scores',\n",
    "    COUNT(DISTINCT m.member_id),\n",
    "    ROUND(\n",
    "      COUNT(DISTINCT m.member_id) * 100.0 / (SELECT COUNT(*) FROM payer_silver.members),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.members m\n",
    "  LEFT JOIN payer_gold.member_risk_scores r ON m.member_id = r.member_id\n",
    "  WHERE r.member_id IS NULL\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Providers Missing NPI',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.providers),\n",
    "      2\n",
    "    ),\n",
    "    'ERROR'\n",
    "  FROM payer_silver.providers\n",
    "  WHERE npi IS NULL OR TRIM(npi) = ''\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT\n",
    "    'Claims with Invalid Status',\n",
    "    COUNT(*),\n",
    "    ROUND(\n",
    "      COUNT(*) * 100.0 / (SELECT COUNT(*) FROM payer_silver.claims),\n",
    "      2\n",
    "    ),\n",
    "    'WARNING'\n",
    "  FROM payer_silver.claims\n",
    "  WHERE claim_status NOT IN ('approved', 'paid', 'pending')\n",
    ")\n",
    "SELECT\n",
    "  metric,\n",
    "  record_count,\n",
    "  COALESCE(pct_of_total, 0.0) as pct_of_total,\n",
    "  severity,\n",
    "  CASE\n",
    "    WHEN severity = 'ERROR' AND record_count > 0 THEN 'FAIL'\n",
    "    WHEN severity = 'WARNING' AND pct_of_total > 5 THEN 'REVIEW'\n",
    "    ELSE 'PASS'\n",
    "  END as status\n",
    "FROM encounter_checks\n",
    "ORDER BY\n",
    "  CASE severity WHEN 'ERROR' THEN 1 WHEN 'WARNING' THEN 2 ELSE 3 END,\n",
    "  pct_of_total DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f520f3f-5bd9-43cf-891f-d3849359f485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View data quality audit results\n",
    "SELECT * FROM payer_gold.data_quality_audit;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9961268c-b1f3-4604-bc54-0f427c2db0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 5: Member Risk Stratification\n",
    "\n",
    "### üë• Business Goal\n",
    "Segment members by risk level to support care management and intervention programs.\n",
    "\n",
    "**Risk Tiers:**\n",
    "- **Very High Risk** (Score > 2.0): Intensive care management\n",
    "- **High Risk** (Score 1.5-2.0): Enhanced monitoring\n",
    "- **Moderate Risk** (Score 1.0-1.5): Standard care\n",
    "- **Low Risk** (Score < 1.0): Preventive care focus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7396318d-098e-4644-b0bc-102527210d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Member Risk Stratification\n",
    "CREATE OR REPLACE TABLE payer_gold.member_risk_stratification AS\n",
    "SELECT\n",
    "  member_id,\n",
    "  CONCAT(first_name, ' ', last_name) as member_name,\n",
    "  age,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  total_risk_score,\n",
    "  hcc_count,\n",
    "  projected_annual_payment,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Very High Risk'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'High Risk'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Moderate Risk'\n",
    "    ELSE 'Low Risk'\n",
    "  END as risk_tier,\n",
    "  CASE\n",
    "    WHEN total_risk_score >= 2.0 THEN 'Intensive Care Management Required'\n",
    "    WHEN total_risk_score >= 1.5 THEN 'Enhanced Monitoring Recommended'\n",
    "    WHEN total_risk_score >= 1.0 THEN 'Standard Care Protocol'\n",
    "    ELSE 'Preventive Care Focus'\n",
    "  END as care_recommendation,\n",
    "  hcc_categories as active_hcc_list\n",
    "FROM payer_gold.member_risk_scores;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fae8461-2e2e-4566-a63d-d6e941b147dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Summary statistics by risk tier\n",
    "SELECT\n",
    "  risk_tier,\n",
    "  COUNT(*) as member_count,\n",
    "  ROUND(AVG(age), 1) as avg_age,\n",
    "  ROUND(AVG(total_risk_score), 3) as avg_risk_score,\n",
    "  ROUND(AVG(hcc_count), 1) as avg_hcc_count,\n",
    "  SUM(projected_annual_payment) as total_revenue,\n",
    "  ROUND(AVG(projected_annual_payment), 2) as avg_payment_per_member\n",
    "FROM payer_gold.member_risk_stratification\n",
    "GROUP BY risk_tier\n",
    "ORDER BY avg_risk_score DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291bd675-7d9a-4f47-9cb4-6d1d93148a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 6: Provider Performance on Risk Capture\n",
    "\n",
    "### üè• Business Goal\n",
    "Identify which providers excel at documenting HCC conditions to guide provider education.\n",
    "\n",
    "**Key Metrics:**\n",
    "- Members per provider\n",
    "- Average risk score of provider's panel\n",
    "- HCC capture rate\n",
    "- Revenue attributed to provider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9548ad6-8c71-45aa-9c7f-28e5075553e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üìä SAS vs. PySpark Comparison\n",
    "\n",
    "#### Traditional SAS Approach\n",
    "\n",
    "```sas\n",
    "/* SAS: Provider Performance on Risk Capture */\n",
    "PROC SQL;\n",
    "    CREATE TABLE work.provider_performance AS\n",
    "    SELECT \n",
    "        p.provider_id,\n",
    "        p.provider_name,\n",
    "        p.specialty,\n",
    "        p.city,\n",
    "        p.state,\n",
    "        COUNT(DISTINCT c.member_id) AS unique_members,\n",
    "        COUNT(c.claim_id) AS total_encounters,\n",
    "        ROUND(AVG(m.total_risk_score), 0.001) AS avg_member_risk_score,\n",
    "        SUM(dh.hcc_coefficient) AS total_hcc_value,\n",
    "        COUNT(DISTINCT dh.hcc_category) AS unique_hccs_captured,\n",
    "        ROUND(SUM(m.projected_annual_payment), 0.01) AS attributed_revenue,\n",
    "        ROUND(COUNT(DISTINCT dh.hcc_category) / COUNT(DISTINCT c.member_id), 0.01) \n",
    "            AS hcc_capture_rate\n",
    "    FROM work.claims AS c\n",
    "    INNER JOIN work.providers AS p \n",
    "        ON c.provider_id = p.provider_id\n",
    "    INNER JOIN work.member_risk_scores AS m \n",
    "        ON c.member_id = m.member_id\n",
    "    INNER JOIN work.diagnosis_with_hcc AS dh \n",
    "        ON c.claim_id = dh.claim_id\n",
    "    GROUP BY p.provider_id, p.provider_name, p.specialty, p.city, p.state\n",
    "    ORDER BY attributed_revenue DESC;\n",
    "QUIT;\n",
    "\n",
    "/* Create top providers report */\n",
    "PROC PRINT DATA=work.provider_performance(OBS=20);\n",
    "    TITLE 'Top 20 Providers by Risk Capture Performance';\n",
    "    VAR provider_name specialty unique_members avg_member_risk_score \n",
    "        unique_hccs_captured attributed_revenue hcc_capture_rate;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "**SAS Limitations:**\n",
    "- ‚ùå Must calculate derived metrics (hcc_capture_rate) in the same SELECT\n",
    "- ‚ùå No withColumn() equivalent for cleaner syntax\n",
    "- ‚ùå Limited to single-server memory for large joins\n",
    "- ‚ùå Static output - no interactive display\n",
    "- ‚ùå Requires separate PROC PRINT for visualization\n",
    "\n",
    "#### PySpark Approach - More Flexible & Scalable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3661be1-6e9a-4ba1-9334-8aea8f02f3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, avg, sum, round as spark_round, col\n",
    "\n",
    "# Provider Performance Analysis\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "members = spark.table(\"payer_gold.member_risk_scores\")\n",
    "diagnosis_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# Join claims with risk scores\n",
    "provider_performance = claims \\\n",
    "    .join(providers, \"provider_id\") \\\n",
    "    .join(members, \"member_id\") \\\n",
    "    .join(diagnosis_hcc, \"claim_id\") \\\n",
    "    .groupBy(\n",
    "        \"provider_id\",\n",
    "        \"provider_name\",\n",
    "        \"specialty\",\n",
    "        \"city\",\n",
    "        \"state\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        countDistinct(\"member_id\").alias(\"unique_members\"),\n",
    "        count(\"claim_id\").alias(\"total_encounters\"),\n",
    "        spark_round(avg(\"total_risk_score\"), 3).alias(\"avg_member_risk_score\"),\n",
    "        sum(col(\"hcc_coefficient\")).alias(\"total_hcc_value\"),\n",
    "        countDistinct(col(\"hcc_category\")).alias(\"unique_hccs_captured\"),\n",
    "        spark_round(sum(\"projected_annual_payment\"), 2).alias(\"attributed_revenue\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"hcc_capture_rate\",\n",
    "        spark_round(col(\"unique_hccs_captured\") / col(\"unique_members\"), 2)\n",
    "    ) \\\n",
    "    .orderBy(col(\"attributed_revenue\").desc())\n",
    "\n",
    "# Display top providers\n",
    "print(\"üè• Top 20 Providers by Risk Capture Performance:\")\n",
    "display(provider_performance.limit(20))\n",
    "\n",
    "# Save to Gold table\n",
    "provider_performance.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.provider_risk_capture_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d7d1eb-2e5f-494a-9bea-b64353481adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Example 7: Encounter Datamart for CMS Submission\n",
    "\n",
    "### üìä Business Goal\n",
    "Create a CMS-ready encounter datamart with all required fields and validations.\n",
    "\n",
    "**CMS Submission Requirements:**\n",
    "- Valid member enrollment\n",
    "- Complete encounter details (dates, provider, diagnosis)\n",
    "- Proper diagnosis code formatting (ICD-10)\n",
    "- Service within coverage period\n",
    "- Provider has valid NPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169b5140-87ca-486b-8256-1b5edac71e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Encounter Datamart for CMS Submission\n",
    "CREATE OR REPLACE TABLE payer_gold.encounter_datamart_cms AS\n",
    "SELECT\n",
    "  -- Encounter identifiers\n",
    "  c.claim_id as encounter_id,\n",
    "  c.claim_date as encounter_date,\n",
    "  c.claim_status as encounter_status,\n",
    "  \n",
    "  -- Member information\n",
    "  m.member_id,\n",
    "  m.first_name,\n",
    "  m.last_name,\n",
    "  m.birth_date,\n",
    "  m.gender,\n",
    "  m.plan_id,\n",
    "  YEAR(CURRENT_DATE()) - YEAR(m.birth_date) as member_age,\n",
    "  \n",
    "  -- Provider information\n",
    "  p.provider_id,\n",
    "  p.npi as provider_npi,\n",
    "  p.provider_name,\n",
    "  p.specialty as provider_specialty,\n",
    "  p.state as provider_state,\n",
    "  \n",
    "  -- Diagnosis information\n",
    "  d.diagnosis_code as icd10_code,\n",
    "  d.diagnosis_desc,\n",
    "  d.hcc_category,\n",
    "  d.hcc_coefficient,\n",
    "  d.is_hcc,\n",
    "  \n",
    "  -- Claim financial\n",
    "  c.total_charge,\n",
    "  \n",
    "  -- Data quality flags\n",
    "  CASE \n",
    "    WHEN m.member_id IS NULL THEN 'FAIL: Missing Member'\n",
    "    WHEN p.npi IS NULL OR TRIM(p.npi) = '' THEN 'FAIL: Missing Provider NPI'\n",
    "    WHEN d.diagnosis_code IS NULL THEN 'FAIL: Missing Diagnosis'\n",
    "    WHEN c.claim_date < m.effective_date THEN 'FAIL: Service Before Coverage'\n",
    "    WHEN c.claim_status NOT IN ('approved', 'paid') THEN 'WARNING: Invalid Status'\n",
    "    ELSE 'PASS'\n",
    "  END as submission_validation_status,\n",
    "  \n",
    "  -- Submission flag\n",
    "  CASE \n",
    "    WHEN m.member_id IS NOT NULL \n",
    "     AND p.npi IS NOT NULL \n",
    "     AND TRIM(p.npi) != ''\n",
    "     AND d.diagnosis_code IS NOT NULL\n",
    "     AND c.claim_date >= m.effective_date\n",
    "     AND c.claim_status IN ('approved', 'paid')\n",
    "    THEN 1 \n",
    "    ELSE 0 \n",
    "  END as cms_submission_ready,\n",
    "  \n",
    "  CURRENT_TIMESTAMP() as datamart_created_at\n",
    "  \n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "LEFT JOIN payer_gold.diagnosis_with_hcc d ON c.claim_id = d.claim_id;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4989838d-73d3-4ab0-8de3-a63968219722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CMS Submission Readiness Summary\n",
    "SELECT\n",
    "  submission_validation_status,\n",
    "  COUNT(*) as encounter_count,\n",
    "  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct_of_total,\n",
    "  SUM(cms_submission_ready) as ready_for_submission\n",
    "FROM payer_gold.encounter_datamart_cms\n",
    "GROUP BY submission_validation_status\n",
    "ORDER BY encounter_count DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d226a329-1f6a-438f-b250-6f4c57b17097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üîÑ Migration Summary: SAS to Databricks\n",
    "\n",
    "### Key Advantages of Databricks Over SAS for Gold Layer Analytics\n",
    "\n",
    "| Feature | SAS | Databricks SQL/PySpark |\n",
    "|---------|-----|------------------------|\n",
    "| **Array Operations** | Manual ARRAY processing | Native COLLECT_SET(), EXPLODE() |\n",
    "| **Query Complexity** | Multiple PROC SQL steps | Single CTE-based queries |\n",
    "| **Scalability** | Single-server memory limits | Distributed processing (petabyte scale) |\n",
    "| **Modern Functions** | Limited window functions | Full SQL:2016 compliance |\n",
    "| **Real-time Analytics** | Batch-only | Streaming + batch unified |\n",
    "| **Visualization** | Separate tools (PROC GPLOT, SAS VA) | Built-in interactive dashboards |\n",
    "| **Governance** | Manual security setup | Unity Catalog (row/column security) |\n",
    "| **Version Control** | Limited | Full Delta Lake time travel |\n",
    "| **Cloud Native** | Legacy architecture | Modern cloud-optimized |\n",
    "| **Cost** | Expensive licensing | Pay-per-use consumption |\n",
    "| **Programming** | SAS language only | SQL, Python, R, Scala, Java |\n",
    "\n",
    "### üí° Migration Best Practices\n",
    "\n",
    "1. **Start with Simple Queries**: Begin with straightforward PROC SQL ‚Üí Databricks SQL translations\n",
    "2. **Leverage CTEs**: Replace multi-step SAS DATA/PROC steps with Common Table Expressions\n",
    "3. **Use PySpark for Complex Logic**: When business rules are complex, PySpark offers more flexibility than SAS macros\n",
    "4. **Embrace Modern Functions**: COLLECT_SET, EXPLODE, and window functions simplify code\n",
    "5. **Unity Catalog = SAS Library**: Map SAS libraries to Unity Catalog schemas\n",
    "6. **Delta Lake = SAS Datasets**: Delta tables provide ACID transactions like SAS datasets, but at scale\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88653bed-c237-4f39-bccb-df17fc7332ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 8: Understanding Lazy Evaluation & Deterministic Execution\n",
    "\n",
    "### üéØ Learning Goal\n",
    "Understand Spark's **lazy evaluation** model and best practices to ensure **deterministic execution** in production pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### What is Lazy Evaluation?\n",
    "\n",
    "**Lazy Evaluation** means Spark doesn't execute transformations immediately. Instead, it builds a **logical plan** (DAG - Directed Acyclic Graph) and only executes when an **action** is called.\n",
    "\n",
    "#### Transformations (Lazy) vs Actions (Eager)\n",
    "\n",
    "| **Transformations (Lazy)** | **Actions (Eager)** |\n",
    "|----------------------------|---------------------|\n",
    "| `.select()`, `.filter()`, `.where()` | `.count()`, `.collect()`, `.show()` |\n",
    "| `.join()`, `.groupBy()`, `.agg()` | `.write()`, `.saveAsTable()` |\n",
    "| `.withColumn()`, `.drop()`, `.distinct()` | `.first()`, `.take()`, `.foreach()` |\n",
    "| `.union()`, `.orderBy()` | `.display()` (Databricks specific) |\n",
    "\n",
    "**Key Insight:** Transformations return DataFrames but don't execute. Actions trigger the entire execution plan.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Does This Matter for HQRI Pipelines?\n",
    "\n",
    "1. **Non-Deterministic Behavior**: If you recompute the same transformation multiple times without caching, you may get different results (e.g., reading streaming data, non-deterministic UDFs).\n",
    "\n",
    "2. **Performance Issues**: Re-reading and re-transforming large datasets (like claims with millions of rows) wastes compute resources.\n",
    "\n",
    "3. **Debugging Difficulty**: Without forcing evaluation at checkpoints, errors propagate and are harder to trace.\n",
    "\n",
    "4. **Data Quality**: For compliance audits, you need deterministic, repeatable results.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices for Deterministic Execution\n",
    "\n",
    "1. **Cache intermediate results** that are reused multiple times\n",
    "2. **Use checkpointing** for very wide transformations\n",
    "3. **Force evaluation with `.count()`** at critical pipeline stages\n",
    "4. **Write to Delta tables** to persist intermediate results\n",
    "5. **Use `.explain()`** to understand execution plans\n",
    "\n",
    "Let's see these practices in action!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697a05c4-97a7-4139-86cf-690c8f72deef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üß™ Demo 1: Lazy Evaluation in Action\n",
    "\n",
    "Let's demonstrate how Spark builds execution plans without executing transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72578d49-3948-4e59-b84b-1343278e9ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, count, sum as _sum, round as spark_round\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMONSTRATION: Lazy Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read claims data\n",
    "print(\"\\n1Ô∏è‚É£ Reading claims table (TRANSFORMATION - no execution yet)...\")\n",
    "start_time = time.time()\n",
    "claims_df = spark.table(\"payer_silver.claims\")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Time elapsed: {elapsed:.4f} seconds\")\n",
    "print(\"   ‚úÖ DataFrame created instantly - no data read yet!\")\n",
    "\n",
    "# Apply multiple transformations\n",
    "print(\"\\n2Ô∏è‚É£ Applying transformations (LAZY - still no execution)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Filter to approved claims\n",
    "approved_claims = claims_df.filter(col(\"claim_status\") == \"approved\")\n",
    "print(\"   ‚Ä¢ Filtered to approved claims\")\n",
    "\n",
    "# Add computed column\n",
    "claims_with_flag = approved_claims.withColumn(\n",
    "    \"high_value_flag\",\n",
    "    col(\"total_charge\") > 1000\n",
    ")\n",
    "print(\"   ‚Ä¢ Added high-value flag column\")\n",
    "\n",
    "# Group and aggregate\n",
    "claims_summary = claims_with_flag.groupBy(\"member_id\").agg(\n",
    "    count(\"*\").alias(\"total_claims\"),\n",
    "    _sum(\"total_charge\").alias(\"total_charges\"),\n",
    "    spark_round(_sum(\"total_charge\") / count(\"*\"), 2).alias(\"avg_claim_amount\")\n",
    ")\n",
    "print(\"   ‚Ä¢ Grouped by member_id and aggregated\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n   ‚è±Ô∏è Time elapsed for 3 transformations: {elapsed:.4f} seconds\")\n",
    "print(\"   ‚úÖ All transformations completed instantly!\")\n",
    "print(\"   üí° No actual computation happened - Spark just built an execution plan\")\n",
    "\n",
    "# Now trigger execution with an ACTION\n",
    "print(\"\\n3Ô∏è‚É£ Triggering execution with .count() ACTION...\")\n",
    "start_time = time.time()\n",
    "record_count = claims_summary.count()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Time elapsed: {elapsed:.4f} seconds\")\n",
    "print(f\"   ‚úÖ Action executed! Found {record_count:,} members with approved claims\")\n",
    "print(\"   üí° This is when Spark actually read data and performed all transformations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAY: Transformations are lazy, actions are eager!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c701f897-c7e1-4746-8cd1-5748ad491cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç Demo 2: Understanding Execution Plans with `.explain()`\n",
    "\n",
    "Use `.explain()` to see Spark's execution plan before running expensive operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6470cd6-2d5c-463a-87ea-b62546a85053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, current_date\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VIEWING EXECUTION PLAN with .explain()\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a complex query\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "\n",
    "complex_query = members \\\n",
    "    .withColumn(\"age\", year(current_date()) - year(col(\"birth_date\"))) \\\n",
    "    .filter(col(\"age\") >= 65) \\\n",
    "    .join(claims, \"member_id\") \\\n",
    "    .filter(col(\"total_charge\") > 500) \\\n",
    "    .groupBy(\"plan_id\") \\\n",
    "    .count()\n",
    "\n",
    "print(\"\\nüìã Physical Execution Plan:\")\n",
    "print(\"-\" * 80)\n",
    "complex_query.explain(mode=\"simple\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° KEY INSIGHTS FROM EXECUTION PLAN:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Spark optimizes the query before execution (Catalyst Optimizer)\")\n",
    "print(\"2. Filters are pushed down early (reducing data to process)\")\n",
    "print(\"3. Joins are optimized based on data size\")\n",
    "print(\"4. You can spot expensive operations (e.g., SortMergeJoin, Shuffle)\")\n",
    "print(\"\\nüí° Use explain() to optimize your queries BEFORE running them on production data!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "044610fc-fada-49cb-88b4-114094b0c0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ö° Demo 3: Forcing Deterministic Execution with Caching\n",
    "\n",
    "**Problem:** If you use the same DataFrame multiple times without caching, Spark recomputes it every time (non-deterministic, slow).\n",
    "\n",
    "**Solution:** Use `.cache()` or `.persist()` to materialize intermediate results in memory.\n",
    "\n",
    "_Note: Caching and persist operations are not supported on Databricks SQL Serverless clusters (as of Nov/14/2025).\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a6cc52-6b27-4ff6-8b6c-0528023c248d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, count, avg, sum as _sum, year, current_date\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMONSTRATION: Caching for Deterministic Execution & Performance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a complex transformation that we'll use multiple times\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "\n",
    "# Complex intermediate result\n",
    "print(\"\\n1Ô∏è‚É£ Creating complex intermediate DataFrame (members + claims)...\")\n",
    "intermediate_df = members.join(\n",
    "    claims, \"member_id\"\n",
    ").filter(\n",
    "    col(\"total_charge\") > 0\n",
    ").withColumn(\n",
    "    \"age\", year(current_date()) - year(col(\"birth_date\"))\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ DataFrame created (no execution yet - lazy!)\")\n",
    "\n",
    "# All operations will recompute the DataFrame\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ùå CACHING NOT SUPPORTED ON SERVERLESS - All operations recompute\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ First operation: Count records...\")\n",
    "start_time = time.time()\n",
    "count_result = intermediate_df.count()\n",
    "time_1 = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Time: {time_1:.4f} seconds | Result: {count_result:,} records\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Second operation: Calculate average charge...\")\n",
    "start_time = time.time()\n",
    "avg_charge = intermediate_df.agg(\n",
    "    avg(\"total_charge\").alias(\"avg_charge\")\n",
    ").collect()[0][\"avg_charge\"]\n",
    "time_2 = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Time: {time_2:.4f} seconds | Result: ${avg_charge:.2f}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Third operation: Group by plan_id...\")\n",
    "start_time = time.time()\n",
    "plan_count = intermediate_df.groupBy(\"plan_id\").count().count()\n",
    "time_3 = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Time: {time_3:.4f} seconds | Result: {plan_count} plans\")\n",
    "\n",
    "total_time_no_cache = time_1 + time_2 + time_3\n",
    "print(f\"\\n‚è±Ô∏è Total time: {total_time_no_cache:.4f} seconds\")\n",
    "print(\"üí° Each operation re-read and re-joined the data!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total time: {total_time_no_cache:.4f} seconds\")\n",
    "print(\"\\nüí° Caching is not available on serverless clusters. Use a classic or pro cluster for caching support.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb072fc1-53ce-4302-ac15-ce8cfe47513b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üíæ Demo 4: Checkpointing & Writing to Delta for Production Pipelines\n",
    "\n",
    "**Caching** is great for interactive analysis, but for **production ETL pipelines**, you need durable, fault-tolerant checkpoints.\n",
    "\n",
    "#### When to Use Each Technique:\n",
    "\n",
    "| Technique | Use Case | Durability | Performance |\n",
    "|-----------|----------|------------|-------------|\n",
    "| **`.cache()`** | Interactive analysis, multiple operations on same data | ‚ùå Memory-only (lost on cluster restart) | ‚ö° Fastest (in-memory) |\n",
    "| **`.persist()`** | More control over storage level (memory, disk, etc.) | ‚ö†Ô∏è Configurable | ‚ö° Fast |\n",
    "| **`.checkpoint()`** | Long lineage chains, fault tolerance | ‚úÖ Durable (disk/cloud storage) | üê¢ Slower (writes to disk) |\n",
    "| **`.write.saveAsTable()`** | Production pipelines, audit trails | ‚úÖ Durable (Delta Lake) | üê¢ Slower (but enables time travel!) |\n",
    "\n",
    "Let's demonstrate best practices for production HQRI pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8adbfa7-a66a-4760-b403-e0b2213bb7af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üÜö Demo 5: SQL vs. PySpark - Lazy Evaluation Differences\n",
    "\n",
    "While both **Databricks SQL** and **PySpark** use Spark's execution engine, they handle lazy evaluation slightly differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f9a29e-9b8e-4222-af25-4853a7f7c4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SQL vs PySpark: Lazy Evaluation Differences\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä DATABRICKS SQL:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"‚Ä¢ CREATE TABLE AS SELECT (CTAS) is an ACTION - executes immediately\")\n",
    "print(\"‚Ä¢ CREATE OR REPLACE VIEW is LAZY - defines query but doesn't execute\")\n",
    "print(\"‚Ä¢ SELECT queries in notebooks execute immediately (action)\")\n",
    "print(\"‚Ä¢ CACHE TABLE forces materialization\")\n",
    "\n",
    "print(\"\\nüêç PYSPARK:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"‚Ä¢ Transformations (.select, .filter, .join) are LAZY\")\n",
    "print(\"‚Ä¢ Actions (.count, .show, .collect, .write) execute immediately\")\n",
    "print(\"‚Ä¢ .createOrReplaceTempView() is LAZY (just registers view)\")\n",
    "print(\"‚Ä¢ .cache() is LAZY until first action\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE: Creating Gold Tables with Different Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Approach 1: SQL CTAS (executes immediately)\n",
    "print(\"\\n1Ô∏è‚É£ SQL CTAS - Executes Immediately\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE payer_gold.member_summary_sql AS\n",
    "    SELECT \n",
    "        member_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        plan_id,\n",
    "        YEAR(CURRENT_DATE()) - YEAR(birth_date) as age\n",
    "    FROM payer_silver.members\n",
    "    WHERE member_id IS NOT NULL\n",
    "\"\"\")\n",
    "elapsed = time.time() - start\n",
    "row_count = spark.sql(\"SELECT COUNT(*) as cnt FROM payer_gold.member_summary_sql\").collect()[0][\"cnt\"]\n",
    "print(f\"   ‚è±Ô∏è Time: {elapsed:.4f} seconds\")\n",
    "print(f\"   ‚úÖ Table created with {row_count:,} rows\")\n",
    "print(\"   üí° CTAS executed immediately - table is ready to query\")\n",
    "\n",
    "# Approach 2: PySpark Lazy + Write (deferred until write)\n",
    "print(\"\\n2Ô∏è‚É£ PySpark Transformations + Write\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "member_summary_df = members \\\n",
    "    .filter(col(\"member_id\").isNotNull()) \\\n",
    "    .select(\n",
    "        \"member_id\",\n",
    "        \"first_name\", \n",
    "        \"last_name\",\n",
    "        \"plan_id\",\n",
    "        (year(current_date()) - year(col(\"birth_date\"))).alias(\"age\")\n",
    "    )\n",
    "transform_time = time.time() - start\n",
    "print(f\"   ‚è±Ô∏è Transformation time: {transform_time:.4f} seconds\")\n",
    "print(\"   üí° Transformations completed instantly (lazy!)\")\n",
    "\n",
    "start = time.time()\n",
    "member_summary_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.member_summary_pyspark\")\n",
    "write_time = time.time() - start\n",
    "print(f\"   ‚è±Ô∏è Write time: {write_time:.4f} seconds\")\n",
    "print(\"   ‚úÖ Table written to Delta Lake\")\n",
    "\n",
    "# Approach 3: SQL VIEW (lazy - no execution)\n",
    "print(\"\\n3Ô∏è‚É£ SQL VIEW - Lazy Definition\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW payer_gold.member_summary_view AS\n",
    "    SELECT \n",
    "        member_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        plan_id,\n",
    "        YEAR(CURRENT_DATE()) - YEAR(birth_date) as age\n",
    "    FROM payer_silver.members\n",
    "    WHERE member_id IS NOT NULL\n",
    "\"\"\")\n",
    "elapsed = time.time() - start\n",
    "print(f\"   ‚è±Ô∏è Time: {elapsed:.4f} seconds\")\n",
    "print(\"   ‚úÖ View created instantly (no data processed)\")\n",
    "print(\"   üí° View is just a query definition - executes when queried\")\n",
    "\n",
    "# Query the view (this executes it)\n",
    "start = time.time()\n",
    "view_count = spark.sql(\"SELECT COUNT(*) as cnt FROM payer_gold.member_summary_view\").collect()[0][\"cnt\"]\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n   Querying the view:\")\n",
    "print(f\"   ‚è±Ô∏è Time: {elapsed:.4f} seconds\")\n",
    "print(f\"   ‚úÖ Result: {view_count:,} rows\")\n",
    "print(\"   üí° View executes query every time it's queried (recomputes data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. SQL CTAS creates tables immediately (good for materialization)\")\n",
    "print(\"2. SQL VIEWs defer execution (good for dynamic queries)\")\n",
    "print(\"3. PySpark transformations are lazy (gives control over execution)\")\n",
    "print(\"4. Use CTAS/saveAsTable for Gold layer tables (deterministic)\")\n",
    "print(\"5. Use VIEWs for frequently-changing business logic\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40a15a6-2971-4935-b0c1-3d1679c79ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! Practice Exercise (After today's session)\n",
    "\n",
    "### Exercise: Build a Deterministic Provider Performance Pipeline\n",
    "\n",
    "**Scenario:** You need to create a Gold layer table that calculates provider performance metrics. The pipeline needs to be:\n",
    "- ‚úÖ Deterministic (reproducible results)\n",
    "- ‚úÖ Performant (uses caching appropriately)\n",
    "- ‚úÖ Production-ready (includes checkpoints and validation)\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Stage 1:** Join claims with diagnosis and HCC data\n",
    "   - Filter to only approved claims with HCC categories\n",
    "   - Checkpoint this intermediate result to `payer_gold.claims_with_hcc_checkpoint`\n",
    "   - Validate with `.count()`\n",
    "\n",
    "2. **Stage 2:** Calculate provider-level metrics from the checkpoint\n",
    "   - Metrics needed:\n",
    "     - Total claims per provider\n",
    "     - Unique HCC categories captured\n",
    "     - Average HCC coefficient per claim\n",
    "     - Total attributed revenue\n",
    "   - Cache this result for multiple uses\n",
    "\n",
    "3. **Stage 3:** Generate two outputs from cached data\n",
    "   - High performers: Providers with > 5 unique HCCs\n",
    "   - Specialty summary: Average metrics by specialty\n",
    "\n",
    "4. **Clean up:** Unpersist cache when done\n",
    "\n",
    "**Starter Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520e4c59-8af0-483b-a03a-34e1f3a7cda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise following best practices for deterministic execution\n",
    "\n",
    "from pyspark.sql.functions import col, count, countDistinct, avg, sum as _sum, round as spark_round\n",
    "\n",
    "print(\"üéØ EXERCISE: Provider Performance Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stage 1: Join and checkpoint\n",
    "print(\"\\nüìã Stage 1: Creating claims with HCC data...\")\n",
    "# TODO: Read tables\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "diagnosis_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "# TODO: Join claims with providers and diagnosis_hcc\n",
    "# TODO: Filter to approved claims with HCC categories (is_hcc == 1)\n",
    "# TODO: Write to checkpoint table 'payer_gold.claims_with_hcc_checkpoint'\n",
    "# TODO: Validate with .count()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "claims_with_hcc = None  # Replace with your transformation\n",
    "\n",
    "# Uncomment when ready:\n",
    "# claims_with_hcc.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"payer_gold.claims_with_hcc_checkpoint\")\n",
    "# stage1_count = spark.table(\"payer_gold.claims_with_hcc_checkpoint\").count()\n",
    "# print(f\"‚úÖ Stage 1 complete: {stage1_count:,} claims with HCC data\")\n",
    "\n",
    "\n",
    "# Stage 2: Calculate provider metrics and cache\n",
    "print(\"\\nüìã Stage 2: Calculating provider metrics...\")\n",
    "# TODO: Read from checkpoint\n",
    "# TODO: Group by provider_id, provider_name, specialty\n",
    "# TODO: Calculate metrics (total claims, unique HCCs, avg coefficient, total revenue)\n",
    "# TODO: Cache the result\n",
    "# TODO: Trigger caching with .count()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "provider_metrics = None  # Replace with your aggregation\n",
    "\n",
    "# Uncomment when ready:\n",
    "# provider_metrics_cached = provider_metrics.cache()\n",
    "# stage2_count = provider_metrics_cached.count()\n",
    "# print(f\"‚úÖ Stage 2 complete: {stage2_count:,} providers cached\")\n",
    "\n",
    "\n",
    "# Stage 3: Generate outputs from cached data\n",
    "print(\"\\nüìã Stage 3: Generating reports...\")\n",
    "# TODO: Filter to high performers (> 5 unique HCCs)\n",
    "# TODO: Calculate specialty summary\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# high_performers = provider_metrics_cached.filter(...)\n",
    "# specialty_summary = provider_metrics_cached.groupBy(...)\n",
    "\n",
    "# print(f\"‚úÖ Found {high_performers.count()} high-performing providers\")\n",
    "# print(f\"‚úÖ Created summary for {specialty_summary.count()} specialties\")\n",
    "\n",
    "\n",
    "# Stage 4: Clean up\n",
    "# TODO: Unpersist cache\n",
    "# YOUR CODE HERE:\n",
    "# provider_metrics_cached.unpersist()\n",
    "# print(\"\\n‚úÖ Cache cleared\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° TIP: Check the solution in the next cell!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624e8308-c932-402a-b1d0-6829c2c47df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Solution: Provider Performance Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83954a2f-90d2-40bf-bd2a-4d0f5e50f9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, countDistinct, avg, sum as _sum, round as spark_round\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SOLUTION: Provider Performance Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stage 1: Join and checkpoint\n",
    "print(\"\\nüìã Stage 1: Creating claims with HCC data...\")\n",
    "\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "diagnosis_hcc = spark.table(\"payer_gold.diagnosis_with_hcc\")\n",
    "\n",
    "claims_with_hcc = (\n",
    "    claims\n",
    "    .join(providers, \"provider_id\")\n",
    "    .join(diagnosis_hcc, \"claim_id\")\n",
    "    .filter(col(\"claim_status\") == \"approved\")\n",
    "    .filter(col(\"is_hcc\") == 1)\n",
    "    .select(\n",
    "        \"claim_id\",\n",
    "        \"provider_id\",\n",
    "        \"provider_name\",\n",
    "        \"specialty\",\n",
    "        \"hcc_category\",\n",
    "        \"hcc_coefficient\",\n",
    "        \"total_charge\"\n",
    "    )\n",
    ")\n",
    "\n",
    "claims_with_hcc.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(\"payer_gold.claims_with_hcc_checkpoint\")\n",
    "\n",
    "stage1_count = spark.table(\"payer_gold.claims_with_hcc_checkpoint\").count()\n",
    "print(f\"‚úÖ Stage 1 complete: {stage1_count:,} claims with HCC data checkpointed\")\n",
    "\n",
    "# Stage 2: Calculate provider metrics (no cache)\n",
    "print(\"\\nüìã Stage 2: Calculating provider metrics...\")\n",
    "\n",
    "claims_checkpoint = spark.table(\"payer_gold.claims_with_hcc_checkpoint\")\n",
    "\n",
    "provider_metrics = claims_checkpoint.groupBy(\n",
    "    \"provider_id\",\n",
    "    \"provider_name\",\n",
    "    \"specialty\"\n",
    ").agg(\n",
    "    count(\"claim_id\").alias(\"total_claims\"),\n",
    "    countDistinct(\"hcc_category\").alias(\"unique_hcc_count\"),\n",
    "    spark_round(avg(\"hcc_coefficient\"), 3).alias(\"avg_hcc_coefficient\"),\n",
    "    spark_round(_sum(\"hcc_coefficient\") * 10000, 2).alias(\"total_attributed_revenue\")\n",
    ")\n",
    "\n",
    "stage2_count = provider_metrics.count()\n",
    "print(f\"‚úÖ Stage 2 complete: {stage2_count:,} providers with metrics\")\n",
    "\n",
    "# Stage 3: Generate outputs from provider_metrics (no cache)\n",
    "print(\"\\nüìã Stage 3: Generating reports...\")\n",
    "\n",
    "high_performers = provider_metrics.filter(col(\"unique_hcc_count\") > 5)\n",
    "high_performer_count = high_performers.count()\n",
    "print(f\"‚úÖ Found {high_performer_count:,} high-performing providers\")\n",
    "\n",
    "print(\"\\nüèÜ Top 5 High Performers:\")\n",
    "display(\n",
    "    high_performers.orderBy(col(\"unique_hcc_count\").desc()).limit(5)\n",
    ")\n",
    "\n",
    "specialty_summary = provider_metrics.groupBy(\"specialty\").agg(\n",
    "    count(\"provider_id\").alias(\"provider_count\"),\n",
    "    spark_round(avg(\"total_claims\"), 1).alias(\"avg_claims_per_provider\"),\n",
    "    spark_round(avg(\"unique_hcc_count\"), 1).alias(\"avg_hccs_captured\"),\n",
    "    spark_round(avg(\"total_attributed_revenue\"), 2).alias(\"avg_revenue_per_provider\")\n",
    ")\n",
    "specialty_count = specialty_summary.count()\n",
    "print(f\"\\n‚úÖ Created summary for {specialty_count} specialties\")\n",
    "\n",
    "print(\"\\nüìä Specialty Performance Summary:\")\n",
    "display(\n",
    "    specialty_summary.orderBy(col(\"avg_revenue_per_provider\").desc())\n",
    ")\n",
    "\n",
    "# Stage 4: Clean up (no cache to unpersist)\n",
    "print(\"\\nüìã Stage 4: Cleanup...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ Pipeline complete! All stages used deterministic execution practices.\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìù Key Practices Demonstrated:\")\n",
    "print(\"   1. ‚úÖ Wrote checkpoint to Delta table (durable)\")\n",
    "print(\"   2. ‚úÖ Validated each stage with .count()\")\n",
    "print(\"   3. ‚úÖ Read from checkpoint (deterministic)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c5a5c9-a41d-45fe-8ec2-6895f8a9512c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcd33ec-d92b-4ce0-b1db-d0738d9fb90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a98a2-db41-4a69-b0ec-9876ee60ccfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Databricks Healthcare Payer Analytics Workshop! Let's review what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished\n",
    "\n",
    "### 1. **Medallion Architecture**\n",
    "- ‚úÖ Built a complete **Bronze ‚Üí Silver ‚Üí Gold** pipeline\n",
    "- ‚úÖ Understood data quality improvement at each layer\n",
    "- ‚úÖ Created analytics-ready datasets\n",
    "\n",
    "### 2. **Data Engineering Skills**\n",
    "- ‚úÖ Loaded data using **COPY INTO**\n",
    "- ‚úÖ Transformed data with **SQL and PySpark**\n",
    "- ‚úÖ Applied data quality checks and validations\n",
    "- ‚úÖ Created aggregations and derived metrics\n",
    "\n",
    "### 3. **Analytics & Visualization**\n",
    "- ‚úÖ Generated business insights from data\n",
    "- ‚úÖ Created interactive visualizations\n",
    "- ‚úÖ Performed statistical analysis\n",
    "- ‚úÖ Built executive dashboards\n",
    "\n",
    "### 4. **Databricks Platform**\n",
    "- ‚úÖ Worked with **Unity Catalog**\n",
    "- ‚úÖ Used **Delta Lake** for reliable data storage\n",
    "- ‚úÖ Leveraged **AI Assistant** for code help\n",
    "- ‚úÖ Applied performance optimization techniques\n",
    "\n",
    "### 5. **Lazy Evaluation & Deterministic Execution**\n",
    "- ‚úÖ Understood **Spark's lazy evaluation** model (transformations vs actions)\n",
    "- ‚úÖ Used **`.cache()`** and **`.persist()`** for performance optimization\n",
    "- ‚úÖ Implemented **checkpointing** with Delta tables for production pipelines\n",
    "- ‚úÖ Applied **`.count()`** to force evaluation at critical pipeline stages\n",
    "- ‚úÖ Used **`.explain()`** to analyze and optimize execution plans\n",
    "- ‚úÖ Built **deterministic, reproducible** pipelines for compliance\n",
    "- ‚úÖ Practiced proper **memory management** (unpersist caches)\n",
    "- ‚úÖ Compared **SQL CTAS vs VIEWs** for different use cases\n",
    "\n",
    "### 6. **Refer to the Best Practices!**\n",
    "Best practices notebook: _**[Reference] Best Practices**_\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8254693864104488,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBX Workshop_HQRI_11142025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "my_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
