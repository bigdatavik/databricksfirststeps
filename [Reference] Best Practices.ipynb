{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55f09c7-c988-44fb-b138-e1d7d91119ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ SAS to Databricks Quick Reference for Actuaries\n",
    "\n",
    "This section shows you **side-by-side comparisons** of common SAS code and Databricks equivalents.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Basic Data Aggregation\n",
    "\n",
    "### SAS: PROC MEANS\n",
    "```sas\n",
    "PROC MEANS DATA=claims NOPRINT;\n",
    "    CLASS specialty;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=summary \n",
    "        N=claim_count \n",
    "        SUM=total_incurred \n",
    "        MEAN=avg_claim;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS total_incurred,\n",
    "    AVG(total_charge) AS avg_claim\n",
    "FROM claims\n",
    "GROUP BY specialty;\n",
    "```\n",
    "\n",
    "**üéØ Key Difference**: In Databricks, it's all in one SELECT statement!\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Frequency Tables\n",
    "\n",
    "### SAS: PROC FREQ\n",
    "```sas\n",
    "PROC FREQ DATA=claims;\n",
    "    TABLES specialty * state / NOCOL NOROW;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY with COUNT\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    state,\n",
    "    COUNT(*) AS frequency\n",
    "FROM claims\n",
    "GROUP BY specialty, state\n",
    "ORDER BY frequency DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Conditional Logic\n",
    "\n",
    "### SAS: DATA Step with IF-THEN\n",
    "```sas\n",
    "DATA claims_categorized;\n",
    "    SET claims;\n",
    "    IF total_charge < 1000 THEN risk_category = 'Low';\n",
    "    ELSE IF total_charge < 5000 THEN risk_category = 'Medium';\n",
    "    ELSE risk_category = 'High';\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: CASE WHEN\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN total_charge < 1000 THEN 'Low'\n",
    "        WHEN total_charge < 5000 THEN 'Medium'\n",
    "        ELSE 'High'\n",
    "    END AS risk_category\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Joining Tables\n",
    "\n",
    "### SAS: PROC SQL Join\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE enriched_claims AS\n",
    "    SELECT c.*, p.specialty, p.provider_name\n",
    "    FROM claims c\n",
    "    LEFT JOIN providers p \n",
    "        ON c.provider_id = p.provider_id;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "### Databricks: SQL Join (Identical!)\n",
    "```sql\n",
    "SELECT c.*, p.specialty, p.provider_name\n",
    "FROM claims c\n",
    "LEFT JOIN providers p \n",
    "    ON c.provider_id = p.provider_id;\n",
    "```\n",
    "\n",
    "**üéØ Great News**: If you know SAS PROC SQL, you already know Databricks SQL!\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Lagging and Leading Values (Trending)\n",
    "\n",
    "### SAS: LAG Function\n",
    "```sas\n",
    "DATA trends;\n",
    "    SET monthly_data;\n",
    "    prior_month = LAG(total_incurred);\n",
    "    growth_pct = (total_incurred - prior_month) / prior_month * 100;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: LAG Window Function\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY month) AS prior_month,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY month) * 100, 2) AS growth_pct\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Percentiles and Quantiles\n",
    "\n",
    "### SAS: PROC UNIVARIATE\n",
    "```sas\n",
    "PROC UNIVARIATE DATA=claims;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=percentiles \n",
    "        PCTLPTS=25 50 75 90 95 99\n",
    "        PCTLPRE=P;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: PERCENTILE_CONT\n",
    "```sql\n",
    "SELECT \n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_charge) AS P25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_charge) AS P50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_charge) AS P75,\n",
    "    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY total_charge) AS P90,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_charge) AS P95,\n",
    "    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY total_charge) AS P99\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Moving Averages (Smoothing)\n",
    "\n",
    "### SAS: Rolling Average\n",
    "```sas\n",
    "DATA moving_avg;\n",
    "    SET monthly_data;\n",
    "    avg_3mo = MEAN(total_incurred, LAG(total_incurred), LAG2(total_incurred));\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: Window Function with ROWS\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    AVG(total_incurred) OVER (\n",
    "        ORDER BY month \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) AS avg_3mo\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Quick Translation Guide\n",
    "\n",
    "| **SAS** | **Databricks** | **Notes** |\n",
    "|---------|----------------|-----------|\n",
    "| `PROC SQL` | SQL queries | Almost identical! |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregations | Very similar |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | Same logic |\n",
    "| `DATA` step | `SELECT` with transformations | Different syntax, same result |\n",
    "| `LAG()` | `LAG() OVER (ORDER BY)` | Window function needed |\n",
    "| `RETAIN` | Window functions | Use cumulative sums |\n",
    "| `MERGE` | `JOIN` | SQL joins |\n",
    "| `WHERE` | `WHERE` | Identical! |\n",
    "| `IF-THEN-ELSE` | `CASE WHEN` | Different syntax |\n",
    "| Macros | Widgets + parameters | Similar concept |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips for SAS Users\n",
    "\n",
    "1. **PROC SQL knowledge transfers 90%**: If you're comfortable with SAS PROC SQL, you'll pick up Databricks quickly!\n",
    "\n",
    "2. **Window functions = LAG/LEAD on steroids**: More powerful than SAS LAG functions.\n",
    "\n",
    "3. **No DATA step needed**: Most transformations can be done in SQL with `CASE WHEN`.\n",
    "\n",
    "4. **CTEs are your friend**: Use `WITH` clauses instead of creating intermediate datasets.\n",
    "\n",
    "5. **Display > PROC PRINT**: Just use `display()` in Python cells or `SELECT` in SQL.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a4c22b-5201-45b4-b589-7374ec5007c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Best Practices & Performance Tips\n",
    "\n",
    "## üöÄ Performance Optimization\n",
    "\n",
    "### 1. **Use Partitioning for Large Tables**\n",
    "```python\n",
    "# Partition by date for time-series data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"claim_date\") \\\n",
    "    .saveAsTable(\"payer_gold.claims_partitioned\")\n",
    "```\n",
    "\n",
    "### 2. **Enable Z-Ordering for Common Filters**\n",
    "```sql\n",
    "OPTIMIZE payer_gold.claims_enriched\n",
    "ZORDER BY (member_id, claim_date);\n",
    "```\n",
    "\n",
    "### 3. **Use Caching for Frequently Accessed DataFrames**\n",
    "```python\n",
    "claims_df = spark.table(\"payer_silver.claims\").cache()\n",
    "# Now use claims_df multiple times without re-reading\n",
    "```\n",
    "\n",
    "### 4. **Broadcast Small Tables in Joins**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîí Data Quality Best Practices\n",
    "\n",
    "### 1. **Always Validate Data**\n",
    "```python\n",
    "# Add constraints\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE payer_silver.claims \n",
    "    ADD CONSTRAINT valid_charge CHECK (total_charge > 0)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. **Use Schema Evolution Carefully**\n",
    "```python\n",
    "# Explicitly define schema for production\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"total_charge\", DoubleType(), True),\n",
    "    # ... more fields\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. **Implement Data Quality Checks**\n",
    "```python\n",
    "def validate_claims(df):\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    checks = {\n",
    "        \"null_claim_ids\": df.filter(col(\"claim_id\").isNull()).count(),\n",
    "        \"negative_charges\": df.filter(col(\"total_charge\") < 0).count(),\n",
    "        \"future_dates\": df.filter(col(\"claim_date\") > current_date()).count()\n",
    "    }\n",
    "    return checks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ Delta Lake Best Practices\n",
    "\n",
    "### 1. **Regular Maintenance**\n",
    "```sql\n",
    "-- Compact small files\n",
    "OPTIMIZE payer_gold.claims_enriched;\n",
    "\n",
    "-- Remove old versions (keep 7 days)\n",
    "VACUUM payer_gold.claims_enriched RETAIN 168 HOURS;\n",
    "\n",
    "-- Update statistics\n",
    "ANALYZE TABLE payer_gold.claims_enriched COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "### 2. **Use Time Travel for Auditing**\n",
    "```sql\n",
    "-- Query previous version\n",
    "SELECT * FROM payer_gold.claims_enriched VERSION AS OF 1;\n",
    "\n",
    "-- Query as of timestamp\n",
    "SELECT * FROM payer_gold.claims_enriched TIMESTAMP AS OF '2025-01-01';\n",
    "```\n",
    "\n",
    "### 3. **Enable Change Data Feed**\n",
    "```sql\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Best Practices\n",
    "\n",
    "### 1. **Medallion Layer Guidelines**\n",
    "- **Bronze**: Keep all source data, minimal transformation\n",
    "- **Silver**: One source system = one silver table (usually)\n",
    "- **Gold**: Many silver tables ‚Üí one gold table (join/aggregate)\n",
    "\n",
    "### 2. **Naming Conventions**\n",
    "```\n",
    "Catalog: <organization>_<environment>\n",
    "Schema: <domain>_<layer>\n",
    "Table: <entity>_<descriptor>\n",
    "\n",
    "Examples:\n",
    "- acme_prod.payer_bronze.claims_raw\n",
    "- acme_dev.payer_silver.claims_cleaned\n",
    "- acme_prod.payer_gold.member_360_view\n",
    "```\n",
    "\n",
    "### 3. **Documentation**\n",
    "```sql\n",
    "-- Add table comments\n",
    "COMMENT ON TABLE payer_gold.claims_enriched IS \n",
    "'Enriched claims with member and provider details for analytics';\n",
    "\n",
    "-- Add column comments\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "ALTER COLUMN total_charge COMMENT 'Total charged amount in USD';\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c63201e-9b73-4adf-8dcb-64a5716c6bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ Quick Reference Guide - SQL & PySpark\n",
    "\n",
    "## Common PySpark Operations\n",
    "\n",
    "### Reading Data\n",
    "```python\n",
    "# From Delta table\n",
    "df = spark.table(\"catalog.schema.table\")\n",
    "\n",
    "# From CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/file.csv\")\n",
    "\n",
    "# From JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# From Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Writing Data\n",
    "```python\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "# Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"table_name\")\n",
    "\n",
    "# With partitioning\n",
    "df.write.format(\"delta\").partitionBy(\"date_col\").saveAsTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Select columns\n",
    "df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Filter rows\n",
    "df.filter(col(\"amount\") > 100)\n",
    "df.where(\"amount > 100\")\n",
    "\n",
    "# Add new column\n",
    "df.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# Rename column\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "# Drop column\n",
    "df.drop(\"col_name\")\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    avg(\"amount\").alias(\"average\")\n",
    ")\n",
    "\n",
    "# Join tables\n",
    "df1.join(df2, \"key_column\")\n",
    "df1.join(df2, df1.key == df2.key, \"left\")\n",
    "\n",
    "# Sort\n",
    "df.orderBy(\"col_name\")\n",
    "df.orderBy(col(\"col_name\").desc())\n",
    "\n",
    "# Remove duplicates\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "### Common Functions\n",
    "```python\n",
    "# String functions\n",
    "upper(\"col_name\")\n",
    "lower(\"col_name\")\n",
    "trim(\"col_name\")\n",
    "concat(\"col1\", \"col2\")\n",
    "substring(\"col_name\", 1, 5)\n",
    "\n",
    "# Date functions\n",
    "current_date()\n",
    "current_timestamp()\n",
    "date_format(\"date_col\", \"yyyy-MM-dd\")\n",
    "year(\"date_col\")\n",
    "month(\"date_col\")\n",
    "datediff(\"date1\", \"date2\")\n",
    "\n",
    "# Math functions\n",
    "round(\"col_name\", 2)\n",
    "abs(\"col_name\")\n",
    "ceil(\"col_name\")\n",
    "floor(\"col_name\")\n",
    "\n",
    "# Conditional logic\n",
    "when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    "\n",
    "# Null handling\n",
    "col(\"col_name\").isNull()\n",
    "col(\"col_name\").isNotNull()\n",
    "coalesce(\"col1\", \"col2\", lit(0))\n",
    "```\n",
    "\n",
    "## Common SQL Operations\n",
    "\n",
    "### DDL Commands\n",
    "```sql\n",
    "-- Create database\n",
    "CREATE DATABASE IF NOT EXISTS database_name;\n",
    "\n",
    "-- Drop database\n",
    "DROP DATABASE IF EXISTS database_name CASCADE;\n",
    "\n",
    "-- Create table\n",
    "CREATE TABLE table_name (\n",
    "    id STRING,\n",
    "    amount DOUBLE,\n",
    "    date DATE\n",
    ");\n",
    "\n",
    "-- Drop table\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "-- Describe table\n",
    "DESCRIBE EXTENDED table_name;\n",
    "SHOW COLUMNS FROM table_name;\n",
    "```\n",
    "\n",
    "### DML Commands\n",
    "```sql\n",
    "-- Insert data\n",
    "INSERT INTO table_name VALUES (1, 'value1', 100);\n",
    "\n",
    "-- Update data (Delta Lake)\n",
    "UPDATE table_name SET amount = 200 WHERE id = 1;\n",
    "\n",
    "-- Delete data (Delta Lake)\n",
    "DELETE FROM table_name WHERE id = 1;\n",
    "\n",
    "-- Merge (Upsert)\n",
    "MERGE INTO target_table\n",
    "USING source_table\n",
    "ON target_table.id = source_table.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "### Query Commands\n",
    "```sql\n",
    "-- Basic SELECT\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "\n",
    "-- With WHERE clause\n",
    "SELECT * FROM table_name WHERE amount > 100;\n",
    "\n",
    "-- Aggregations\n",
    "SELECT category, COUNT(*), SUM(amount), AVG(amount)\n",
    "FROM table_name\n",
    "GROUP BY category;\n",
    "\n",
    "-- Joins\n",
    "SELECT a.*, b.name\n",
    "FROM table_a a\n",
    "INNER JOIN table_b b ON a.id = b.id;\n",
    "\n",
    "-- Window functions\n",
    "SELECT \n",
    "    *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as rank\n",
    "FROM table_name;\n",
    "\n",
    "-- CTE (Common Table Expression)\n",
    "WITH summary AS (\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM table_name\n",
    "    GROUP BY category\n",
    ")\n",
    "SELECT * FROM summary WHERE total > 1000;\n",
    "```\n",
    "\n",
    "## Databricks Utilities\n",
    "```python\n",
    "# File system operations\n",
    "dbutils.fs.ls(\"path/\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "dbutils.fs.rm(\"path/\", recurse=True)\n",
    "dbutils.fs.mkdirs(\"path/\")\n",
    "\n",
    "# Widgets (parameters)\n",
    "dbutils.widgets.text(\"param_name\", \"default_value\")\n",
    "param_value = dbutils.widgets.get(\"param_name\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, {\"param\": \"value\"})\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5cb1313-051c-4ea6-ae57-c2ca3a3ad4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Lazy Evaluation & Deterministic Execution - Quick Reference\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Best Practices Checklist for HQRI Gold Layer Pipelines\n",
    "\n",
    "| Scenario | Recommended Approach | Why? |\n",
    "|----------|---------------------|------|\n",
    "| **Interactive analysis** on same dataset | `.cache()` or `.persist()` | Fast in-memory reuse |\n",
    "| **Production ETL** between pipeline stages | `.write.saveAsTable()` | Durable, auditable, time travel |\n",
    "| **Multiple branches** from same transformation | `.cache()` + `.count()` | Forces evaluation once, reuse many times |\n",
    "| **Very wide transformations** (many joins) | Write to Delta checkpoint | Breaks lineage, fault-tolerant |\n",
    "| **Data quality validation** | `.count()` after each stage | Forces execution, verifies row counts |\n",
    "| **Reusable business logic** | CREATE VIEW | Dynamic, always reflects latest data |\n",
    "| **Materialized analytics tables** | CREATE TABLE AS SELECT | Static, fast queries, predictable |\n",
    "\n",
    "---\n",
    "\n",
    "### üö® Common Pitfalls to Avoid\n",
    "\n",
    "#### ‚ùå Pitfall 1: Recomputing expensive transformations\n",
    "```python\n",
    "# BAD: Recomputes join 3 times\n",
    "df = members.join(claims, \"member_id\")\n",
    "count1 = df.count()\n",
    "count2 = df.filter(col(\"age\") > 65).count()\n",
    "count3 = df.groupBy(\"plan_id\").count().count()\n",
    "\n",
    "# GOOD: Cache and reuse\n",
    "df = members.join(claims, \"member_id\").cache()\n",
    "df.count()  # Trigger caching\n",
    "count1 = df.count()\n",
    "count2 = df.filter(col(\"age\") > 65).count()\n",
    "count3 = df.groupBy(\"plan_id\").count().count()\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "#### ‚ùå Pitfall 2: Not forcing evaluation at checkpoints\n",
    "```python\n",
    "# BAD: Error only surfaces at the end\n",
    "df1 = spark.table(\"table1\").filter(col(\"bad_column\") == 1)  # Typo in column name\n",
    "df2 = df1.join(other_table, \"id\")\n",
    "df3 = df2.groupBy(\"plan_id\").count()\n",
    "df3.write.saveAsTable(\"output\")  # ERROR HERE - hard to debug!\n",
    "\n",
    "# GOOD: Validate early\n",
    "df1 = spark.table(\"table1\").filter(col(\"bad_column\") == 1)\n",
    "print(f\"Stage 1: {df1.count()} rows\")  # ERROR HERE - easy to debug!\n",
    "df2 = df1.join(other_table, \"id\")\n",
    "print(f\"Stage 2: {df2.count()} rows\")\n",
    "df3 = df2.groupBy(\"plan_id\").count()\n",
    "df3.write.saveAsTable(\"output\")\n",
    "```\n",
    "\n",
    "#### ‚ùå Pitfall 3: Caching too early or too much\n",
    "```python\n",
    "# BAD: Caching before filtering (wastes memory)\n",
    "df = spark.table(\"claims\").cache()\n",
    "df.count()\n",
    "filtered = df.filter(col(\"total_charge\") > 1000)\n",
    "\n",
    "# GOOD: Filter first, then cache\n",
    "df = spark.table(\"claims\").filter(col(\"total_charge\") > 1000).cache()\n",
    "df.count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Decision Tree: When to Use Each Technique\n",
    "\n",
    "```\n",
    "Start: Do you need the result more than once?\n",
    "‚îÇ\n",
    "‚îú‚îÄ NO ‚Üí Don't cache, let Spark optimize\n",
    "‚îÇ\n",
    "‚îî‚îÄ YES ‚Üí Is this a production pipeline?\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ YES ‚Üí Write to Delta table (.saveAsTable)\n",
    "    ‚îÇ        ‚Ä¢ Durable across cluster restarts\n",
    "    ‚îÇ        ‚Ä¢ Enables time travel\n",
    "    ‚îÇ        ‚Ä¢ Can be read by other systems\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ NO ‚Üí Is it used in the same session?\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ YES ‚Üí Use .cache() or .persist()\n",
    "        ‚îÇ        ‚Ä¢ Fast in-memory access\n",
    "        ‚îÇ        ‚Ä¢ Remember to .unpersist() when done\n",
    "        ‚îÇ\n",
    "        ‚îî‚îÄ NO ‚Üí Create a VIEW\n",
    "                 ‚Ä¢ Reusable across sessions\n",
    "                 ‚Ä¢ Dynamic (always fresh data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Summary: Lazy Evaluation Principles\n",
    "\n",
    "1. **Transformations are lazy** - build execution plan without running\n",
    "2. **Actions are eager** - trigger execution of entire plan\n",
    "3. **Cache for reuse** - avoid recomputing expensive transformations\n",
    "4. **Write checkpoints** - break lineage and ensure durability\n",
    "5. **Force evaluation** - use `.count()` to validate at each stage\n",
    "6. **Clean up** - `.unpersist()` to free memory\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4668d8c0-5c38-4141-bc8a-c39ad7a6165d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Keep this reference handy as you build your data pipelines!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ba72b2-01c9-40c2-b72e-36abd16002d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[Reference] Best Practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
