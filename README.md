# Databricks Workshop: HQRI Risk Adjustment & Analytics

This workshop is specifically designed for **healthcare data analysts, data engineers, and SAS users** who are new to Databricks. It provides a hands-on introduction to **Medicare risk adjustment analytics** using Databricks' medallion architecture (Bronze/Silver/Gold layers), with a strong focus on **HCC risk scoring**, **CMS encounter datamart**, **data quality**, and **performance optimization**.

## ğŸš€ Overview

This interactive workshop guides you through:
- ğŸ’° **HCC Risk Score Calculations**: Calculate Medicare Advantage risk scores for CMS payment determination
- ğŸ“Š **Encounter Datamart**: Build CMS submission-ready encounter data with validations
- â­ **Star Ratings Analytics**: Revenue impact analysis and member stratification
- ğŸ” **Data Quality Audits**: Comprehensive compliance checks for regulatory requirements
- ğŸ’» **SAS to Databricks Migration**: Side-by-side comparisons of familiar SAS PROC steps
- âš¡ **Lazy Evaluation & Optimization**: Best practices for deterministic, production-ready pipelines
- ğŸ¯ **SQL & PySpark**: Hands-on examples with both approaches


## ğŸ“‚ Medallion Architecture

- **Bronze Layer (Raw Data):** Raw data ingestion from CSV files using `COPY INTO` - preserves original data for audit trails
- **Silver Layer (Cleaned Data):** Data cleansing, deduplication, type corrections, and standardization
- **Gold Layer (Business Analytics ğŸ‰):** HCC risk scoring, revenue forecasting, compliance audits, and production pipelines

This modular pattern ensures data lineage, scalability, ACID compliance, and aligns with regulatory requirements for Medicare Advantage reporting.

## ğŸ—ï¸ Features

### ğŸ¯ Gold Layer Analytics Examples

* Example 1: HCC Risk Score Calculation
* Example 2: Revenue Forecast & Impact Analysis
* Example 3: HCC Distribution Analysis
* Example 4: Data Quality & Compliance Audit
* Example 5: Member Risk Stratification
* Example 6: Provider Performance on Risk Capture
* Example 7: Encounter Datamart for CMS Submission
* Example 8: Lazy Evaluation & Deterministic Execution âš¡

### ğŸ’» SAS to Databricks Migration
- **Side-by-side comparisons**: SAS PROC SQL â†’ Databricks SQL/PySpark
- **Modern functions**: COLLECT_SET(), EXPLODE(), window functions
- **Performance advantages**: Distributed processing vs. single-server SAS
- **Cost benefits**: Pay-per-use vs. expensive SAS licensing
- **Migration best practices**: CTE-based queries, array operations, caching strategies

### ğŸ› ï¸ Technical Features
- **Unity Catalog**: Unified governance, row/column-level security
- **Delta Lake**: ACID transactions, time travel, schema evolution
- **Predictive Optimization**: Automatic table maintenance and optimization
- **AI/BI & Genie**: Natural language queries and self-service analytics
- **Production-ready patterns**: Checkpointing, caching, deterministic execution


## ğŸ“‹ HQRI Data Model

**Datasets (Bronze â†’ Silver â†’ Gold):**
- **Members**: Medicare Advantage enrollees (demographics, plan info, enrollment dates)
- **Claims**: Medical encounters with diagnosis codes for HCC mapping
- **Providers**: Healthcare providers (NPI, specialty, location)
- **Diagnoses**: ICD-10 diagnosis codes linked to claims
- **Procedures**: CPT/HCPCS procedure codes and charges

**Reference Data:**
- **HCC Reference Table**: ICD-10 to HCC category mapping with coefficients

**Gold Layer Analytics Tables Created:**
- `member_risk_scores`: Member-level HCC risk scores and projected payments
- `revenue_forecast`: Revenue projections by plan and risk tier
- `hcc_distribution`: HCC category revenue impact analysis
- `data_quality_audit`: CMS submission validation results
- `member_risk_stratification`: Risk tier segmentation for care management
- `provider_risk_capture_performance`: Provider HCC documentation performance
- `encounter_datamart_cms`: CMS-ready encounter submission table
- `enriched_claims_checkpoint`: Production pipeline checkpoint example


## ğŸ› ï¸ Getting Started

### Prerequisites
- Databricks workspace (Community Edition or higher)
- Basic SQL knowledge
- Familiarity with healthcare payer data (helpful but not required)
- No prior Spark/PySpark experience needed

### Quick Start (5 minutes)

**In Databricks:**
1. Open the notebook `DBX Workshop_HQRI_11142025.ipynb` in your workspace
2. Run the setup cells to configure catalog, schemas, and load data
3. Follow along with examples sequentially:
4. Work through hands-on exercises and experiment with your own queries!


## ğŸ“‘ Project Structure

```
â”œâ”€â”€ DBX Workshop_HQRI_11142025.ipynb             â­ Training notebook
â”œâ”€â”€ [Reference] Best Practices                    ğŸ“š Best practices guide
â”œâ”€â”€ README.md                                      ğŸ“– This file
â”œâ”€â”€ LICENSE.md                                      ğŸ“– License
â””â”€â”€ data/
    â”œâ”€â”€ claims.csv                                 ğŸ’° Medical encounters
    â”œâ”€â”€ diagnoses.csv                             ğŸ¥ ICD-10 diagnosis codes
    â”œâ”€â”€ procedures.csv                            ğŸ”¬ CPT/HCPCS procedure codes
    â”œâ”€â”€ providers.csv                             ğŸ‘¨â€âš•ï¸ Healthcare providers (with NPI)
    â”œâ”€â”€ member.csv                                 ğŸ‘¥ Medicare Advantage enrollees
    â””â”€â”€ Payor_Archive.zip                          ğŸ“¦ Source data archive
```

---

## ğŸ¯ Workshop Objectives Summary

By the end of this workshop, you will be able to:

1. âœ… Build **Medallion Architecture** pipelines for Medicare risk adjustment data
2. âœ… Calculate **HCC risk scores** and project CMS payments
3. âœ… Create **CMS encounter datamart** tables with validation
4. âœ… Perform **data quality audits** for regulatory compliance
5. âœ… Build **Gold layer analytics** for revenue optimization
6. âœ… Implement **lazy evaluation** and **deterministic execution** best practices
7. âœ… Migrate **SAS workflows** to Databricks efficiently

---

### Â© 2025 | Healthcare Quality Reporting & Improvement (HQRI) Analytics Workshop
**Target Audience:** Healthcare data analysts, data engineers, and SAS users transitioning to Databricks  
**Difficulty Level:** Beginner to intermediate  
**Focus Areas:** Medicare Advantage, HCC risk adjustment, CMS submissions, performance optimization

*Last updated: November 14, 2025*