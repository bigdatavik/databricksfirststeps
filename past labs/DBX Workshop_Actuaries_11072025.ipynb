{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419c4a6c-a8be-40cd-b902-3fd8312c8ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Updated date: November 7, 2025_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks for Actuaries: Healthcare Analytics Workshop\n",
    "**A Hands-On Guide for SAS Users Transitioning to Databricks**\n",
    "\n",
    "---\n",
    "\n",
    "## üëã Welcome Actuaries & Analysts!\n",
    "\n",
    "This workshop is designed specifically for **actuaries and analysts** who are familiar with **SAS** and want to learn Databricks. Don't worry if you're new to Python or SQL - we'll guide you step by step!\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand how Databricks compares to your SAS workflows\n",
    "2. ‚úÖ Load and query healthcare payer data using **simple SQL and PySpark**\n",
    "3. ‚úÖ Perform actuarial analyses you're familiar with (loss ratios, trends, reserving)\n",
    "4. ‚úÖ Create interactive visualizations without complex code\n",
    "5. ‚úÖ Build analytics tables for pricing, reserving, and risk management\n",
    "\n",
    "---\n",
    "\n",
    "### üè• Dataset Overview\n",
    "\n",
    "We'll work with **healthcare payer data** including:\n",
    "- **Members**: Health plan enrollees\n",
    "- **Claims**: Medical claim submissions\n",
    "- **Providers**: Healthcare providers\n",
    "- **Diagnoses**: Diagnosis codes from claims\n",
    "- **Procedures**: Medical procedures performed\n",
    "\n",
    "**Think of it as**: Claims = Losses, Members = Policies, Providers = Service Providers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04992b09-9e35-4995-8121-fb7bd4f9a40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data Model\n",
    "\n",
    "For a payer, commonly used tables include:\n",
    "\n",
    "- **Members**: members enrolled in a health plan\n",
    "- **Claims**: medical claim submissions\n",
    "- **Providers**: healthcare providers (doctors, clinics)\n",
    "- **Diagnoses**: claim diagnosis codes\n",
    "- **Procedures**: procedures/services performed\n",
    "\n",
    "Each table should have at least 50 rows.\n",
    "\n",
    "<img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/bdd54dc0-f3c7-4975-80a3-0017ebdb121c.png\" alt=\"Managed Tables\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe39bed2-7a8e-4992-8c17-3df323aec010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction for Actuaries\n",
    "\n",
    "## ü§î Why Databricks for Actuaries?\n",
    "\n",
    "If you're coming from **SAS**, you might be wondering: \"Why learn another tool?\"\n",
    "\n",
    "### Here's Why:\n",
    "- **Scalability**: Handle millions of claims instantly (no more waiting for PROC SQL!)\n",
    "- **Modern Analytics**: Built-in ML, real-time dashboards, and collaboration\n",
    "- **Cost-Effective**: Cloud-based, pay only for what you use\n",
    "- **Still Use SQL**: 90% of your SAS PROC SQL knowledge transfers directly!\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ SAS vs Databricks: Quick Comparison\n",
    "\n",
    "| **What You Do in SAS** | **How You Do It in Databricks** | **Difficulty** |\n",
    "|------------------------|----------------------------------|----------------|\n",
    "| `PROC SQL` | SQL queries (almost identical!) | ‚≠ê Easy |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregate functions | ‚≠ê Easy |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | ‚≠ê Easy |\n",
    "| `DATA` step | SQL `SELECT` or simple Python | ‚≠ê‚≠ê Moderate |\n",
    "| `PROC EXPAND` (trending) | Window functions | ‚≠ê‚≠ê Moderate |\n",
    "| Macros | Parameters + reusable queries | ‚≠ê‚≠ê‚≠ê Learning curve |\n",
    "\n",
    "**Good News**: Most of what you do can be done with **SQL alone**!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üè† What is a Lakehouse? (Simple Explanation)\n",
    "\n",
    "**For Actuaries**: Think of it as a **super-powered SAS library** that:\n",
    "- Stores all your data in one place (claims, policies, members)\n",
    "- Lets you analyze it with SQL (like PROC SQL)\n",
    "- Handles millions of rows instantly\n",
    "- Keeps track of all changes (audit trail)\n",
    "- Lets multiple people work at once (no locking issues!)\n",
    "\n",
    "**Key Benefit**: Unlike SAS datasets, you can query **billions** of claims in seconds!\n",
    "\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png\" alt=\"Lakehouse\" width=\"500\" height=\"350\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Unity Catalog (Data Organization - Like SAS Libraries)\n",
    "\n",
    "**For Actuaries**: Think of Unity Catalog as your **SAS library structure**, but better organized:\n",
    "\n",
    "```\n",
    "In SAS:                    In Databricks:\n",
    "LIBNAME.DATASET            CATALOG.SCHEMA.TABLE\n",
    "  ‚Üì                           ‚Üì\n",
    "work.claims        ‚Üí       my_catalog.payer_bronze.claims\n",
    "actuarial.loss_ratios ‚Üí    my_catalog.payer_gold.loss_ratios\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Everyone sees the same data (no duplicate datasets!)\n",
    "- ‚úÖ Built-in security (control who can see PHI/PII)\n",
    "- ‚úÖ Audit trail (track all data access)\n",
    "- ‚úÖ Easy to find data (searchable catalog)\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086\" alt=\"Unity Catalog\" width=\"500\" height=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ü•âü•àü•á Medallion Architecture (Your Data Quality Layers)\n",
    "\n",
    "**For Actuaries**: This is like your SAS data prep workflow, but organized into layers:\n",
    "\n",
    "### üì• Bronze (Raw Data) \n",
    "- **Like**: Your raw claims extracts from source systems\n",
    "- **Contains**: Data exactly as received (CSV, database extracts)\n",
    "- **Example**: Raw claims file from claims system\n",
    "- **Today**: We'll load this quickly!\n",
    "\n",
    "### üîß Silver (Cleaned Data)\n",
    "- **Like**: Your cleaned/standardized SAS datasets\n",
    "- **Contains**: Deduplicated, standardized data\n",
    "- **Example**: Claims with proper data types, duplicates removed\n",
    "- **Today**: We'll auto-clean this!\n",
    "\n",
    "### ‚≠ê Gold (Analytics Tables)\n",
    "- **Like**: Your final analysis datasets (loss triangles, premium summaries)\n",
    "- **Contains**: Business-ready tables for actuarial analysis\n",
    "- **Example**: Loss ratios, development factors, IBNR estimates\n",
    "- **Today**: This is where we'll spend most time! üéâ\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Medallion Architecture\" width=\"500\" height=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP\n",
    "Just run next couple of cells for setup! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975b322a-404a-44b6-915a-aeb68af6d08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"my_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\"\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fcaff8f-cb35-4a44-a949-a9fe0070e4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")\n",
    "\n",
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677fecfc-f600-4d95-a141-905af1f9e2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze Layer ‚Äì Ingest Raw Data\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Bronze Layer?\n",
    "\n",
    "The **Bronze Layer** is the landing zone for raw data. Here we:\n",
    "- üìÇ Load data \"as-is\" from source files (CSV, JSON, Parquet, etc.)\n",
    "- üíæ Store in Delta Lake format for ACID transactions\n",
    "- üìù Apply minimal transformation (just schema inference)\n",
    "- ‚è±Ô∏è Keep historical data for audit and reprocessing\n",
    "\n",
    "> **üí° Best Practice**: Use `COPY INTO` for incremental, idempotent loading from raw data files into Delta Lake tables. It automatically skips already-loaded files!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/my_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load Claims Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files.\n",
    "\n",
    "\n",
    "-- Load Diagnosis Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Members Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Procedures Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Providers Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/my_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîß Silver Layer ‚Äì Transform, Clean, and Join\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Silver Layer?\n",
    "\n",
    "The **Silver Layer** is where we transform raw data into clean, validated, and enriched datasets. Here we:\n",
    "\n",
    "- üßπ **Clean**: Remove nulls, trim whitespace, fix data quality issues\n",
    "- üîÑ **Transform**: Cast data types, standardize formats\n",
    "- üóëÔ∏è **Deduplicate**: Remove duplicate records based on business keys\n",
    "- üîç **Validate**: Apply business rules and data quality checks\n",
    "- üìä **Enrich**: Join related tables, calculate derived columns\n",
    "\n",
    "> **üí° Best Practice**: Silver tables should be \"analytics-ready\" ‚Äì cleaned, validated, and properly typed!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6466f560-5e31-4b0d-b8bd-9ed70734a0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc2dda1-5dc9-47c3-80d7-588fd34eac40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! (3 mins)\n",
    "Ask Databricks Assistant: \"How do I calculate the total claims by specialty in SQL?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì Actuarial Analytics (The Fun Part!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is the Gold Layer? (For Actuaries)\n",
    "\n",
    "**This is where YOU spend most of your time!** The Gold Layer is like your **final SAS analysis datasets** - ready for actuarial work.\n",
    "\n",
    "### What We'll Build (Actuarial Examples):\n",
    "\n",
    "1. **Loss Ratios by Segment**\n",
    "2. **Claims Trending Analysis**\n",
    "3. **Claims Development Triangle**\n",
    "4. **High-Risk Member Identification**\n",
    "5. **Frequency & Severity Analysis by Demographics**\n",
    "6. **Data Quality Checks for Actuarial Analysis**\n",
    "7. **Bias Detection in Healthcare Data**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How This Compares to SAS\n",
    "\n",
    "| **Your SAS Workflow** | **In Databricks Gold Layer** |\n",
    "|----------------------|------------------------------|\n",
    "| Create final analysis dataset | Create Gold table |\n",
    "| PROC SQL with aggregations | SQL SELECT with GROUP BY |\n",
    "| PROC MEANS for summary stats | Aggregate functions (AVG, SUM, etc.) |\n",
    "| Multiple DATA steps for calcs | Single SQL statement with CTEs |\n",
    "| Macros for repeated calcs | Parameterized queries |\n",
    "| Export to Excel for viz | Built-in interactive charts! |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Your Actuarial Toolbox\n",
    "\n",
    "Today you'll learn SQL equivalents for common actuarial analyses:\n",
    "\n",
    "- **Loss Ratios**: `SUM(claims)/SUM(premium)`\n",
    "- **Trending**: Window functions (`LAG`, `LEAD`)\n",
    "- **Development Factors**: `GROUP BY` claim year + development period\n",
    "- **Percentiles**: `PERCENTILE_CONT` function\n",
    "- **Risk Scores**: `CASE WHEN` logic\n",
    "\n",
    "**Ready?** Let's start building! üöÄ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9923eb4-1dec-4ecd-923c-7a3759104c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 1: Loss Ratios by Segment\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What is our loss ratio by provider specialty and state?\"**\n",
    "\n",
    "This is a **fundamental actuarial metric** - you probably calculate this quarterly or annually!\n",
    "\n",
    "### üìù SAS vs Databricks\n",
    "\n",
    "**In SAS, you might write:**\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE loss_ratios AS\n",
    "    SELECT \n",
    "        p.specialty,\n",
    "        p.state,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(c.total_charge) AS incurred_losses,\n",
    "        ROUND(SUM(c.total_charge) / COUNT(*), 0.01) AS loss_ratio\n",
    "    FROM claims AS c\n",
    "    LEFT JOIN providers AS p\n",
    "        ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.specialty, p.state;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**In Databricks, we write:**\n",
    "```sql\n",
    "-- Very similar! Most SQL transfers directly.\n",
    "```\n",
    "\n",
    "Let's build this now! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f6f9ad-d859-41b1-ba0a-8b696eff931a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ACTUARIAL ANALYSIS: Loss Ratios by Specialty and State\n",
    "CREATE OR REPLACE TABLE payer_gold.loss_ratios AS\n",
    "SELECT\n",
    "    p.specialty,\n",
    "    p.state,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(c.total_charge) AS incurred_losses,\n",
    "    ROUND(SUM(c.total_charge) / COUNT(*), 2) AS loss_ratio\n",
    "FROM payer_silver.claims c\n",
    "LEFT JOIN payer_silver.providers p\n",
    "    ON c.provider_id = p.provider_id\n",
    "GROUP BY p.specialty, p.state;\n",
    "\n",
    "-- Display the results including loss_ratio\n",
    "SELECT \n",
    "    specialty,\n",
    "    state,\n",
    "    claim_count,\n",
    "    incurred_losses,\n",
    "    loss_ratio\n",
    "FROM payer_gold.loss_ratios;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9641dd2b-d31a-446b-933f-68341e28dce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! (3 mins)\n",
    "Using SAS script example below, ask Assistant to convert into Databricks SQL and Pyspark (_Example prompt: Convert below SAS script into Pyspark_)\n",
    "\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE loss_ratios_summary AS\n",
    "    SELECT \n",
    "        specialty,\n",
    "        state,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(incurred_losses) AS total_incurred_losses,\n",
    "        AVG(loss_ratio) AS avg_loss_ratio\n",
    "    FROM payer_gold.loss_ratios\n",
    "    GROUP BY specialty, state\n",
    "    ORDER BY avg_loss_ratio DESC;\n",
    "QUIT;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee97453-ce7c-42ce-a7bb-3ca47d797784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 2: Claims Trending Analysis\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What are our monthly claim trends? Are claims trending up or down?\"**\n",
    "\n",
    "This is crucial for:\n",
    "- **Rate making** (applying trend factors)\n",
    "- **Reserving** (projecting ultimate losses)\n",
    "- **Budgeting** (forecasting next year's costs)\n",
    "\n",
    "### üìù What We're Calculating\n",
    "\n",
    "```\n",
    "Month-over-Month Growth = (This Month - Last Month) / Last Month\n",
    "Year-over-Year Growth = (This Month - Same Month Last Year) / Same Month Last Year\n",
    "```\n",
    "\n",
    "### üîß SAS Equivalent\n",
    "In SAS, you might use **PROC EXPAND** or **LAG functions** in a DATA step. \n",
    "\n",
    "In Databricks, we use **Window Functions** - specifically `LAG()` and `LEAD()`.\n",
    "\n",
    "Let's build it! üëá\n",
    "\n",
    "Wait, I don't know what is  `LAG()` function - let's ask Assistant!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5751b7-d91f-4bae-a5cd-a2055ed1e04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ACTUARIAL ANALYSIS: Monthly Claims Trending\n",
    "-- Window functions for MoM and YoY calculations\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_trend_analysis AS\n",
    "WITH monthly_claims AS (\n",
    "    -- Step 1: Aggregate claims by month\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        YEAR(claim_date) AS claim_year,\n",
    "        MONTH(claim_date) AS claim_month_num,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(total_charge) AS total_incurred,\n",
    "        ROUND(AVG(total_charge), 2) AS avg_claim_cost\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month, claim_year, claim_month_num\n",
    ")\n",
    "SELECT \n",
    "    claim_month,\n",
    "    claim_count,\n",
    "    total_incurred,\n",
    "    avg_claim_cost,\n",
    "    \n",
    "    -- Month-over-Month Comparison\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY claim_month) AS prior_month_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY claim_month) * 100, 2) AS mom_growth_pct,\n",
    "    \n",
    "    -- Year-over-Year Comparison (12 months ago)\n",
    "    LAG(total_incurred, 12) OVER (ORDER BY claim_month) AS prior_year_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 12) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 12) OVER (ORDER BY claim_month) * 100, 2) AS yoy_growth_pct,\n",
    "    \n",
    "    -- 3-Month Moving Average (for smoothing)\n",
    "    ROUND(AVG(total_incurred) OVER (ORDER BY claim_month \n",
    "                                     ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), 2) AS moving_avg_3mo\n",
    "    \n",
    "FROM monthly_claims\n",
    "ORDER BY claim_month;\n",
    "\n",
    "-- Display the results\n",
    "SELECT * FROM payer_gold.claims_trend_analysis;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "327b52b8-6664-4f4d-b08d-3727a57c49b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 3: Claims Development Triangle\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"How do claims develop over time? What are our age-to-age factors?\"**\n",
    "\n",
    "This is **THE fundamental tool** for actuarial reserving!\n",
    "\n",
    "### üìù What We're Building\n",
    "\n",
    "A **development triangle** shows:\n",
    "- **Rows**: Accident/Policy Year\n",
    "- **Columns**: Development Period (months since occurrence)\n",
    "- **Values**: Cumulative claims at each development point\n",
    "\n",
    "Then we calculate:\n",
    "- **Age-to-Age Factors** (e.g., 12-to-24 month factor)\n",
    "- **Ultimate Loss Projections**\n",
    "\n",
    "### üîß Why This Matters\n",
    "- **IBNR Reserves**: Estimate unreported claims\n",
    "- **Case Reserve Adequacy**: Check if reserves are sufficient\n",
    "- **Rate Adequacy**: Are our prices sufficient?\n",
    "\n",
    "Let's build a simple development view! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffc6e96-200c-4284-9d8a-8f9b3c1109ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ACTUARIAL ANALYSIS: Claims Development Pattern\n",
    "-- Shows how claims emerge over time (by months since occurrence)\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_development AS\n",
    "WITH claim_development AS (\n",
    "    SELECT \n",
    "        c.claim_id,\n",
    "        DATE_TRUNC('YEAR', c.claim_date) AS accident_year,\n",
    "        c.claim_date,\n",
    "        c.total_charge,\n",
    "        m.effective_date AS member_effective_date,\n",
    "        \n",
    "        -- Development period in months (months between member effective date and claim date)\n",
    "        -- This simulates months since policy inception\n",
    "        DATEDIFF(MONTH, m.effective_date, c.claim_date) AS development_months,\n",
    "        \n",
    "        -- Group into development periods (quarterly for simplicity)\n",
    "        CASE \n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 3 THEN '0-3 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 6 THEN '4-6 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 12 THEN '7-12 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 24 THEN '13-24 months'\n",
    "            ELSE '24+ months'\n",
    "        END AS development_period\n",
    "        \n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "    WHERE c.claim_date >= m.effective_date  -- Claims after member enrollment\n",
    ")\n",
    "SELECT \n",
    "    accident_year,\n",
    "    development_period,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS cumulative_incurred,\n",
    "    ROUND(AVG(total_charge), 2) AS avg_claim_size,\n",
    "    \n",
    "    -- Calculate % of claims reported in each period\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY accident_year), 2) AS pct_of_total_claims\n",
    "    \n",
    "FROM claim_development\n",
    "GROUP BY accident_year, development_period\n",
    "ORDER BY accident_year, development_period;\n",
    "\n",
    "-- Display the triangle\n",
    "SELECT * FROM payer_gold.claims_development;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7acaab6f-7ac1-49f5-87bf-0029a42e624e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! (5 mins)\n",
    "Work with Databricks Assistant and answer \"How to estimate ultimate losses by accident year using the development triangle and age-to-age factors in SQL?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e936ab-cb9d-42b5-9315-e9842dab7b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 4: High-Risk Member Identification\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"Which members are high-risk and driving our costs? How do we identify them for care management?\"**\n",
    "\n",
    "This analysis is critical for:\n",
    "- **Care Management**: Target high-risk members for interventions\n",
    "- **Reserving**: Set appropriate case reserves for known high-cost cases\n",
    "- **Pricing**: Understand risk distribution for rate development\n",
    "- **Provider Contracting**: Identify members who may need specialized care\n",
    "\n",
    "### üìù What We're Calculating\n",
    "\n",
    "We'll use **percentile analysis** to identify members whose costs exceed the 95th percentile - a common threshold for \"high-risk\" classification.\n",
    "\n",
    "**Key Metrics:**\n",
    "- 95th percentile of claim costs\n",
    "- Member-level total incurred\n",
    "- Claim frequency by member\n",
    "- Average claim severity\n",
    "\n",
    "### üîß SAS Equivalent\n",
    "In SAS, you might use **PROC UNIVARIATE** for percentiles and then filter:\n",
    "```sas\n",
    "PROC UNIVARIATE DATA=claims;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=pctiles PCTLPTS=95 PCTLPRE=P;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "In Databricks, we use **PERCENTILE_CONT** function combined with CTEs.\n",
    "\n",
    "Let's build it! üëá\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ab9d8c-e0bc-467f-9877-3c904630f84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ACTUARIAL ANALYSIS: High-Risk Member Identification\n",
    "-- Identify members exceeding 95th percentile for targeted interventions\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.high_risk_members AS\n",
    "WITH member_summary AS (\n",
    "    -- Step 1: Aggregate claims by member\n",
    "    SELECT \n",
    "        c.member_id,\n",
    "        COUNT(c.claim_id) AS claim_count,\n",
    "        SUM(c.total_charge) AS total_incurred,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost,\n",
    "        MIN(c.claim_date) AS first_claim_date,\n",
    "        MAX(c.claim_date) AS last_claim_date,\n",
    "        COUNT(DISTINCT YEAR(c.claim_date)) AS years_with_claims\n",
    "    FROM payer_silver.claims c\n",
    "    GROUP BY c.member_id\n",
    "),\n",
    "risk_threshold AS (\n",
    "    -- Step 2: Calculate 95th percentile threshold\n",
    "    SELECT \n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_incurred) AS p95_threshold,\n",
    "        PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY total_incurred) AS p90_threshold,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_incurred) AS p75_threshold\n",
    "    FROM member_summary\n",
    "),\n",
    "risk_classification AS (\n",
    "    -- Step 3: Classify members by risk tier\n",
    "    SELECT \n",
    "        ms.*,\n",
    "        ROUND(ms.total_incurred / ms.claim_count, 2) AS severity,\n",
    "        CASE \n",
    "            WHEN ms.total_incurred >= rt.p95_threshold THEN 'Critical Risk (95th+ percentile)'\n",
    "            WHEN ms.total_incurred >= rt.p90_threshold THEN 'High Risk (90th-95th percentile)'\n",
    "            WHEN ms.total_incurred >= rt.p75_threshold THEN 'Moderate Risk (75th-90th percentile)'\n",
    "            ELSE 'Standard Risk'\n",
    "        END AS risk_tier,\n",
    "        rt.p95_threshold,\n",
    "        rt.p90_threshold,\n",
    "        rt.p75_threshold\n",
    "    FROM member_summary ms\n",
    "    CROSS JOIN risk_threshold rt\n",
    ")\n",
    "SELECT \n",
    "    rc.*,\n",
    "    m.first_name,\n",
    "    m.last_name,\n",
    "    m.gender,\n",
    "    m.birth_date,\n",
    "    YEAR(CURRENT_DATE()) - YEAR(m.birth_date) AS age,\n",
    "    m.plan_id,\n",
    "    -- m.state removed because it does not exist\n",
    "    DATEDIFF(DAY, rc.last_claim_date, CURRENT_DATE()) AS days_since_last_claim\n",
    "FROM risk_classification rc\n",
    "INNER JOIN payer_silver.members m ON rc.member_id = m.member_id\n",
    "WHERE rc.total_incurred >= rc.p75_threshold  -- Focus on moderate risk and above\n",
    "ORDER BY rc.total_incurred DESC;\n",
    "\n",
    "-- Display top 20 high-risk members\n",
    "SELECT * FROM payer_gold.high_risk_members LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "099d2455-31cc-42eb-9fb7-3de4d1a70a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 5: Frequency & Severity Analysis by Demographics\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"How do claims frequency and severity vary by age and gender? What are the key rating factors?\"**\n",
    "\n",
    "This is **fundamental for pricing** and understanding your book of business:\n",
    "- **Rate Development**: Age/gender are primary rating factors\n",
    "- **Experience Analysis**: Compare actual vs expected by demographic cell\n",
    "- **Product Design**: Understand which demographics drive costs\n",
    "- **Underwriting**: Identify profitable vs unprofitable segments\n",
    "\n",
    "### üìù What We're Calculating\n",
    "\n",
    "```\n",
    "Frequency = Number of Claims / Number of Members (claims per member per year)\n",
    "Severity = Total Incurred / Number of Claims (average cost per claim)\n",
    "Pure Premium = Frequency √ó Severity (expected cost per member)\n",
    "```\n",
    "\n",
    "### üîß SAS Equivalent\n",
    "In SAS, you might use:\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE freq_sev AS\n",
    "    SELECT \n",
    "        age_group,\n",
    "        gender,\n",
    "        COUNT(*) as claim_count,\n",
    "        COUNT(DISTINCT member_id) as member_count,\n",
    "        CALCULATED claim_count / CALCULATED member_count as frequency,\n",
    "        SUM(total_charge) / CALCULATED claim_count as severity\n",
    "    FROM claims\n",
    "    GROUP BY age_group, gender;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "In Databricks, the SQL is nearly identical but with added window functions for benchmarking!\n",
    "\n",
    "Let's build it! üëá\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fad91b5-1852-4727-a54f-ce6417ed1747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üí∞ ACTUARIAL ANALYSIS: Frequency & Severity by Age/Gender\n",
    "-- Core rating factor analysis for pricing and reserving\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.freq_sev_by_demographics AS\n",
    "WITH demographic_claims AS (\n",
    "    -- Step 1: Categorize members into age bands\n",
    "    SELECT \n",
    "        c.claim_id,\n",
    "        c.member_id,\n",
    "        c.total_charge,\n",
    "        c.claim_date,\n",
    "        m.gender,\n",
    "        m.birth_date,\n",
    "        YEAR(CURRENT_DATE()) - YEAR(m.birth_date) AS age,\n",
    "        CASE \n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 18 THEN '0-17'\n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 25 THEN '18-24'\n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 35 THEN '25-34'\n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 45 THEN '35-44'\n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 55 THEN '45-54'\n",
    "            WHEN YEAR(CURRENT_DATE()) - YEAR(m.birth_date) < 65 THEN '55-64'\n",
    "            ELSE '65+'\n",
    "        END AS age_band,\n",
    "        m.plan_id\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "),\n",
    "freq_sev_calcs AS (\n",
    "    -- Step 2: Calculate frequency and severity by demographic cell\n",
    "    SELECT \n",
    "        age_band,\n",
    "        gender,\n",
    "        COUNT(claim_id) AS claim_count,\n",
    "        COUNT(DISTINCT member_id) AS member_count,\n",
    "        SUM(total_charge) AS total_incurred,\n",
    "        \n",
    "        -- Key Actuarial Metrics\n",
    "        ROUND(COUNT(claim_id) * 1.0 / COUNT(DISTINCT member_id), 4) AS frequency,\n",
    "        ROUND(SUM(total_charge) / COUNT(claim_id), 2) AS severity,\n",
    "        ROUND((COUNT(claim_id) * 1.0 / COUNT(DISTINCT member_id)) * \n",
    "              (SUM(total_charge) / COUNT(claim_id)), 2) AS pure_premium,\n",
    "        \n",
    "        -- Percentiles for distribution analysis\n",
    "        ROUND(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_charge), 2) AS median_claim,\n",
    "        ROUND(PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY total_charge), 2) AS p90_claim\n",
    "        \n",
    "    FROM demographic_claims\n",
    "    GROUP BY age_band, gender\n",
    ")\n",
    "SELECT \n",
    "    *,\n",
    "    -- Relativities compared to overall average\n",
    "    ROUND(frequency / AVG(frequency) OVER (), 3) AS frequency_relativity,\n",
    "    ROUND(severity / AVG(severity) OVER (), 3) AS severity_relativity,\n",
    "    ROUND(pure_premium / AVG(pure_premium) OVER (), 3) AS pure_premium_relativity,\n",
    "    \n",
    "    -- Credibility indicator (simple)\n",
    "    CASE \n",
    "        WHEN claim_count >= 1000 THEN 'Full Credibility'\n",
    "        WHEN claim_count >= 500 THEN 'Partial Credibility'\n",
    "        WHEN claim_count >= 100 THEN 'Limited Credibility'\n",
    "        ELSE 'Low Credibility'\n",
    "    END AS credibility_indicator\n",
    "    \n",
    "FROM freq_sev_calcs\n",
    "ORDER BY age_band, gender;\n",
    "\n",
    "-- Display results\n",
    "SELECT * FROM payer_gold.freq_sev_by_demographics;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd1d040-bd7c-4699-9beb-65953396bb48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 6: Data Quality Checks for Actuarial Analysis\n",
    "\n",
    "**Objective**: Identify data quality issues that could impact your actuarial analysis.\n",
    "\n",
    "As actuaries, you know that **garbage in = garbage out**. Before any analysis, you must check your data quality!\n",
    "\n",
    "### üéØ Common Data Quality Issues in Healthcare:\n",
    "1. **Completeness**: Missing critical fields (claim amounts, dates, member IDs)\n",
    "2. **Accuracy**: Negative claim amounts, future dates, invalid codes\n",
    "3. **Consistency**: Duplicate claims, conflicting information\n",
    "4. **Timeliness**: Lag in claim reporting\n",
    "\n",
    "### üìù What to Check:\n",
    "\n",
    "**Part A: Completeness Check**\n",
    "```sql\n",
    "-- Example: Find records with missing critical data\n",
    "SELECT \n",
    "    'Missing claim_id' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE claim_id IS NULL\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Missing total_charge' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge IS NULL\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Missing member_id' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE member_id IS NULL;\n",
    "```\n",
    "\n",
    "**Part B: Accuracy Check**\n",
    "```sql\n",
    "-- Example: Find data accuracy issues\n",
    "SELECT \n",
    "    'Negative charges' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    SUM(total_charge) AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge < 0\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Future claim dates' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    NULL AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE claim_date > CURRENT_DATE()\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Zero dollar claims' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    0 AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge = 0;\n",
    "```\n",
    "\n",
    "**Part C: Duplicate Check**\n",
    "```sql\n",
    "-- Example: Find duplicate claims (same claim_id appearing multiple times)\n",
    "SELECT \n",
    "    claim_id,\n",
    "    COUNT(*) AS duplicate_count,\n",
    "    SUM(total_charge) AS total_duplicate_amount\n",
    "FROM payer_silver.claims\n",
    "GROUP BY claim_id\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY duplicate_count DESC;\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "### üí° Actuarial Insight:\n",
    "Data quality issues can significantly impact:\n",
    "- **Loss ratio calculations** (missing claims = underestimated losses)\n",
    "- **Trend analysis** (inconsistent reporting lags)\n",
    "- **IBNR estimates** (late-reported claims)\n",
    "- **Rate filings** (regulators scrutinize data quality!)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f834dfc8-db93-4262-9969-d7ff8d6c47ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Exercise: Data Quality Checks in Gold \n",
    "\n",
    "Here's a complete data quality report combining all checks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc950082-6eef-489a-ae46-ad8349ccb341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ‚úÖ Comprehensive Data Quality Report\n",
    "\n",
    "-- Part A: Completeness Checks\n",
    "SELECT 'COMPLETENESS' AS check_category, 'Missing claim_id' AS issue, COUNT(*) AS record_count\n",
    "FROM payer_silver.claims WHERE claim_id IS NULL\n",
    "UNION ALL\n",
    "SELECT 'COMPLETENESS', 'Missing total_charge', COUNT(*)\n",
    "FROM payer_silver.claims WHERE total_charge IS NULL\n",
    "UNION ALL\n",
    "SELECT 'COMPLETENESS', 'Missing member_id', COUNT(*)\n",
    "FROM payer_silver.claims WHERE member_id IS NULL\n",
    "UNION ALL\n",
    "SELECT 'COMPLETENESS', 'Missing claim_date', COUNT(*)\n",
    "FROM payer_silver.claims WHERE claim_date IS NULL\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Part B: Accuracy Checks\n",
    "SELECT 'ACCURACY', 'Negative charges', COUNT(*)\n",
    "FROM payer_silver.claims WHERE total_charge < 0\n",
    "UNION ALL\n",
    "SELECT 'ACCURACY', 'Future claim dates', COUNT(*)\n",
    "FROM payer_silver.claims WHERE claim_date > CURRENT_DATE()\n",
    "UNION ALL\n",
    "SELECT 'ACCURACY', 'Zero dollar claims', COUNT(*)\n",
    "FROM payer_silver.claims WHERE total_charge = 0\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Part C: Consistency Checks (Duplicates)\n",
    "SELECT 'CONSISTENCY', 'Duplicate claim_ids', COUNT(*) - COUNT(DISTINCT claim_id)\n",
    "FROM payer_silver.claims\n",
    "\n",
    "ORDER BY check_category, issue;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f8e26-28ba-48f6-9ec2-51d17257f2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ‚úÖ Data Quality Metrics with Percentages\n",
    "\n",
    "WITH total_records AS (\n",
    "    SELECT COUNT(*) AS total_count FROM payer_silver.claims\n",
    "),\n",
    "quality_issues AS (\n",
    "    SELECT \n",
    "        SUM(CASE WHEN claim_id IS NULL THEN 1 ELSE 0 END) AS missing_claim_id,\n",
    "        SUM(CASE WHEN total_charge IS NULL THEN 1 ELSE 0 END) AS missing_amount,\n",
    "        SUM(CASE WHEN total_charge < 0 THEN 1 ELSE 0 END) AS negative_amounts,\n",
    "        SUM(CASE WHEN claim_date > CURRENT_DATE() THEN 1 ELSE 0 END) AS future_dates,\n",
    "        SUM(CASE WHEN total_charge = 0 THEN 1 ELSE 0 END) AS zero_charges,\n",
    "        SUM(CASE WHEN provider_id IS NULL THEN 1 ELSE 0 END) AS missing_provider\n",
    "    FROM payer_silver.claims\n",
    ")\n",
    "SELECT \n",
    "    t.total_count,\n",
    "    q.missing_claim_id,\n",
    "    ROUND(q.missing_claim_id * 100.0 / t.total_count, 2) AS pct_missing_id,\n",
    "    q.missing_amount,\n",
    "    ROUND(q.missing_amount * 100.0 / t.total_count, 2) AS pct_missing_amount,\n",
    "    q.negative_amounts,\n",
    "    ROUND(q.negative_amounts * 100.0 / t.total_count, 2) AS pct_negative,\n",
    "    q.future_dates,\n",
    "    ROUND(q.future_dates * 100.0 / t.total_count, 2) AS pct_future_dates,\n",
    "    q.zero_charges,\n",
    "    ROUND(q.zero_charges * 100.0 / t.total_count, 2) AS pct_zero_charges,\n",
    "    q.missing_provider,\n",
    "    ROUND(q.missing_provider * 100.0 / t.total_count, 2) AS pct_missing_provider\n",
    "FROM total_records t, quality_issues q;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c67a76cd-d0b1-442d-900e-f24b0d25aec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actuarial Example 7: Bias Detection in Healthcare Data\n",
    "\n",
    "**Objective**: Detect potential biases in your data that could lead to unfair pricing or discriminatory practices.\n",
    "\n",
    "As actuaries, you have an **ethical and regulatory obligation** to ensure your analyses are fair and unbiased. This is especially critical in healthcare!\n",
    "\n",
    "### üéØ Types of Bias to Watch For:\n",
    "\n",
    "1. **Selection Bias**: Are certain populations underrepresented?\n",
    "2. **Geographic Bias**: Do certain regions have systematically different patterns?\n",
    "3. **Temporal Bias**: Has data collection changed over time?\n",
    "4. **Provider Bias**: Do certain providers have unusual claim patterns?\n",
    "5. **Demographic Bias**: Are outcomes correlated with protected characteristics?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 7A: Demographic Representation Bias\n",
    "\n",
    "**Check**: Are all demographic groups adequately represented?\n",
    "\n",
    "```sql\n",
    "-- Analyze member distribution by demographics\n",
    "SELECT \n",
    "    gender,\n",
    "    CASE \n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 18 THEN 'Under 18'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 35 THEN '18-34'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 50 THEN '35-49'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 65 THEN '50-64'\n",
    "        ELSE '65+'\n",
    "    END AS age_group,\n",
    "    COUNT(*) AS member_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS pct_of_total\n",
    "FROM payer_silver.members\n",
    "GROUP BY gender, age_group\n",
    "ORDER BY gender, age_group;\n",
    "```\n",
    "\n",
    "**What to Look For:**\n",
    "- Are any groups severely underrepresented (< 2% of population)?\n",
    "- Are certain age/gender combinations missing?\n",
    "- Could this skew your loss ratio analysis?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 7B: Geographic Bias Detection\n",
    "\n",
    "**Check**: Are certain states or cities systematically excluded or overrepresented?\n",
    "\n",
    "```sql\n",
    "-- Example: Analyze geographic distribution\n",
    "WITH state_stats AS (\n",
    "    SELECT \n",
    "        p.state,\n",
    "        COUNT(DISTINCT c.claim_id) AS claim_count,\n",
    "        COUNT(DISTINCT c.member_id) AS member_count,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost,\n",
    "        ROUND(SUM(c.total_charge), 2) AS total_incurred\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.state\n",
    "),\n",
    "overall_avg AS (\n",
    "    SELECT AVG(avg_claim_cost) AS overall_avg_cost\n",
    "    FROM state_stats\n",
    ")\n",
    "SELECT \n",
    "    s.state,\n",
    "    s.claim_count,\n",
    "    s.member_count,\n",
    "    s.avg_claim_cost,\n",
    "    o.overall_avg_cost,\n",
    "    -- Deviation from average\n",
    "    ROUND((s.avg_claim_cost - o.overall_avg_cost) / o.overall_avg_cost * 100, 2) AS pct_deviation_from_avg,\n",
    "    -- Flag for significant deviation\n",
    "    CASE \n",
    "        WHEN ABS((s.avg_claim_cost - o.overall_avg_cost) / o.overall_avg_cost) > 0.25 \n",
    "        THEN '‚ö†Ô∏è HIGH DEVIATION'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS bias_flag\n",
    "FROM state_stats s, overall_avg o\n",
    "ORDER BY ABS(s.avg_claim_cost - o.overall_avg_cost) DESC;\n",
    "```\n",
    "\n",
    "**Actuarial Questions:**\n",
    "- Are high-cost states being excluded from your analysis?\n",
    "- Could this bias your rate setting?\n",
    "- Should you stratify by geography?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 7C: Temporal Bias (Claims Reporting Lag)\n",
    "\n",
    "**Check**: Has claim reporting behavior changed over time?\n",
    "\n",
    "```sql\n",
    "-- Detect if recent months have unusual patterns (incomplete data?)\n",
    "WITH monthly_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        COUNT(*) AS claim_count,\n",
    "        ROUND(AVG(total_charge), 2) AS avg_claim_size,\n",
    "        COUNT(DISTINCT member_id) AS unique_members\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month\n",
    "),\n",
    "stats AS (\n",
    "    SELECT \n",
    "        AVG(claim_count) AS avg_monthly_claims,\n",
    "        STDDEV(claim_count) AS stddev_claims\n",
    "    FROM monthly_metrics\n",
    "    WHERE claim_month < DATE_TRUNC('MONTH', ADD_MONTHS(CURRENT_DATE(), -1))  -- Exclude current month\n",
    ")\n",
    "SELECT \n",
    "    m.claim_month,\n",
    "    m.claim_count,\n",
    "    m.avg_claim_size,\n",
    "    s.avg_monthly_claims,\n",
    "    -- Flag months with unusually low counts (potential incomplete data)\n",
    "    CASE \n",
    "        WHEN m.claim_count < (s.avg_monthly_claims - 2 * s.stddev_claims) \n",
    "        THEN '‚ö†Ô∏è UNUSUALLY LOW - POTENTIAL BIAS'\n",
    "        WHEN m.claim_count > (s.avg_monthly_claims + 2 * s.stddev_claims)\n",
    "        THEN '‚ö†Ô∏è UNUSUALLY HIGH - INVESTIGATE'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS completeness_flag\n",
    "FROM monthly_metrics m, stats s\n",
    "ORDER BY m.claim_month DESC\n",
    "LIMIT 12;  -- Last 12 months\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- Recent months may have incomplete data (IBNR!)\n",
    "- Including incomplete months will **understate** your loss ratios\n",
    "- Critical for reserving and trend analysis\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 7D: Provider Bias Detection\n",
    "\n",
    "**Check**: Are certain providers outliers? Could this indicate fraud, coding issues, or data errors?\n",
    "\n",
    "```sql\n",
    "-- Example: Identify provider outliers\n",
    "WITH provider_metrics AS (\n",
    "    SELECT \n",
    "        p.provider_id,\n",
    "        p.provider_name,\n",
    "        p.specialty,\n",
    "        COUNT(c.claim_id) AS claim_count,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost,\n",
    "        ROUND(SUM(c.total_charge), 2) AS total_billed\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.provider_id, p.provider_name, p.specialty\n",
    "    HAVING COUNT(c.claim_id) >= 10  -- Minimum credibility\n",
    "),\n",
    "specialty_benchmarks AS (\n",
    "    SELECT \n",
    "        specialty,\n",
    "        AVG(avg_claim_cost) AS specialty_avg,\n",
    "        STDDEV(avg_claim_cost) AS specialty_stddev\n",
    "    FROM provider_metrics\n",
    "    GROUP BY specialty\n",
    ")\n",
    "SELECT \n",
    "    pm.provider_id,\n",
    "    pm.provider_name,\n",
    "    pm.specialty,\n",
    "    pm.claim_count,\n",
    "    pm.avg_claim_cost,\n",
    "    sb.specialty_avg,\n",
    "    -- Calculate Z-score (standard deviations from mean)\n",
    "    ROUND((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0), 2) AS z_score,\n",
    "    -- Flag outliers\n",
    "    CASE \n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) > 3 \n",
    "        THEN 'üö® EXTREME HIGH - INVESTIGATE'\n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) > 2 \n",
    "        THEN '‚ö†Ô∏è HIGH OUTLIER'\n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) < -2 \n",
    "        THEN '‚ö†Ô∏è LOW OUTLIER'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS outlier_flag\n",
    "FROM provider_metrics pm\n",
    "INNER JOIN specialty_benchmarks sb ON pm.specialty = sb.specialty\n",
    "WHERE ABS((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0)) > 2  -- Only show outliers\n",
    "ORDER BY ABS((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0)) DESC;\n",
    "```\n",
    "\n",
    "**Actuarial Actions:**\n",
    "- Investigate extreme outliers (fraud? coding errors?)\n",
    "- Consider excluding outliers from benchmarks\n",
    "- Document your methodology for rate filings\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Actuarial Ethics & Compliance:\n",
    "\n",
    "**Why This Matters:**\n",
    "1. **Regulatory Compliance**: ACA prohibits discrimination based on protected characteristics\n",
    "2. **Actuarial Standards of Practice (ASOP)**: Require documentation of data quality and potential biases\n",
    "3. **Rate Filing Requirements**: Regulators will question biased or incomplete data\n",
    "4. **Professional Ethics**: Actuaries must ensure fairness in pricing and risk selection\n",
    "\n",
    "**Best Practices:**\n",
    "- ‚úÖ Document all data limitations and potential biases\n",
    "- ‚úÖ Stratify analysis by key demographics to detect disparities\n",
    "- ‚úÖ Use credibility weighting for small segments\n",
    "- ‚úÖ Clearly communicate assumptions and limitations to stakeholders\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ee3f51-2d4f-4127-94e9-54d034af7144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa71c33b-47d1-4b46-9940-2fdfd3cfb439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dae305e5-7f23-43f9-83a3-6ebe62a4d4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary - You Did It!\n",
    "\n",
    "## üéâ Congratulations, Actuaries!\n",
    "\n",
    "You've just completed your first Databricks workshop! Let's review what you learned.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished Today\n",
    "\n",
    "### 1. **Transitioned from SAS to Databricks** üîÑ\n",
    "- Learned how your SAS knowledge transfers\n",
    "- Ran SQL queries (very similar to PROC SQL!)\n",
    "- Used window functions (LAG, LEAD for trending)\n",
    "\n",
    "### 2. **Built Actuarial Analytics** üìä\n",
    "- ‚úÖ **Loss Ratios** by segment\n",
    "- ‚úÖ **Claims Trending** (month-over-month growth)\n",
    "- ‚úÖ **Development Patterns** (claims emergence)\n",
    "- ‚úÖ **Risk Segmentation** (high-cost members)\n",
    "- ‚úÖ **Provider Analysis** (cost drivers)\n",
    "\n",
    "### 3. **Learned Key SQL Techniques** üíª\n",
    "- `GROUP BY` for aggregations (like PROC MEANS)\n",
    "- `JOIN` for combining tables\n",
    "- `CASE WHEN` for conditional logic (like IF-THEN)\n",
    "- `LAG/LEAD` for trending (like SAS LAG functions)\n",
    "- `PERCENTILE_CONT` for quantiles (like PROC UNIVARIATE)\n",
    "- `WINDOW FUNCTIONS` for running calculations\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How to Use This at Work\n",
    "\n",
    "### Immediate Applications:\n",
    "\n",
    "1. **Quarterly Loss Ratio Reports**\n",
    "   - Use the loss ratio queries we built\n",
    "   - Group by state, specialty, plan type\n",
    "   - Export to dashboards (no more Excel!)\n",
    "\n",
    "2. **Monthly Trending Analysis**\n",
    "   - Monitor claims frequency and severity trends\n",
    "   - Identify unusual spikes early\n",
    "   - Feed into your pricing models\n",
    "\n",
    "3. **Reserving Support**\n",
    "   - Build development triangles\n",
    "   - Calculate age-to-age factors\n",
    "   - Track IBNR emergence patterns\n",
    "\n",
    "4. **Risk Management**\n",
    "   - Identify high-risk members\n",
    "   - Segment populations for care management\n",
    "   - Calculate risk scores\n",
    "\n",
    "5. **Rate Filings**\n",
    "   - Trend historical claims\n",
    "   - Support rate change justifications\n",
    "   - Build exhibits for regulators\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "### 1. **You Already Know More Than You Think!**\n",
    "If you know SAS PROC SQL, you know 90% of Databricks SQL. The syntax is almost identical!\n",
    "\n",
    "### 2. **Start Simple**\n",
    "Don't try to learn everything at once. Start with:\n",
    "- Basic `SELECT` queries\n",
    "- Simple aggregations (`GROUP BY`)\n",
    "- Joins\n",
    "\n",
    "Then gradually add:\n",
    "- Window functions\n",
    "- CTEs (WITH clauses)\n",
    "- Advanced analytics\n",
    "\n",
    "### 3. **SQL is Enough for Most Actuarial Work**\n",
    "You don't need to learn Python/PySpark right away. Most actuarial analyses can be done with SQL alone!\n",
    "\n",
    "### 4. **Iterate and Improve**\n",
    "Your first queries won't be perfect. That's okay! \n",
    "- Start with something that works\n",
    "- Refine it over time\n",
    "- Ask for feedback\n",
    "\n",
    "### 5. **Refer to the Best Practices!**\n",
    "Best practices notebook: _**[Reference] Best Practices**_\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8183174594201588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DBX Workshop_Actuaries_11072025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "my_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
