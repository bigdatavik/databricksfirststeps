{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks for Actuaries: Healthcare Analytics Workshop\n",
    "**A Hands-On Guide for SAS Users Transitioning to Databricks**\n",
    "\n",
    "---\n",
    "\n",
    "## üëã Welcome Actuaries!\n",
    "\n",
    "This workshop is designed specifically for **actuaries** who are familiar with **SAS** and want to learn Databricks. Don't worry if you're new to Python or SQL - we'll guide you step by step!\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand how Databricks compares to your SAS workflows\n",
    "2. ‚úÖ Load and query healthcare payer data using **simple SQL**\n",
    "3. ‚úÖ Perform actuarial analyses you're familiar with (loss ratios, trends, reserving)\n",
    "4. ‚úÖ Create interactive visualizations without complex code\n",
    "5. ‚úÖ Build analytics tables for pricing, reserving, and risk management\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What Makes This Workshop Different?\n",
    "\n",
    "- **Beginner-Friendly**: No prior PySpark/Python experience needed\n",
    "- **Actuarial-Focused**: Examples from pricing, reserving, and risk management\n",
    "- **SAS Comparisons**: See how familiar SAS code translates to Databricks\n",
    "- **Interactive Exercises**: Learn by doing with real actuarial problems\n",
    "- **Hands-On**: Less theory, more practice!\n",
    "\n",
    "---\n",
    "\n",
    "### üè• Dataset Overview\n",
    "\n",
    "We'll work with **healthcare payer data** including:\n",
    "- **Members**: Health plan enrollees (like your policy data)\n",
    "- **Claims**: Medical claim submissions (incurred losses)\n",
    "- **Providers**: Healthcare providers (similar to provider networks)\n",
    "- **Diagnoses**: Diagnosis codes from claims\n",
    "- **Procedures**: Medical procedures performed\n",
    "\n",
    "**Think of it as**: Claims = Losses, Members = Policies, Providers = Service Providers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b67ad5a-ac8b-4034-8885-cf6ed972bdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìë Table of Contents\n",
    "\n",
    "### Part 1: Getting Started (Quick Setup)\n",
    "1. **[Introduction for Actuaries](#introduction)**\n",
    "   - Databricks vs SAS: What's Different?\n",
    "   - Quick Tour of the Platform\n",
    "   - Your First Query\n",
    "\n",
    "2. **[Setup (We'll Do This Together)](#setup)**\n",
    "   - Loading Your Data (Bronze & Silver - Simplified)\n",
    "   - Checking Your Tables\n",
    "\n",
    "### Part 2: Actuarial Analytics (The Fun Part! üéâ)\n",
    "\n",
    "3. **[Gold Layer - Actuarial Analytics](#gold-layer)**\n",
    "   - **Loss Ratio Analysis** ‚≠ê\n",
    "   - **Claims Development & Trending** ‚≠ê\n",
    "   - **Frequency & Severity Metrics** ‚≠ê\n",
    "   - **Risk Segmentation & Scoring** ‚≠ê\n",
    "   - **IBNR Indicators** ‚≠ê\n",
    "\n",
    "4. **[Interactive Exercises](#exercises)**\n",
    "   - üéØ Exercise 1: Calculate Loss Ratios by Segment\n",
    "   - üéØ Exercise 2: Trend Analysis (Like PROC EXPAND!)\n",
    "   - üéØ Exercise 3: Age-to-Age Development Factors\n",
    "   - üéØ Exercise 4: Premium Adequacy Analysis\n",
    "   - üéØ Exercise 5: Risk Score Modeling\n",
    "   - üéØ Exercise 6: Claims Development Triangle\n",
    "\n",
    "5. **[Visualizations for Actuaries](#visualizations)**\n",
    "   - Loss Ratio Charts\n",
    "   - Development Triangles\n",
    "   - Trend Lines\n",
    "   - Distribution Analysis\n",
    "\n",
    "6. **[SAS to Databricks Quick Reference](#sas-reference)**\n",
    "   - Common SAS Procedures ‚Üí SQL/PySpark\n",
    "   - PROC SQL ‚Üí Databricks SQL\n",
    "   - PROC MEANS ‚Üí Aggregations\n",
    "   - PROC FREQ ‚Üí GROUP BY\n",
    "   - Data Steps ‚Üí Transformations\n",
    "\n",
    "7. **[Next Steps](#next-steps)**\n",
    "   - Taking This Back to Your Work\n",
    "   - Resources for Actuaries\n",
    "   - Getting Help\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Workshop Flow**: We'll move quickly through setup, then spend most of our time on actuarial analytics. Feel free to ask questions anytime!\n",
    "    - Continue Learning\n",
    "    - Resources & Certifications\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Workshop Tip**: This is a hands-on workshop! Execute each cell as you go and experiment with the code. Don't hesitate to ask questions or use the Databricks AI Assistant.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe39bed2-7a8e-4992-8c17-3df323aec010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction for Actuaries\n",
    "\n",
    "## ü§î Why Databricks for Actuaries?\n",
    "\n",
    "If you're coming from **SAS**, you might be wondering: \"Why learn another tool?\"\n",
    "\n",
    "### Here's Why:\n",
    "- **Scalability**: Handle millions of claims instantly (no more waiting for PROC SQL!)\n",
    "- **Modern Analytics**: Built-in ML, real-time dashboards, and collaboration\n",
    "- **Cost-Effective**: Cloud-based, pay only for what you use\n",
    "- **Still Use SQL**: 90% of your SAS PROC SQL knowledge transfers directly!\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ SAS vs Databricks: Quick Comparison\n",
    "\n",
    "| **What You Do in SAS** | **How You Do It in Databricks** | **Difficulty** |\n",
    "|------------------------|----------------------------------|----------------|\n",
    "| `PROC SQL` | SQL queries (almost identical!) | ‚≠ê Easy |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregate functions | ‚≠ê Easy |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | ‚≠ê Easy |\n",
    "| `DATA` step | SQL `SELECT` or simple Python | ‚≠ê‚≠ê Moderate |\n",
    "| `PROC EXPAND` (trending) | Window functions | ‚≠ê‚≠ê Moderate |\n",
    "| Macros | Parameters + reusable queries | ‚≠ê‚≠ê‚≠ê Learning curve |\n",
    "\n",
    "**Good News**: Most of what you do can be done with **SQL alone**!\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Quick Concept: Medallion Architecture (Simplified)\n",
    "\n",
    "Think of it like your SAS workflow:\n",
    "\n",
    "```\n",
    "üì• BRONZE (Raw Data)        ‚Üí  Like your input datasets from source systems\n",
    "   ‚Üì\n",
    "üîß SILVER (Clean Data)      ‚Üí  Like your cleaned/standardized datasets  \n",
    "   ‚Üì\n",
    "‚≠ê GOLD (Analytics Tables)   ‚Üí  Like your final reporting/analysis datasets\n",
    "```\n",
    "\n",
    "**Today's Focus**: We'll quickly load Bronze/Silver, then spend most time on **Gold** (actuarial analytics)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üè† What is a Lakehouse? (Simple Explanation)\n",
    "\n",
    "**For Actuaries**: Think of it as a **super-powered SAS library** that:\n",
    "- Stores all your data in one place (claims, policies, members)\n",
    "- Lets you analyze it with SQL (like PROC SQL)\n",
    "- Handles millions of rows instantly\n",
    "- Keeps track of all changes (audit trail)\n",
    "- Lets multiple people work at once (no locking issues!)\n",
    "\n",
    "**Key Benefit**: Unlike SAS datasets, you can query **billions** of claims in seconds!\n",
    "\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png\" alt=\"Lakehouse\" width=\"500\" height=\"350\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Unity Catalog (Data Organization - Like SAS Libraries)\n",
    "\n",
    "**For Actuaries**: Think of Unity Catalog as your **SAS library structure**, but better organized:\n",
    "\n",
    "```\n",
    "In SAS:                    In Databricks:\n",
    "LIBNAME.DATASET            CATALOG.SCHEMA.TABLE\n",
    "  ‚Üì                           ‚Üì\n",
    "work.claims        ‚Üí       my_catalog.payer_bronze.claims\n",
    "actuarial.loss_ratios ‚Üí    my_catalog.payer_gold.loss_ratios\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Everyone sees the same data (no duplicate datasets!)\n",
    "- ‚úÖ Built-in security (control who can see PHI/PII)\n",
    "- ‚úÖ Audit trail (track all data access)\n",
    "- ‚úÖ Easy to find data (searchable catalog)\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086\" alt=\"Unity Catalog\" width=\"500\" height=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ü•âü•àü•á Medallion Architecture (Your Data Quality Layers)\n",
    "\n",
    "**For Actuaries**: This is like your SAS data prep workflow, but organized into layers:\n",
    "\n",
    "### üì• Bronze (Raw Data) \n",
    "- **Like**: Your raw claims extracts from source systems\n",
    "- **Contains**: Data exactly as received (CSV, database extracts)\n",
    "- **Example**: Raw claims file from claims system\n",
    "- **Today**: We'll load this quickly!\n",
    "\n",
    "### üîß Silver (Cleaned Data)\n",
    "- **Like**: Your cleaned/standardized SAS datasets\n",
    "- **Contains**: Deduplicated, standardized data\n",
    "- **Example**: Claims with proper data types, duplicates removed\n",
    "- **Today**: We'll auto-clean this!\n",
    "\n",
    "### ‚≠ê Gold (Analytics Tables)\n",
    "- **Like**: Your final analysis datasets (loss triangles, premium summaries)\n",
    "- **Contains**: Business-ready tables for actuarial analysis\n",
    "- **Example**: Loss ratios, development factors, IBNR estimates\n",
    "- **Today**: This is where we'll spend most time! üéâ\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Medallion Architecture\" width=\"500\" height=\"350\">\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° For This Workshop**: We'll speed through Bronze/Silver (15 min) so we can focus on Gold analytics (1.5 hours)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8292153-2b0a-47dd-b6c6-b1cf4c45cdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managed tables\n",
    "\n",
    "[How Unity Catalog Managed Tables Automate Performance at Scale](https://www.databricks.com/blog/how-unity-catalog-managed-tables-automate-performance-scale) with [Predictive Optimization](https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization)\n",
    "\n",
    "\n",
    "<!-- ![](https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384) -->\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9115bf6-333a-4cb8-965c-5e73a2d204d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Faster Queries: 20X query latency reduction](https://www.databricks.com/blog/predictive-optimization-automatically-delivers-faster-queries-and-lower-tco)\n",
    "\n",
    "**Predictive Optimization** in Databricks automates table management by leveraging Unity Catalog and the Data Intelligence Platform. This innovative feature currently runs the following optimizations for Unity Catalog managed tables:\n",
    "\n",
    "* **OPTIMIZE** - Triggers incremental clustering for enabled tables. Improves query performance by optimizing file sizes.\n",
    "* **VACUUM** - Reduces storage costs by deleting data files no longer referenced by the table.\n",
    "* **ANALYZE** - Triggers incremental update of statistics to improve query performance. \n",
    "\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/styles/max_1000x1000/public/2024-05/db-976-blog-img-og.png?itok=qWBT8VA-&v=1717158571\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n",
    "\n",
    "**Compaction** - This enhances query performance by optimizing file sizes, ensuring that data retrieval is efficient.\n",
    "\n",
    "**Liquid Clustering** - This technique incrementally clusters incoming data, enabling optimal data layout and efficient data skipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce4ebf7-dc51-44bc-8bf2-804fec60b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Medallion Pipeline for a Healthcare Payer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Modeling Concepts\n",
    "\n",
    "Databricks fully supports both **dimensional modeling** (Kimball/star schema) and **Inmon-style, Data Vault architectures (hubs, satellites, links)** on the Lakehouse platform. For dimensional models, you can build classic star and snowflake schemas directly with SQL, benefiting from ACID transactions and scalable Delta Lake tables.\n",
    "\n",
    "For Inmon/Data Vault use cases, Databricks provides rich support for hub-and-satellite models that address core enterprise needs for history, auditability, and extensibility‚Äîfind end.\n",
    "\n",
    "The Lakehouse approach lets you mix these styles as needed within a single platform, so you can incrementally land data in Raw Vault/EDW structures and later expose it as dimensional marts‚Äîall with Delta Live Tables, fine-grained security, and open formats.\n",
    "\n",
    "Key blog resources:\n",
    "\n",
    "[Implementing Dimensional Modeling](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "[Implementing Data Vault/Hub-Satellite](https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html) \n",
    "\n",
    "[Data Vault Best Practices](https://www.databricks.com/blog/data-vault-best-practice-implementation-lakehouse)\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043442-a66b-47f2-9f2c-c6890d2de349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data Model\n",
    "\n",
    "For a payer, commonly used tables include:\n",
    "\n",
    "- **Members**: members enrolled in a health plan\n",
    "- **Claims**: medical claim submissions\n",
    "- **Providers**: healthcare providers (doctors, clinics)\n",
    "- **Diagnoses**: claim diagnosis codes\n",
    "- **Procedures**: procedures/services performed\n",
    "\n",
    "Each table should have at least 50 rows.\n",
    "\n",
    "<img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/bdd54dc0-f3c7-4975-80a3-0017ebdb121c.png\" alt=\"Managed Tables\" width=\"400\" height=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bd63ca-5638-422f-a991-0d6c3d37a2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table\tKey Columns\n",
    "\n",
    "**Members**\tmember_id, first_name, last_name, birth_date, gender, plan_id, effective_date\n",
    "\n",
    "**Claims**\tclaim_id, member_id, provider_id, claim_date, total_charge, claim_status\n",
    "\n",
    "**Providers**\tprovider_id, npi, provider_name, specialty, address, city, state\n",
    "\n",
    "**Diagnoses**\tclaim_id, diagnosis_code, diagnosis_desc\n",
    "\n",
    "**Procedures**\tclaim_id, procedure_code, procedure_desc, amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"my_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3beab112-8351-412b-89dd-b7d12bed854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1346d2f0-da56-4664-9549-ca712a979960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc54c890-6b43-4031-ae41-39e8aff39654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create new **Volumes** as below and upload shared files to your volumes.\n",
    "\n",
    "(schema) payer_bronze \\\n",
    "|--- payer/files/ \\\n",
    "|------ claims \\\n",
    "|------ members \\\n",
    "|------ providers \\\n",
    "|------ diagnoses \\\n",
    "|------ procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee52692-6c62-4c15-b115-e05aca2d260f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3974a340-43ce-4065-bf06-4d899386100b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f529a21-c047-4a8c-81f1-4d6c452dee52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8747d067-1569-4679-a336-41dcc5479627",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Catalog and Create Bronze Schema"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Set the catalog and schema\n",
    "# CREATE CATALOG IF NOT EXISTS my_catalog;\n",
    "# USE CATALOG my_catalog;\n",
    "\n",
    "# -- Create bronze schema\n",
    "# CREATE SCHEMA IF NOT EXISTS payer_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze Layer ‚Äì Ingest Raw Data\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Bronze Layer?\n",
    "\n",
    "The **Bronze Layer** is the landing zone for raw data. Here we:\n",
    "- üìÇ Load data \"as-is\" from source files (CSV, JSON, Parquet, etc.)\n",
    "- üíæ Store in Delta Lake format for ACID transactions\n",
    "- üìù Apply minimal transformation (just schema inference)\n",
    "- ‚è±Ô∏è Keep historical data for audit and reprocessing\n",
    "\n",
    "> **üí° Best Practice**: Use `COPY INTO` for incremental, idempotent loading. It automatically skips already-loaded files!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/my_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Claims Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c87df9-f460-4e15-bd6d-c9e3da090f48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Diagnosis Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863a1bfd-e455-4e93-bdd1-a66c1ede59af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Members Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398146a1-96d1-45a2-be1d-ff80834d7e37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Procedures Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c10dc9-2a3f-4f1f-a9ef-113259d7bae7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Providers Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/my_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9506c098-f429-4193-91a8-bd5696c4ae03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Exercise: Query Bronze Tables\n",
    "\n",
    "Now that we've loaded data into Bronze tables, let's explore what we have:\n",
    "\n",
    "**Try these queries yourself:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b757d-bb2d-476e-a8cc-b880ef7a965c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query: Count records in each bronze table using PySpark\n",
    "from pyspark.sql.functions import lit, count\n",
    "\n",
    "tables = ['claims_raw', 'members_raw', 'providers_raw', 'diagnosis_raw', 'procedures_raw']\n",
    "row_counts = []\n",
    "\n",
    "for table in tables:\n",
    "    cnt = spark.table(f\"payer_bronze.{table}\").count()\n",
    "    row_counts.append((table, cnt))\n",
    "    \n",
    "# Create DataFrame to display results\n",
    "result_df = spark.createDataFrame(row_counts, [\"table_name\", \"row_count\"])\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîß Silver Layer ‚Äì Transform, Clean, and Join\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Silver Layer?\n",
    "\n",
    "The **Silver Layer** is where we transform raw data into clean, validated, and enriched datasets. Here we:\n",
    "\n",
    "- üßπ **Clean**: Remove nulls, trim whitespace, fix data quality issues\n",
    "- üîÑ **Transform**: Cast data types, standardize formats\n",
    "- üóëÔ∏è **Deduplicate**: Remove duplicate records based on business keys\n",
    "- üîç **Validate**: Apply business rules and data quality checks\n",
    "- üìä **Enrich**: Join related tables, calculate derived columns\n",
    "\n",
    "> **üí° Best Practice**: Silver tables should be \"analytics-ready\" ‚Äì cleaned, validated, and properly typed!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cb09e1-fe4c-4c5b-bcba-fbff4eb501fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Exercise: Data Quality Checks\n",
    "\n",
    "Let's verify our Silver transformations worked correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e90cd4-2b05-41cd-9fe9-3de6a242c6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Data Quality Check 1: Check for nulls in key columns\n",
    "print(\"=== NULL CHECK ===\")\n",
    "members_df = spark.table(\"payer_silver.members\")\n",
    "null_counts = members_df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in members_df.columns\n",
    "])\n",
    "display(null_counts)\n",
    "\n",
    "# Data Quality Check 2: Check for duplicates\n",
    "print(\"\\n=== DUPLICATE CHECK ===\")\n",
    "claims_df = spark.table(\"payer_silver.claims\")\n",
    "total_rows = claims_df.count()\n",
    "distinct_rows = claims_df.select(\"claim_id\").distinct().count()\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct claim_ids: {distinct_rows}\")\n",
    "print(f\"Duplicates: {total_rows - distinct_rows}\")\n",
    "\n",
    "# Data Quality Check 3: Value range checks\n",
    "print(\"\\n=== VALUE RANGE CHECK ===\")\n",
    "claims_stats = claims_df.agg(\n",
    "    {\"total_charge\": \"min\", \"total_charge\": \"max\", \"total_charge\": \"avg\"}\n",
    ")\n",
    "display(claims_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì Actuarial Analytics (The Fun Part!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is the Gold Layer? (For Actuaries)\n",
    "\n",
    "**This is where YOU spend most of your time!** The Gold Layer is like your **final SAS analysis datasets** - ready for actuarial work.\n",
    "\n",
    "### What We'll Build (Actuarial Examples):\n",
    "\n",
    "1. **üìä Loss Ratios** - By state, specialty, plan (like your quarterly reports)\n",
    "2. **üìà Claims Trends** - Frequency & severity trending (PROC EXPAND style)\n",
    "3. **üî∫ Development Triangles** - Age-to-age factors (for reserving)\n",
    "4. **üí∞ Premium Adequacy** - Earned vs Incurred ratios\n",
    "5. **üé≤ Risk Scoring** - Member risk segmentation\n",
    "6. **üìâ IBNR Indicators** - Late claims reporting patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How This Compares to SAS\n",
    "\n",
    "| **Your SAS Workflow** | **In Databricks Gold Layer** |\n",
    "|----------------------|------------------------------|\n",
    "| Create final analysis dataset | Create Gold table |\n",
    "| PROC SQL with aggregations | SQL SELECT with GROUP BY |\n",
    "| PROC MEANS for summary stats | Aggregate functions (AVG, SUM, etc.) |\n",
    "| Multiple DATA steps for calcs | Single SQL statement with CTEs |\n",
    "| Macros for repeated calcs | Parameterized queries |\n",
    "| Export to Excel for viz | Built-in interactive charts! |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Your Actuarial Toolbox\n",
    "\n",
    "Today you'll learn SQL equivalents for common actuarial analyses:\n",
    "\n",
    "- **Loss Ratios**: `SUM(claims)/SUM(premium)`\n",
    "- **Trending**: Window functions (`LAG`, `LEAD`)\n",
    "- **Development Factors**: `GROUP BY` claim year + development period\n",
    "- **Percentiles**: `PERCENTILE_CONT` function\n",
    "- **Risk Scores**: `CASE WHEN` logic\n",
    "\n",
    "**Ready?** Let's start building! üöÄ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Actuarial Example 1: Loss Ratios by Segment\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What is our loss ratio by provider specialty and state?\"**\n",
    "\n",
    "This is a **fundamental actuarial metric** - you probably calculate this quarterly or annually!\n",
    "\n",
    "### üìù SAS vs Databricks\n",
    "\n",
    "**In SAS, you might write:**\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE loss_ratios AS\n",
    "    SELECT \n",
    "        specialty,\n",
    "        state,\n",
    "        COUNT(*) as claim_count,\n",
    "        SUM(total_charge) as incurred_losses,\n",
    "        CALCULATED incurred_losses / (SELECT SUM(premium) FROM policies) as loss_ratio\n",
    "    FROM claims c\n",
    "    LEFT JOIN providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY specialty, state;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**In Databricks, we write:**\n",
    "```sql\n",
    "-- Very similar! Most SQL transfers directly.\n",
    "```\n",
    "\n",
    "Let's build this now! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üìä ACTUARIAL ANALYSIS: Loss Ratios by Specialty and State\n",
    "-- This calculates incurred claims / exposure (using claim count as proxy for premium)\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.loss_ratios_by_segment AS\n",
    "SELECT \n",
    "    p.specialty,\n",
    "    p.state,\n",
    "    COUNT(DISTINCT c.claim_id) AS claim_count,\n",
    "    COUNT(DISTINCT c.member_id) AS member_count,\n",
    "    SUM(c.total_charge) AS total_incurred,\n",
    "    ROUND(AVG(c.total_charge), 2) AS avg_claim_size,\n",
    "    \n",
    "    -- Loss Ratio Calculation (using member count as premium proxy)\n",
    "    -- In real life, you'd join to a premium table!\n",
    "    ROUND(SUM(c.total_charge) / COUNT(DISTINCT c.member_id), 2) AS loss_ratio_per_member,\n",
    "    \n",
    "    -- Claim Frequency (claims per member)\n",
    "    ROUND(COUNT(c.claim_id) * 1.0 / COUNT(DISTINCT c.member_id), 2) AS frequency,\n",
    "    \n",
    "    -- Severity (avg cost per claim)\n",
    "    ROUND(SUM(c.total_charge) / COUNT(c.claim_id), 2) AS severity\n",
    "    \n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "GROUP BY p.specialty, p.state\n",
    "HAVING COUNT(c.claim_id) >= 5  -- Filter out small segments\n",
    "ORDER BY total_incurred DESC;\n",
    "\n",
    "-- Display results\n",
    "SELECT * FROM payer_gold.loss_ratios_by_segment LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ YOUR TURN! Exercise 1: Specialty Analysis\n",
    "\n",
    "**Task**: Modify the query above to answer: **\"Which specialty has the highest severity (cost per claim)?\"**\n",
    "\n",
    "**Hints:**\n",
    "1. You already have the `severity` column!\n",
    "2. Just change the `ORDER BY` clause\n",
    "3. Maybe add `LIMIT 10` to see top specialties\n",
    "\n",
    "**Try it below:** (Click the cell and modify the SQL)\n",
    "\n",
    "```sql\n",
    "-- YOUR CODE HERE\n",
    "-- Hint: Copy the query above and modify the ORDER BY\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output**: You should see specialties ranked by average claim cost.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Actuarial Example 2: Claims Trending Analysis\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What are our monthly claim trends? Are claims trending up or down?\"**\n",
    "\n",
    "This is crucial for:\n",
    "- **Rate making** (applying trend factors)\n",
    "- **Reserving** (projecting ultimate losses)\n",
    "- **Budgeting** (forecasting next year's costs)\n",
    "\n",
    "### üìù What We're Calculating\n",
    "\n",
    "```\n",
    "Month-over-Month Growth = (This Month - Last Month) / Last Month\n",
    "Year-over-Year Growth = (This Month - Same Month Last Year) / Same Month Last Year\n",
    "```\n",
    "\n",
    "### üîß SAS Equivalent\n",
    "In SAS, you might use **PROC EXPAND** or **LAG functions** in a DATA step. \n",
    "\n",
    "In Databricks, we use **Window Functions** - specifically `LAG()` and `LEAD()`.\n",
    "\n",
    "Let's build it! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üìà ACTUARIAL ANALYSIS: Monthly Claims Trending\n",
    "-- Window functions for MoM and YoY calculations\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_trend_analysis AS\n",
    "WITH monthly_claims AS (\n",
    "    -- Step 1: Aggregate claims by month\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        YEAR(claim_date) AS claim_year,\n",
    "        MONTH(claim_date) AS claim_month_num,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(total_charge) AS total_incurred,\n",
    "        ROUND(AVG(total_charge), 2) AS avg_claim_cost\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month, claim_year, claim_month_num\n",
    ")\n",
    "SELECT \n",
    "    claim_month,\n",
    "    claim_count,\n",
    "    total_incurred,\n",
    "    avg_claim_cost,\n",
    "    \n",
    "    -- Month-over-Month Comparison\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY claim_month) AS prior_month_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY claim_month) * 100, 2) AS mom_growth_pct,\n",
    "    \n",
    "    -- Year-over-Year Comparison (12 months ago)\n",
    "    LAG(total_incurred, 12) OVER (ORDER BY claim_month) AS prior_year_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 12) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 12) OVER (ORDER BY claim_month) * 100, 2) AS yoy_growth_pct,\n",
    "    \n",
    "    -- 3-Month Moving Average (for smoothing)\n",
    "    ROUND(AVG(total_incurred) OVER (ORDER BY claim_month \n",
    "                                     ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), 2) AS moving_avg_3mo\n",
    "    \n",
    "FROM monthly_claims\n",
    "ORDER BY claim_month;\n",
    "\n",
    "-- Display the results\n",
    "SELECT * FROM payer_gold.claims_trend_analysis;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî∫ Actuarial Example 3: Claims Development Triangle\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"How do claims develop over time? What are our age-to-age factors?\"**\n",
    "\n",
    "This is **THE fundamental tool** for actuarial reserving!\n",
    "\n",
    "### üìù What We're Building\n",
    "\n",
    "A **development triangle** shows:\n",
    "- **Rows**: Accident/Policy Year\n",
    "- **Columns**: Development Period (months since occurrence)\n",
    "- **Values**: Cumulative claims at each development point\n",
    "\n",
    "Then we calculate:\n",
    "- **Age-to-Age Factors** (e.g., 12-to-24 month factor)\n",
    "- **Ultimate Loss Projections**\n",
    "\n",
    "### üîß Why This Matters\n",
    "- **IBNR Reserves**: Estimate unreported claims\n",
    "- **Case Reserve Adequacy**: Check if reserves are sufficient\n",
    "- **Rate Adequacy**: Are our prices sufficient?\n",
    "\n",
    "Let's build a simple development view! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üî∫ ACTUARIAL ANALYSIS: Claims Development Pattern\n",
    "-- Shows how claims emerge over time (by months since occurrence)\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_development AS\n",
    "WITH claim_development AS (\n",
    "    SELECT \n",
    "        c.claim_id,\n",
    "        DATE_TRUNC('YEAR', c.claim_date) AS accident_year,\n",
    "        c.claim_date,\n",
    "        c.total_charge,\n",
    "        m.effective_date AS member_effective_date,\n",
    "        \n",
    "        -- Development period in months (months between member effective date and claim date)\n",
    "        -- This simulates months since policy inception\n",
    "        DATEDIFF(MONTH, m.effective_date, c.claim_date) AS development_months,\n",
    "        \n",
    "        -- Group into development periods (quarterly for simplicity)\n",
    "        CASE \n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 3 THEN '0-3 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 6 THEN '4-6 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 12 THEN '7-12 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 24 THEN '13-24 months'\n",
    "            ELSE '24+ months'\n",
    "        END AS development_period\n",
    "        \n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "    WHERE c.claim_date >= m.effective_date  -- Claims after member enrollment\n",
    ")\n",
    "SELECT \n",
    "    accident_year,\n",
    "    development_period,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS cumulative_incurred,\n",
    "    ROUND(AVG(total_charge), 2) AS avg_claim_size,\n",
    "    \n",
    "    -- Calculate % of claims reported in each period\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY accident_year), 2) AS pct_of_total_claims\n",
    "    \n",
    "FROM claim_development\n",
    "GROUP BY accident_year, development_period\n",
    "ORDER BY accident_year, development_period;\n",
    "\n",
    "-- Display the triangle\n",
    "SELECT * FROM payer_gold.claims_development;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ YOUR TURN! Exercise 2: Development Analysis\n",
    "\n",
    "**Task**: Calculate the age-to-age development factor from 0-3 months to 4-6 months.\n",
    "\n",
    "**What You Need:**\n",
    "1. Sum of cumulative incurred for \"0-3 months\"\n",
    "2. Sum of cumulative incurred for \"4-6 months\"  \n",
    "3. Factor = (4-6 months) / (0-3 months)\n",
    "\n",
    "**Hint**: Use the table we just created!\n",
    "\n",
    "```sql\n",
    "-- YOUR CODE HERE\n",
    "SELECT \n",
    "    '0-3 to 4-6 month factor' AS factor_name,\n",
    "    -- Calculate the ratio here\n",
    "FROM payer_gold.claims_development\n",
    "WHERE development_period IN ('0-3 months', '4-6 months');\n",
    "```\n",
    "\n",
    "**Why This Matters**: Development factors help you project ultimate losses!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ SAS to Databricks Quick Reference for Actuaries\n",
    "\n",
    "This section shows you **side-by-side comparisons** of common SAS code and Databricks equivalents.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Basic Data Aggregation\n",
    "\n",
    "### SAS: PROC MEANS\n",
    "```sas\n",
    "PROC MEANS DATA=claims NOPRINT;\n",
    "    CLASS specialty;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=summary \n",
    "        N=claim_count \n",
    "        SUM=total_incurred \n",
    "        MEAN=avg_claim;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS total_incurred,\n",
    "    AVG(total_charge) AS avg_claim\n",
    "FROM claims\n",
    "GROUP BY specialty;\n",
    "```\n",
    "\n",
    "**üéØ Key Difference**: In Databricks, it's all in one SELECT statement!\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Frequency Tables\n",
    "\n",
    "### SAS: PROC FREQ\n",
    "```sas\n",
    "PROC FREQ DATA=claims;\n",
    "    TABLES specialty * state / NOCOL NOROW;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY with COUNT\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    state,\n",
    "    COUNT(*) AS frequency\n",
    "FROM claims\n",
    "GROUP BY specialty, state\n",
    "ORDER BY frequency DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Conditional Logic\n",
    "\n",
    "### SAS: DATA Step with IF-THEN\n",
    "```sas\n",
    "DATA claims_categorized;\n",
    "    SET claims;\n",
    "    IF total_charge < 1000 THEN risk_category = 'Low';\n",
    "    ELSE IF total_charge < 5000 THEN risk_category = 'Medium';\n",
    "    ELSE risk_category = 'High';\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: CASE WHEN\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN total_charge < 1000 THEN 'Low'\n",
    "        WHEN total_charge < 5000 THEN 'Medium'\n",
    "        ELSE 'High'\n",
    "    END AS risk_category\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Joining Tables\n",
    "\n",
    "### SAS: PROC SQL Join\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE enriched_claims AS\n",
    "    SELECT c.*, p.specialty, p.provider_name\n",
    "    FROM claims c\n",
    "    LEFT JOIN providers p \n",
    "        ON c.provider_id = p.provider_id;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "### Databricks: SQL Join (Identical!)\n",
    "```sql\n",
    "SELECT c.*, p.specialty, p.provider_name\n",
    "FROM claims c\n",
    "LEFT JOIN providers p \n",
    "    ON c.provider_id = p.provider_id;\n",
    "```\n",
    "\n",
    "**üéØ Great News**: If you know SAS PROC SQL, you already know Databricks SQL!\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Lagging and Leading Values (Trending)\n",
    "\n",
    "### SAS: LAG Function\n",
    "```sas\n",
    "DATA trends;\n",
    "    SET monthly_data;\n",
    "    prior_month = LAG(total_incurred);\n",
    "    growth_pct = (total_incurred - prior_month) / prior_month * 100;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: LAG Window Function\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY month) AS prior_month,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY month) * 100, 2) AS growth_pct\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Percentiles and Quantiles\n",
    "\n",
    "### SAS: PROC UNIVARIATE\n",
    "```sas\n",
    "PROC UNIVARIATE DATA=claims;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=percentiles \n",
    "        PCTLPTS=25 50 75 90 95 99\n",
    "        PCTLPRE=P;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: PERCENTILE_CONT\n",
    "```sql\n",
    "SELECT \n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_charge) AS P25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_charge) AS P50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_charge) AS P75,\n",
    "    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY total_charge) AS P90,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_charge) AS P95,\n",
    "    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY total_charge) AS P99\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Moving Averages (Smoothing)\n",
    "\n",
    "### SAS: Rolling Average\n",
    "```sas\n",
    "DATA moving_avg;\n",
    "    SET monthly_data;\n",
    "    avg_3mo = MEAN(total_incurred, LAG(total_incurred), LAG2(total_incurred));\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: Window Function with ROWS\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    AVG(total_incurred) OVER (\n",
    "        ORDER BY month \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) AS avg_3mo\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Quick Translation Guide\n",
    "\n",
    "| **SAS** | **Databricks** | **Notes** |\n",
    "|---------|----------------|-----------|\n",
    "| `PROC SQL` | SQL queries | Almost identical! |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregations | Very similar |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | Same logic |\n",
    "| `DATA` step | `SELECT` with transformations | Different syntax, same result |\n",
    "| `LAG()` | `LAG() OVER (ORDER BY)` | Window function needed |\n",
    "| `RETAIN` | Window functions | Use cumulative sums |\n",
    "| `MERGE` | `JOIN` | SQL joins |\n",
    "| `WHERE` | `WHERE` | Identical! |\n",
    "| `IF-THEN-ELSE` | `CASE WHEN` | Different syntax |\n",
    "| Macros | Widgets + parameters | Similar concept |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips for SAS Users\n",
    "\n",
    "1. **PROC SQL knowledge transfers 90%**: If you're comfortable with SAS PROC SQL, you'll pick up Databricks quickly!\n",
    "\n",
    "2. **Window functions = LAG/LEAD on steroids**: More powerful than SAS LAG functions.\n",
    "\n",
    "3. **No DATA step needed**: Most transformations can be done in SQL with `CASE WHEN`.\n",
    "\n",
    "4. **CTEs are your friend**: Use `WITH` clauses instead of creating intermediate datasets.\n",
    "\n",
    "5. **Display > PROC PRINT**: Just use `display()` in Python cells or `SELECT` in SQL.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Interactive Exercises - Test Your Skills!\n",
    "\n",
    "Now it's time to practice! These exercises cover common actuarial analyses.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Calculate Loss Ratios by Plan Type\n",
    "\n",
    "**Objective**: Find which plan has the worst loss ratio (highest incurred per member).\n",
    "\n",
    "**What to Calculate**:\n",
    "- Claims count by plan_id\n",
    "- Total incurred by plan_id\n",
    "- Average claim per member by plan_id\n",
    "- Sort by worst loss ratio first\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "-- YOUR TURN! Complete this query\n",
    "SELECT \n",
    "    m.plan_id,\n",
    "    COUNT(c.claim_id) AS _____,\n",
    "    SUM(c.total_charge) AS _____,\n",
    "    COUNT(DISTINCT m.member_id) AS member_count,\n",
    "    ROUND(SUM(c.total_charge) / COUNT(DISTINCT m.member_id), 2) AS loss_ratio_per_member\n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "GROUP BY _____\n",
    "ORDER BY _____ DESC;\n",
    "```\n",
    "\n",
    "**Hints**:\n",
    "- Fill in the blanks (_____)\n",
    "- Think about what metrics you need\n",
    "- How should you order the results?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Identify High-Risk Members\n",
    "\n",
    "**Objective**: Find members with unusually high claims (potential high-risk individuals).\n",
    "\n",
    "**Criteria**:\n",
    "- Members with total claims > 95th percentile\n",
    "- Include member demographics\n",
    "- Calculate their total incurred and claim count\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "-- Step 1: Find the 95th percentile threshold\n",
    "WITH percentile_threshold AS (\n",
    "    SELECT PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_charge) AS p95\n",
    "    FROM payer_silver.claims\n",
    "),\n",
    "-- Step 2: Identify high-cost members\n",
    "high_cost_members AS (\n",
    "    SELECT \n",
    "        member_id,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(total_charge) AS total_incurred\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY member_id\n",
    "    HAVING SUM(total_charge) > (SELECT p95 FROM percentile_threshold)\n",
    ")\n",
    "-- Step 3: YOUR TURN - Join with member details and display results\n",
    "SELECT \n",
    "    -- Add your columns here\n",
    "FROM high_cost_members h\n",
    "INNER JOIN payer_silver.members m ON h.member_id = m.member_id\n",
    "ORDER BY total_incurred DESC;\n",
    "```\n",
    "\n",
    "**What This Tells You**: These are your high-risk members who need care management!\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Month-over-Month Growth Rate\n",
    "\n",
    "**Objective**: Calculate which month had the highest growth in claims.\n",
    "\n",
    "**What to Find**:\n",
    "- Monthly total incurred\n",
    "- Prior month incurred (use LAG)\n",
    "- Month-over-month growth %\n",
    "- Which month had highest growth?\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "WITH monthly_totals AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        SUM(total_charge) AS monthly_incurred\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month\n",
    ")\n",
    "SELECT \n",
    "    claim_month,\n",
    "    monthly_incurred,\n",
    "    -- YOUR TURN: Add LAG function to get prior month\n",
    "    LAG(monthly_incurred, ___) OVER (ORDER BY ___) AS prior_month,\n",
    "    -- YOUR TURN: Calculate growth percentage\n",
    "    ROUND((monthly_incurred - LAG(_____, ___) OVER (ORDER BY _____)) / \n",
    "          LAG(_____, ___) OVER (ORDER BY _____) * 100, 2) AS mom_growth_pct\n",
    "FROM monthly_totals\n",
    "ORDER BY mom_growth_pct DESC  -- Highest growth first\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Actuarial Insight**: Sudden spikes might indicate unusual events (epidemics, policy changes, fraud).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: Provider Network Analysis\n",
    "\n",
    "**Objective**: Which providers/specialties drive the most costs?\n",
    "\n",
    "**Analysis Steps**:\n",
    "1. Group by provider specialty\n",
    "2. Calculate total incurred, claim count, avg claim size\n",
    "3. Calculate % of total costs\n",
    "4. Identify top cost drivers\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "WITH specialty_costs AS (\n",
    "    SELECT \n",
    "        p.specialty,\n",
    "        COUNT(c.claim_id) AS claim_count,\n",
    "        SUM(c.total_charge) AS total_incurred,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.specialty\n",
    ")\n",
    "SELECT \n",
    "    specialty,\n",
    "    claim_count,\n",
    "    total_incurred,\n",
    "    avg_claim_cost,\n",
    "    -- YOUR TURN: Calculate % of total costs\n",
    "    ROUND(total_incurred * 100.0 / SUM(total_incurred) OVER (), 2) AS pct_of_total_cost\n",
    "FROM specialty_costs\n",
    "ORDER BY total_incurred DESC;\n",
    "```\n",
    "\n",
    "**Why This Matters**: Understanding cost drivers helps with:\n",
    "- Network contracting\n",
    "- Provider steerage\n",
    "- Benefit design\n",
    "- Rate setting\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Challenge Exercise: Build Your Own Analysis!\n",
    "\n",
    "Pick one of these actuarial questions and build the SQL to answer it:\n",
    "\n",
    "1. **Geographic Analysis**: Which states have the best/worst loss ratios?\n",
    "2. **Gender Analysis**: Do claims patterns differ by gender?\n",
    "3. **Age Analysis**: How do claims trend with member age? (Use birth_date to calculate age)\n",
    "4. **Seasonality**: Are there seasonal patterns in claims? (Monthly analysis)\n",
    "5. **Large Claims**: Identify claims above $10,000 - what specialties are they from?\n",
    "\n",
    "**Your Code Here**:\n",
    "```sql\n",
    "-- Pick a question and write your analysis!\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Workshop Summary - You Did It!\n",
    "\n",
    "## üéâ Congratulations, Actuaries!\n",
    "\n",
    "You've just completed your first Databricks workshop! Let's review what you learned.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished Today\n",
    "\n",
    "### 1. **Transitioned from SAS to Databricks** üîÑ\n",
    "- Learned how your SAS knowledge transfers\n",
    "- Ran SQL queries (very similar to PROC SQL!)\n",
    "- Used window functions (LAG, LEAD for trending)\n",
    "\n",
    "### 2. **Built Actuarial Analytics** üìä\n",
    "- ‚úÖ **Loss Ratios** by segment\n",
    "- ‚úÖ **Claims Trending** (month-over-month growth)\n",
    "- ‚úÖ **Development Patterns** (claims emergence)\n",
    "- ‚úÖ **Risk Segmentation** (high-cost members)\n",
    "- ‚úÖ **Provider Analysis** (cost drivers)\n",
    "\n",
    "### 3. **Learned Key SQL Techniques** üíª\n",
    "- `GROUP BY` for aggregations (like PROC MEANS)\n",
    "- `JOIN` for combining tables\n",
    "- `CASE WHEN` for conditional logic (like IF-THEN)\n",
    "- `LAG/LEAD` for trending (like SAS LAG functions)\n",
    "- `PERCENTILE_CONT` for quantiles (like PROC UNIVARIATE)\n",
    "- `WINDOW FUNCTIONS` for running calculations\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How to Use This at Work\n",
    "\n",
    "### Immediate Applications:\n",
    "\n",
    "1. **Quarterly Loss Ratio Reports**\n",
    "   - Use the loss ratio queries we built\n",
    "   - Group by state, specialty, plan type\n",
    "   - Export to dashboards (no more Excel!)\n",
    "\n",
    "2. **Monthly Trending Analysis**\n",
    "   - Monitor claims frequency and severity trends\n",
    "   - Identify unusual spikes early\n",
    "   - Feed into your pricing models\n",
    "\n",
    "3. **Reserving Support**\n",
    "   - Build development triangles\n",
    "   - Calculate age-to-age factors\n",
    "   - Track IBNR emergence patterns\n",
    "\n",
    "4. **Risk Management**\n",
    "   - Identify high-risk members\n",
    "   - Segment populations for care management\n",
    "   - Calculate risk scores\n",
    "\n",
    "5. **Rate Filings**\n",
    "   - Trend historical claims\n",
    "   - Support rate change justifications\n",
    "   - Build exhibits for regulators\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps for Actuaries\n",
    "\n",
    "### Week 1: Practice the Basics\n",
    "- [ ] Recreate one of your SAS reports in Databricks\n",
    "- [ ] Try writing 5 simple SQL queries\n",
    "- [ ] Calculate a loss ratio for your own data\n",
    "- [ ] Create one visualization\n",
    "\n",
    "### Week 2-4: Build Real Analyses\n",
    "- [ ] Build a complete loss ratio analysis\n",
    "- [ ] Create a development triangle\n",
    "- [ ] Set up monthly trend monitoring\n",
    "- [ ] Share a dashboard with your team\n",
    "\n",
    "### Month 2-3: Advanced Topics\n",
    "- [ ] Learn PySpark (for complex calculations)\n",
    "- [ ] Automate your reports (scheduled notebooks)\n",
    "- [ ] Build predictive models (ML)\n",
    "- [ ] Create interactive dashboards (Databricks SQL)\n",
    "\n",
    "---\n",
    "\n",
    "## üÜò Getting Help\n",
    "\n",
    "### When You're Stuck:\n",
    "\n",
    "1. **Use the AI Assistant** ü§ñ\n",
    "   - Click the AI icon in any cell\n",
    "   - Ask: \"How do I calculate a loss ratio?\"\n",
    "   - Ask: \"Convert this SAS code to SQL\"\n",
    "\n",
    "2. **Databricks Documentation**\n",
    "   - [SQL Reference](https://docs.databricks.com/sql/language-manual/)\n",
    "   - [Window Functions](https://docs.databricks.com/sql/language-manual/sql-ref-window-functions.html)\n",
    "   - [Date Functions](https://docs.databricks.com/sql/language-manual/sql-ref-datetime-functions.html)\n",
    "\n",
    "3. **Community Resources**\n",
    "   - [Databricks Community Forums](https://community.databricks.com/)\n",
    "   - [Stack Overflow - Databricks Tag](https://stackoverflow.com/questions/tagged/databricks)\n",
    "   - Internal company Databricks experts\n",
    "\n",
    "4. **Your Colleagues**\n",
    "   - Share this notebook with other actuaries!\n",
    "   - Form a study group\n",
    "   - Practice together\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "### 1. **You Already Know More Than You Think!**\n",
    "If you know SAS PROC SQL, you know 90% of Databricks SQL. The syntax is almost identical!\n",
    "\n",
    "### 2. **Start Simple**\n",
    "Don't try to learn everything at once. Start with:\n",
    "- Basic `SELECT` queries\n",
    "- Simple aggregations (`GROUP BY`)\n",
    "- Joins\n",
    "\n",
    "Then gradually add:\n",
    "- Window functions\n",
    "- CTEs (WITH clauses)\n",
    "- Advanced analytics\n",
    "\n",
    "### 3. **SQL is Enough for Most Actuarial Work**\n",
    "You don't need to learn Python/PySpark right away. Most actuarial analyses can be done with SQL alone!\n",
    "\n",
    "### 4. **Iterate and Improve**\n",
    "Your first queries won't be perfect. That's okay! \n",
    "- Start with something that works\n",
    "- Refine it over time\n",
    "- Ask for feedback\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Your Action Plan\n",
    "\n",
    "**This Week:**\n",
    "1. Open Databricks and run your first query\n",
    "2. Try recreating ONE simple SAS report\n",
    "3. Share your success with a colleague!\n",
    "\n",
    "**This Month:**\n",
    "1. Build one complete analysis (loss ratios or trends)\n",
    "2. Create one visualization\n",
    "3. Teach someone else what you learned\n",
    "\n",
    "**This Quarter:**\n",
    "1. Migrate one regular report to Databricks\n",
    "2. Learn one new technique (window functions, CTEs, etc.)\n",
    "3. Explore PySpark basics (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Recommended Learning Resources\n",
    "\n",
    "### For Actuaries:\n",
    "1. **Databricks SQL Guide** (focus on aggregations and window functions)\n",
    "2. **Healthcare Analytics Examples** (look for payer-specific use cases)\n",
    "3. **SAS to SQL Translation Guides** (many available online)\n",
    "\n",
    "### Courses:\n",
    "1. **Databricks Fundamentals** (free online course)\n",
    "2. **SQL for Data Analysis** (many free resources)\n",
    "3. **Healthcare Analytics on Databricks** (industry-specific)\n",
    "\n",
    "### Books:\n",
    "1. \"SQL for Data Analytics\" (O'Reilly)\n",
    "2. \"Databricks Certified Data Analyst Study Guide\"\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop! Remember:\n",
    "- **Be patient with yourself** - Learning new tools takes time\n",
    "- **Practice regularly** - Try to use Databricks weekly\n",
    "- **Ask questions** - No question is too simple\n",
    "- **Help others** - Teaching reinforces your own learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéÅ Bonus: Cheat Sheet for Your Desk\n",
    "\n",
    "```\n",
    "COMMON ACTUARIAL QUERIES:\n",
    "\n",
    "1. Loss Ratio:\n",
    "   SELECT segment, SUM(claims)/SUM(premium) \n",
    "   FROM data GROUP BY segment;\n",
    "\n",
    "2. Trending:\n",
    "   SELECT month, value, LAG(value) OVER (ORDER BY month)\n",
    "   FROM data;\n",
    "\n",
    "3. Percentiles:\n",
    "   SELECT PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY claim)\n",
    "   FROM claims;\n",
    "\n",
    "4. Moving Average:\n",
    "   SELECT date, AVG(value) OVER (ORDER BY date \n",
    "          ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)\n",
    "   FROM data;\n",
    "\n",
    "5. Development Factor:\n",
    "   SELECT period, SUM(claims), \n",
    "          SUM(claims)/LAG(SUM(claims)) OVER (ORDER BY period)\n",
    "   FROM triangle GROUP BY period;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Stay Connected\n",
    "\n",
    "- **Questions?** Ask your workshop instructor or Databricks team\n",
    "- **Want More?** Let us know what topics you'd like to see\n",
    "- **Success Stories?** Share how you're using Databricks!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Analyzing!** üöÄüìäüéØ\n",
    "\n",
    "*Remember: Every expert was once a beginner. You've got this!*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b0153-df99-4566-982a-745e97ff0888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Enriched Tables with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da30fc9-0891-4c8a-8bbd-aae8687f1372",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Enriched Claims, Members, and Providers Summary Tables"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create gold schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_gold;\n",
    "\n",
    "-- Gold: Claims with member and provider details\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_enriched AS\n",
    "SELECT\n",
    "  c.claim_id,\n",
    "  c.claim_date,\n",
    "  c.total_charge,\n",
    "  c.claim_status,\n",
    "  m.member_id,\n",
    "  m.first_name,\n",
    "  m.last_name,\n",
    "  m.gender,\n",
    "  m.plan_id,\n",
    "  p.provider_id,\n",
    "  p.provider_name,\n",
    "  p.specialty,\n",
    "  p.city,\n",
    "  p.state\n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id;\n",
    "\n",
    "\n",
    "-- Gold: Claim Aggregates per Member\n",
    "CREATE OR REPLACE TABLE payer_gold.member_claim_summary AS\n",
    "SELECT\n",
    "  member_id,\n",
    "  COUNT(DISTINCT claim_id) AS total_claims,\n",
    "  SUM(total_charge) AS sum_claims,\n",
    "  MAX(total_charge) AS max_claim,\n",
    "  MIN(total_charge) AS min_claim\n",
    "FROM payer_silver.claims\n",
    "GROUP BY member_id;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67f5872-4eed-41cb-a33d-20b41fa85b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Advanced Analytics with PySpark\n",
    "\n",
    "Let's create more sophisticated analytics tables using PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd070c5e-2128-43c3-bb65-cb5fec3566f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1: Provider Performance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fc395e-70a3-43f4-a0e4-0698fcf46d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, min, max, stddev, countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load tables\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "\n",
    "# Create provider performance metrics\n",
    "provider_performance = claims.join(providers, \"provider_id\") \\\n",
    "    .groupBy(\n",
    "        \"provider_id\", \n",
    "        \"provider_name\", \n",
    "        \"specialty\", \n",
    "        \"city\", \n",
    "        \"state\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        countDistinct(\"member_id\").alias(\"unique_patients\"),\n",
    "        sum(\"total_charge\").alias(\"total_revenue\"),\n",
    "        avg(\"total_charge\").alias(\"avg_claim_amount\"),\n",
    "        min(\"total_charge\").alias(\"min_claim_amount\"),\n",
    "        max(\"total_charge\").alias(\"max_claim_amount\"),\n",
    "        stddev(\"total_charge\").alias(\"stddev_claim_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"Top 10 Providers by Revenue:\")\n",
    "display(provider_performance.limit(10))\n",
    "\n",
    "# Save to Gold table\n",
    "provider_performance.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.provider_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ff1561-f6e2-43db-91a2-bef5a7e9ccd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2: Time-Series Analysis - Monthly Claims Trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0060180a-6efb-4de0-bc2f-31e90ba6a9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, date_format, lag, round as spark_round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create monthly trends\n",
    "monthly_claims = claims \\\n",
    "    .withColumn(\"year\", year(\"claim_date\")) \\\n",
    "    .withColumn(\"month\", month(\"claim_date\")) \\\n",
    "    .withColumn(\"month_year\", date_format(\"claim_date\", \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year\", \"month\", \"month_year\", \"claim_status\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"claim_count\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge\")\n",
    "    ) \\\n",
    "    .orderBy(\"year\", \"month\", \"claim_status\")\n",
    "\n",
    "# Calculate month-over-month growth\n",
    "window_spec = Window.partitionBy(\"claim_status\").orderBy(\"year\", \"month\")\n",
    "\n",
    "monthly_trends = monthly_claims \\\n",
    "    .withColumn(\"prev_month_charges\", lag(\"total_charges\").over(window_spec)) \\\n",
    "    .withColumn(\n",
    "        \"mom_growth_pct\", \n",
    "        spark_round(\n",
    "            ((col(\"total_charges\") - col(\"prev_month_charges\")) / col(\"prev_month_charges\") * 100), \n",
    "            2\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Monthly Claims Trends with Growth:\")\n",
    "display(monthly_trends)\n",
    "\n",
    "# Save to Gold\n",
    "monthly_trends.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.monthly_claims_trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dc7637-0cbc-45aa-830a-6eef7f338892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"What kind of aggregations can I do with table payer_gold.claims_enriched?\"\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713ffea-c715-46f9-a267-75093518164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìä Analytics & Visualization\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Databricks Visualizations\n",
    "\n",
    "Databricks provides powerful built-in visualization capabilities. You can create:\n",
    "- üìä Bar charts, line charts, scatter plots\n",
    "- üó∫Ô∏è Geographic maps\n",
    "- üìà Histograms and box plots\n",
    "- ü•ß Pie charts and area charts\n",
    "\n",
    "> **üí° Tip**: Use `display()` function to automatically generate interactive visualizations!\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Analysis with PySpark\n",
    "\n",
    "Let's explore our data using statistical analysis and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d43e2-a24b-4243-9566-3e3d8c733913",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Payer Claims Enriched Table"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"payer_gold.claims_enriched\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6513d65d-db72-4aa1-89a8-ffb2de886d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27af84c5-623f-44d6-9a93-019c3f90fb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Claims by Status (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55eeef7f-0571-40ba-9269-87c1388958c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges by Claim Status"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHN1bSwgY291bnQsIGF2ZwoKIyBBZ2dyZWdhdGUgdG90YWwgY2hhcmdlcyBieSBjbGFpbSBzdGF0dXMKY2xhaW1zX2J5X3N0YXR1cyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJjbGFpbV9zdGF0dXMiKSBcCiAgICAuYWdnKAogICAgICAgIHN1bSgidG90YWxfY2hhcmdlIikuYWxpYXMoInRvdGFsX2NoYXJnZXMiKSwKICAgICAgICBjb3VudCgiY2xhaW1faWQiKS5hbGlhcygiY2xhaW1fY291bnQiKSwKICAgICAgICBhdmcoInRvdGFsX2NoYXJnZSIpLmFsaWFzKCJhdmdfY2hhcmdlIikKICAgICkgXAogICAgLm9yZGVyQnkoInRvdGFsX2NoYXJnZXMiLCBhc2NlbmRpbmc9RmFsc2UpCgojIFZpc3VhbGl6ZSBhcyBhIGJhciBjaGFydDogWC1heGlzID0gY2xhaW1fc3RhdHVzLCBZLWF4aXMgPSB0b3RhbF9jaGFyZ2VzCmRpc3BsYXkoY2xhaW1zX2J5X3N0YXR1cykKCiMgVGlwOiBJbiB0aGUgdmlzdWFsaXphdGlvbiBvcHRpb25zLCBzZWxlY3QgIkJhciBDaGFydCIsIHNldCBYLWF4aXMgdG8gJ2NsYWltX3N0YXR1cycsIFktYXhpcyB0byAndG90YWxfY2hhcmdlcyc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewa441258\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewa441258\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewa441258\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewa441258) SELECT `claim_status`,SUM(`total_charges`) `column_3c6cda842464` FROM q GROUP BY `claim_status`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewa441258\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_status",
             "id": "column_3c6cda842463"
            },
            "y": [
             {
              "column": "total_charges",
              "id": "column_3c6cda842464",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842464": {
             "name": "sum(total_charge)",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "516b639a-ca4d-4846-837e-77f7620816f5",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_status",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_status",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842464",
           "args": [
            {
             "column": "total_charges",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg\n",
    "\n",
    "# Aggregate total charges by claim status\n",
    "claims_by_status = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"claim_status\") \\\n",
    "    .agg(\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        count(\"claim_id\").alias(\"claim_count\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_charges\", ascending=False)\n",
    "\n",
    "# Visualize as a bar chart: X-axis = claim_status, Y-axis = total_charges\n",
    "display(claims_by_status)\n",
    "\n",
    "# Tip: In the visualization options, select \"Bar Chart\", set X-axis to 'claim_status', Y-axis to 'total_charges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bd3b72-1e38-4861-a90a-b1c0155d02d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Gender Distribution (Pie Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a772eaa0-abc2-43a6-8185-6bbcb61ba3d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count of Claims Grouped by Gender"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBBbmFseXplIGNsYWltcyBkaXN0cmlidXRpb24gYnkgZ2VuZGVyCmdlbmRlcl9hbmFseXNpcyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJnZW5kZXIiKSBcCiAgICAuYWdnKAogICAgICAgIGNvdW50KCJjbGFpbV9pZCIpLmFsaWFzKCJ0b3RhbF9jbGFpbXMiKSwKICAgICAgICBzdW0oInRvdGFsX2NoYXJnZSIpLmFsaWFzKCJ0b3RhbF9jaGFyZ2VzIiksCiAgICAgICAgYXZnKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygiYXZnX2NoYXJnZV9wZXJfY2xhaW0iKQogICAgKQoKcHJpbnQoIvCfkaUgQ2xhaW1zIEFuYWx5c2lzIGJ5IEdlbmRlcjoiKQpkaXNwbGF5KGdlbmRlcl9hbmFseXNpcykKCiMgVGlwOiBUcnkgY2hhbmdpbmcgdGhlIHZpc3VhbGl6YXRpb24gdG8gYSBQaWUgQ2hhcnQgdG8gc2VlIHRoZSBkaXN0cmlidXRpb24h\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewf3f116c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewf3f116c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewf3f116c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewf3f116c) SELECT `gender`,COUNT(`gender`) `column_3c6cda842478` FROM q GROUP BY `gender`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewf3f116c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "gender",
             "id": "column_3c6cda842477"
            },
            "y": [
             {
              "column": "gender",
              "id": "column_3c6cda842478",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842478": {
             "name": "count",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c7d6472d-90b5-4136-813a-67b5fb01f6b8",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "gender",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "gender",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842478",
           "args": [
            {
             "column": "gender",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze claims distribution by gender\n",
    "gender_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"gender\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge_per_claim\")\n",
    "    )\n",
    "\n",
    "print(\"üë• Claims Analysis by Gender:\")\n",
    "display(gender_analysis)\n",
    "\n",
    "# Tip: Try changing the visualization to a Pie Chart to see the distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fb4408-de01-4392-b809-162cdc6e3cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Time Series - Claims Over Time (Line Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85dff0f-de3d-4084-bff0-4e8d1c11a22f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges Grouped by Claim Date"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBUaW1lIHNlcmllcyBhbmFseXNpcyBvZiBjbGFpbXMKdGltZV9zZXJpZXMgPSBzcGFyay50YWJsZSgicGF5ZXJfZ29sZC5jbGFpbXNfZW5yaWNoZWQiKSBcCiAgICAuZ3JvdXBCeSgiY2xhaW1fZGF0ZSIpIFwKICAgIC5hZ2coCiAgICAgICAgc3VtKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygiZGFpbHlfY2hhcmdlcyIpLAogICAgICAgIGNvdW50KCJjbGFpbV9pZCIpLmFsaWFzKCJkYWlseV9jbGFpbV9jb3VudCIpCiAgICApIFwKICAgIC5vcmRlckJ5KCJjbGFpbV9kYXRlIikKCnByaW50KCLwn5OIIERhaWx5IENsYWltcyBUcmVuZHM6IikKZGlzcGxheSh0aW1lX3NlcmllcykKCiMgVGlwOiBTZWxlY3QgTGluZSBDaGFydCB2aXN1YWxpemF0aW9uIHdpdGggY2xhaW1fZGF0ZSBhcyBYLWF4aXMgYW5kIGRhaWx5X2NoYXJnZXMgYXMgWS1heGlzIQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd0f7212\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd0f7212\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd0f7212\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd0f7212) SELECT `claim_date`,SUM(`daily_charges`) `column_3c6cda842489` FROM q GROUP BY `claim_date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd0f7212\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842488"
            },
            "y": [
             {
              "column": "daily_charges",
              "id": "column_3c6cda842489",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842489": {
             "name": "sum(total_charge)",
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "a82fd1a1-168b-4171-b219-ca56315cf563",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842489",
           "args": [
            {
             "column": "daily_charges",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time series analysis of claims\n",
    "time_series = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"claim_date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_charge\").alias(\"daily_charges\"),\n",
    "        count(\"claim_id\").alias(\"daily_claim_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"claim_date\")\n",
    "\n",
    "print(\"üìà Daily Claims Trends:\")\n",
    "display(time_series)\n",
    "\n",
    "# Tip: Select Line Chart visualization with claim_date as X-axis and daily_charges as Y-axis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7439bb5-34bd-47c3-a604-99b8471ab7a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Geographic Analysis - Claims by City (Map/Bar Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6923f450-56f2-46da-a356-dbb09b5a5ec1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group Claims by City and Count"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBHZW9ncmFwaGljIGRpc3RyaWJ1dGlvbiBvZiBjbGFpbXMKY2l0eV9hbmFseXNpcyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJjaXR5IiwgInN0YXRlIikgXAogICAgLmFnZygKICAgICAgICBjb3VudCgiY2xhaW1faWQiKS5hbGlhcygidG90YWxfY2xhaW1zIiksCiAgICAgICAgc3VtKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygidG90YWxfY2hhcmdlcyIpLAogICAgICAgIGNvdW50RGlzdGluY3QoIm1lbWJlcl9pZCIpLmFsaWFzKCJ1bmlxdWVfbWVtYmVycyIpCiAgICApIFwKICAgIC5vcmRlckJ5KCJ0b3RhbF9jaGFyZ2VzIiwgYXNjZW5kaW5nPUZhbHNlKQoKcHJpbnQoIvCfl7rvuI8gQ2xhaW1zIERpc3RyaWJ1dGlvbiBieSBDaXR5OiIpCmRpc3BsYXkoY2l0eV9hbmFseXNpcy5saW1pdCgyMCkpCgojIFRpcDogVHJ5IE1hcCB2aXN1YWxpemF0aW9uIGlmIHlvdXIgZGF0YSBoYXMgcHJvcGVyIGdlb2dyYXBoaWMgZmllbGRzIQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewaec95fc\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewaec95fc\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewaec95fc\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewaec95fc) SELECT `city`,SUM(`count`) `column_3c6cda842507` FROM q GROUP BY `city`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewaec95fc\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Bar Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "city",
             "id": "column_3c6cda842506"
            },
            "y": [
             {
              "column": "count",
              "id": "column_3c6cda842507",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842507": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "448dac26-5767-4652-830e-d0e81c3424cc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "city",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "city",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842507",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "# Geographic distribution of claims\ncity_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n    .groupBy(\"city\", \"state\") \\\n    .agg(\n        count(\"claim_id\").alias(\"total_claims\"),\n        sum(\"total_charge\").alias(\"total_charges\"),\n        countDistinct(\"member_id\").alias(\"unique_members\")\n    ) \\\n    .orderBy(\"total_charges\", ascending=False)\n\nprint(\"üó∫Ô∏è Claims Distribution by City:\")\ndisplay(city_analysis.limit(20))\n\n# Tip: Try Map visualization if your data has proper geographic fields!",
       "commandTitle": "Map",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHOROPLETH"
         },
         {
          "key": "options",
          "value": {
           "clusteringMode": "e",
           "colors": {
            "background": "#ffffff",
            "borders": "#ffffff",
            "max": "#002FB4",
            "min": "#799CFF",
            "noValue": "#dddddd"
           },
           "keyColumn": "city",
           "legend": {
            "alignText": "right",
            "position": "bottom-left",
            "visible": true
           },
           "mapType": "usa",
           "noValuePlaceholder": "N/A",
           "popup": {
            "enabled": true,
            "template": "Region: <b>{{ @@name }}</b>\n<br>\nValue: <b>{{ @@value }}</b>"
           },
           "steps": 5,
           "targetField": "name",
           "tooltip": {
            "enabled": true,
            "template": "<b>{{ @@name }}</b>: {{ @@value }}"
           },
           "valueColumn": "total_charges",
           "valueFormat": "0,0.00"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b62534d2-5c77-4526-bffd-6ac36cb1e5b4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Geographic distribution of claims\n",
    "city_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"city\", \"state\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        countDistinct(\"member_id\").alias(\"unique_members\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_charges\", ascending=False)\n",
    "\n",
    "print(\"üó∫Ô∏è Claims Distribution by City:\")\n",
    "display(city_analysis.limit(20))\n",
    "\n",
    "# Tip: Try Map visualization if your data has proper geographic fields!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e0c6dc-e3a0-451a-930f-f39c928a55c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Distribution Analysis - Charge Amount Histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ee3c88-20a6-4788-95b2-7bc433d43534",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Total Charges from Payer Claims Table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBBbmFseXplIHRoZSBkaXN0cmlidXRpb24gb2YgY2xhaW0gY2hhcmdlcwpjaGFyZ2VzID0gc3BhcmsudGFibGUoInBheWVyX2dvbGQuY2xhaW1zX2VucmljaGVkIikuc2VsZWN0KCJ0b3RhbF9jaGFyZ2UiKQoKIyBHZXQgc3RhdGlzdGljYWwgc3VtbWFyeQpwcmludCgi8J+TiiBTdGF0aXN0aWNhbCBTdW1tYXJ5IG9mIENsYWltIENoYXJnZXM6IikKY2hhcmdlcy5kZXNjcmliZSgpLnNob3coKQoKIyBEaXNwbGF5IGhpc3RvZ3JhbQpwcmludCgiXG7wn5K1IENoYXJnZSBEaXN0cmlidXRpb24gKEhpc3RvZ3JhbSk6IikKZGlzcGxheShjaGFyZ2VzKQoKIyBUaXA6IFNlbGVjdCBIaXN0b2dyYW0gdmlzdWFsaXphdGlvbiB0byBzZWUgdGhlIGRpc3RyaWJ1dGlvbiBvZiBjaGFyZ2VzIQojIFlvdSBjYW4gYWRqdXN0IHRoZSBudW1iZXIgb2YgYmlucyBmb3IgYmV0dGVyIGdyYW51bGFyaXR5Lg==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView77febbe\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView77febbe\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView77febbe\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView77febbe) ,min_max AS (SELECT `total_charge`,(SELECT MAX(`total_charge`) FROM q) `target_column_max`,(SELECT MIN(`total_charge`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `total_charge`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 10 `step` FROM min_max) SELECT IF(ISNULL(`total_charge`),NULL,LEAST(WIDTH_BUCKET(`total_charge`,`min_value`,`max_value`,10),10)) `total_charge_BIN`,FIRST(`min_value` + ((IF(ISNULL(`total_charge`),NULL,LEAST(WIDTH_BUCKET(`total_charge`,`min_value`,`max_value`,10),10)) - 1) * `step`)) `total_charge_BIN_LOWER_BOUND`,FIRST(`step`) `total_charge_BIN_STEP`,COUNT(`total_charge`) `COUNT` FROM histogram_meta GROUP BY `total_charge_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView77febbe\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Bar Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "total_charge",
             "id": "column_3c6cda842509"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "5641bb40-65b6-4acd-aafb-5f180b0ec2e7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "total_charge_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "total_charge_BIN",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "total_charge_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "total_charge_BIN_STEP",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze the distribution of claim charges\n",
    "charges = spark.table(\"payer_gold.claims_enriched\").select(\"total_charge\")\n",
    "\n",
    "# Get statistical summary\n",
    "print(\"üìä Statistical Summary of Claim Charges:\")\n",
    "charges.describe().show()\n",
    "\n",
    "# Display histogram\n",
    "print(\"\\nüíµ Charge Distribution (Histogram):\")\n",
    "display(charges)\n",
    "\n",
    "# Tip: Select Histogram visualization to see the distribution of charges!\n",
    "# You can adjust the number of bins for better granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c481c9d-e2db-4f9d-bb8d-7a059e0fd3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Correlation Analysis - Scatter Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b953e3-6eba-43c5-8157-48245b421e22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Claim Date and Total Charge from Payer Claims"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTY2F0dGVyIHBsb3QgdG8gZmluZCByZWxhdGlvbnNoaXBzIGJldHdlZW4gdmFyaWFibGVzCnNjYXR0ZXJfZGF0YSA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5zZWxlY3QoImNsYWltX2RhdGUiLCAidG90YWxfY2hhcmdlIiwgImNsYWltX3N0YXR1cyIpCgpwcmludCgi8J+TjSBTY2F0dGVyIFBsb3Q6IENsYWltIERhdGUgdnMgQ2hhcmdlIEFtb3VudCIpCmRpc3BsYXkoc2NhdHRlcl9kYXRhKQoKIyBUaXA6IFNlbGVjdCBTY2F0dGVyIFBsb3QgdmlzdWFsaXphdGlvbgojIFgtYXhpczogY2xhaW1fZGF0ZSwgWS1heGlzOiB0b3RhbF9jaGFyZ2UKIyBHcm91cCBieTogY2xhaW1fc3RhdHVzIGZvciBjb2xvciBjb2Rpbmc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView00ca72a\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView00ca72a\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView00ca72a\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView00ca72a) SELECT `claim_date`,`total_charge` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView00ca72a\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Scatter Plot",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842514"
            },
            "y": [
             {
              "column": "total_charge",
              "id": "column_3c6cda842515"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842515": {
             "name": "total_charge",
             "type": "scatter",
             "yAxis": 0
            },
            "total_charge": {
             "name": "total_charge",
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8e59f8fe-b565-4c7a-aad2-48446055c509",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.45947265625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "column": "total_charge",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTY2F0dGVyIHBsb3QgdG8gZmluZCByZWxhdGlvbnNoaXBzIGJldHdlZW4gdmFyaWFibGVzCnNjYXR0ZXJfZGF0YSA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5zZWxlY3QoImNsYWltX2RhdGUiLCAidG90YWxfY2hhcmdlIiwgImNsYWltX3N0YXR1cyIpCgpwcmludCgi8J+TjSBTY2F0dGVyIFBsb3Q6IENsYWltIERhdGUgdnMgQ2hhcmdlIEFtb3VudCIpCmRpc3BsYXkoc2NhdHRlcl9kYXRhKQoKIyBUaXA6IFNlbGVjdCBTY2F0dGVyIFBsb3QgdmlzdWFsaXphdGlvbgojIFgtYXhpczogY2xhaW1fZGF0ZSwgWS1heGlzOiB0b3RhbF9jaGFyZ2UKIyBHcm91cCBieTogY2xhaW1fc3RhdHVzIGZvciBjb2xvciBjb2Rpbmc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView05381e1\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView05381e1\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView05381e1\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView05381e1) SELECT `claim_date`,SUM(`total_charge`) `column_3c6cda842511` FROM q GROUP BY `claim_date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView05381e1\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Line Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842510"
            },
            "y": [
             {
              "column": "total_charge",
              "id": "column_3c6cda842511",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842511": {
             "name": "total_charge",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "093ecb4c-2a74-40a8-9b46-2d675add2207",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842511",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot to find relationships between variables\n",
    "scatter_data = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .select(\"claim_date\", \"total_charge\", \"claim_status\")\n",
    "\n",
    "print(\"üìç Scatter Plot: Claim Date vs Charge Amount\")\n",
    "display(scatter_data)\n",
    "\n",
    "# Tip: Select Scatter Plot visualization\n",
    "# X-axis: claim_date, Y-axis: total_charge\n",
    "# Group by: claim_status for color coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b7166b-70ab-4b63-8055-51c5d42cc956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üéØ Advanced Analytics Example: Cohort Analysis\n",
    "\n",
    "Let's create a more complex analysis - member cohort analysis based on their enrollment date:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0836fe4a-ef44-4b1d-bea7-f8458e833451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, floor\n",
    "\n",
    "# Join members with their claims\n",
    "member_claims = spark.table(\"payer_silver.members\") \\\n",
    "    .join(spark.table(\"payer_silver.claims\"), \"member_id\", \"left\")\n",
    "\n",
    "# Create cohorts based on enrollment month\n",
    "cohort_analysis = member_claims \\\n",
    "    .withColumn(\"enrollment_month\", date_format(\"effective_date\", \"yyyy-MM\")) \\\n",
    "    .withColumn(\"claim_month\", date_format(\"claim_date\", \"yyyy-MM\")) \\\n",
    "    .withColumn(\n",
    "        \"months_since_enrollment\", \n",
    "        floor(months_between(\"claim_date\", \"effective_date\"))\n",
    "    ) \\\n",
    "    .groupBy(\"enrollment_month\", \"months_since_enrollment\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"member_id\").alias(\"active_members\"),\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\")\n",
    "    ) \\\n",
    "    .filter(col(\"months_since_enrollment\").isNotNull()) \\\n",
    "    .orderBy(\"enrollment_month\", \"months_since_enrollment\")\n",
    "\n",
    "print(\"üìÖ Member Cohort Analysis:\")\n",
    "display(cohort_analysis)\n",
    "\n",
    "# Save as a Gold table for future analysis\n",
    "cohort_analysis.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.member_cohort_analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a066c3-a0c9-41e8-bdd4-475282fb0950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa71c33b-47d1-4b46-9940-2fdfd3cfb439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafd9a2a-eca0-430e-8cd9-edfa1595df17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Best Practices & Performance Tips\n",
    "\n",
    "## üöÄ Performance Optimization\n",
    "\n",
    "### 1. **Use Partitioning for Large Tables**\n",
    "```python\n",
    "# Partition by date for time-series data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"claim_date\") \\\n",
    "    .saveAsTable(\"payer_gold.claims_partitioned\")\n",
    "```\n",
    "\n",
    "### 2. **Enable Z-Ordering for Common Filters**\n",
    "```sql\n",
    "OPTIMIZE payer_gold.claims_enriched\n",
    "ZORDER BY (member_id, claim_date);\n",
    "```\n",
    "\n",
    "### 3. **Use Caching for Frequently Accessed DataFrames**\n",
    "```python\n",
    "claims_df = spark.table(\"payer_silver.claims\").cache()\n",
    "# Now use claims_df multiple times without re-reading\n",
    "```\n",
    "\n",
    "### 4. **Broadcast Small Tables in Joins**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîí Data Quality Best Practices\n",
    "\n",
    "### 1. **Always Validate Data**\n",
    "```python\n",
    "# Add constraints\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE payer_silver.claims \n",
    "    ADD CONSTRAINT valid_charge CHECK (total_charge > 0)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. **Use Schema Evolution Carefully**\n",
    "```python\n",
    "# Explicitly define schema for production\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"total_charge\", DoubleType(), True),\n",
    "    # ... more fields\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. **Implement Data Quality Checks**\n",
    "```python\n",
    "def validate_claims(df):\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    checks = {\n",
    "        \"null_claim_ids\": df.filter(col(\"claim_id\").isNull()).count(),\n",
    "        \"negative_charges\": df.filter(col(\"total_charge\") < 0).count(),\n",
    "        \"future_dates\": df.filter(col(\"claim_date\") > current_date()).count()\n",
    "    }\n",
    "    return checks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ Delta Lake Best Practices\n",
    "\n",
    "### 1. **Regular Maintenance**\n",
    "```sql\n",
    "-- Compact small files\n",
    "OPTIMIZE payer_gold.claims_enriched;\n",
    "\n",
    "-- Remove old versions (keep 7 days)\n",
    "VACUUM payer_gold.claims_enriched RETAIN 168 HOURS;\n",
    "\n",
    "-- Update statistics\n",
    "ANALYZE TABLE payer_gold.claims_enriched COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "### 2. **Use Time Travel for Auditing**\n",
    "```sql\n",
    "-- Query previous version\n",
    "SELECT * FROM payer_gold.claims_enriched VERSION AS OF 1;\n",
    "\n",
    "-- Query as of timestamp\n",
    "SELECT * FROM payer_gold.claims_enriched TIMESTAMP AS OF '2025-01-01';\n",
    "```\n",
    "\n",
    "### 3. **Enable Change Data Feed**\n",
    "```sql\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Best Practices\n",
    "\n",
    "### 1. **Medallion Layer Guidelines**\n",
    "- **Bronze**: Keep all source data, minimal transformation\n",
    "- **Silver**: One source system = one silver table (usually)\n",
    "- **Gold**: Many silver tables ‚Üí one gold table (join/aggregate)\n",
    "\n",
    "### 2. **Naming Conventions**\n",
    "```\n",
    "Catalog: <organization>_<environment>\n",
    "Schema: <domain>_<layer>\n",
    "Table: <entity>_<descriptor>\n",
    "\n",
    "Examples:\n",
    "- acme_prod.payer_bronze.claims_raw\n",
    "- acme_dev.payer_silver.claims_cleaned\n",
    "- acme_prod.payer_gold.member_360_view\n",
    "```\n",
    "\n",
    "### 3. **Documentation**\n",
    "```sql\n",
    "-- Add table comments\n",
    "COMMENT ON TABLE payer_gold.claims_enriched IS \n",
    "'Enriched claims with member and provider details for analytics';\n",
    "\n",
    "-- Add column comments\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "ALTER COLUMN total_charge COMMENT 'Total charged amount in USD';\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a98a2-db41-4a69-b0ec-9876ee60ccfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Databricks Healthcare Payer Analytics Workshop! Let's review what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished\n",
    "\n",
    "### 1. **Medallion Architecture**\n",
    "- ‚úÖ Built a complete **Bronze ‚Üí Silver ‚Üí Gold** pipeline\n",
    "- ‚úÖ Understood data quality improvement at each layer\n",
    "- ‚úÖ Created analytics-ready datasets\n",
    "\n",
    "### 2. **Data Engineering Skills**\n",
    "- ‚úÖ Loaded data using **COPY INTO**\n",
    "- ‚úÖ Transformed data with **SQL and PySpark**\n",
    "- ‚úÖ Applied data quality checks and validations\n",
    "- ‚úÖ Created aggregations and derived metrics\n",
    "\n",
    "### 3. **Analytics & Visualization**\n",
    "- ‚úÖ Generated business insights from data\n",
    "- ‚úÖ Created interactive visualizations\n",
    "- ‚úÖ Performed statistical analysis\n",
    "- ‚úÖ Built executive dashboards\n",
    "\n",
    "### 4. **Databricks Platform**\n",
    "- ‚úÖ Worked with **Unity Catalog**\n",
    "- ‚úÖ Used **Delta Lake** for reliable data storage\n",
    "- ‚úÖ Leveraged **AI Assistant** for code help\n",
    "- ‚úÖ Applied performance optimization techniques\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. ‚≠ê **Bookmark this notebook** for future reference\n",
    "2. üìñ Complete the hands-on exercises\n",
    "3. üîÑ Try modifying the code with your own logic\n",
    "4. üíæ Export your results and share with your team\n",
    "\n",
    "### Continue Learning\n",
    "\n",
    "#### üìö Advanced Topics to Explore\n",
    "- **Delta Live Tables (DLT)**: Declarative pipeline framework\n",
    "- **Databricks Workflows**: Orchestration and scheduling\n",
    "- **Unity Catalog**: Advanced governance features\n",
    "- **Databricks SQL**: Performance tuning and optimization\n",
    "- **Machine Learning**: MLflow and Feature Store\n",
    "- **Streaming**: Structured Streaming with Delta Lake\n",
    "\n",
    "#### üîó Helpful Resources\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training)\n",
    "- [Community Forums](https://community.databricks.com/)\n",
    "- [Databricks Blog](https://www.databricks.com/blog)\n",
    "\n",
    "#### üéØ Recommended Certifications\n",
    "- **Databricks Lakehouse Platform Fundamentals**\n",
    "- **Databricks Certified Data Engineer Associate**\n",
    "- **Databricks Certified Data Analyst Associate**\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Applications\n",
    "\n",
    "Apply these skills to:\n",
    "- üè• **Healthcare**: Claims processing, patient analytics, risk scoring\n",
    "- üè¶ **Finance**: Fraud detection, transaction analysis, risk management\n",
    "- üõí **Retail**: Customer analytics, inventory optimization, sales forecasting\n",
    "- üì± **Technology**: User behavior analysis, product metrics, A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Get Help & Share\n",
    "\n",
    "### Need Help?\n",
    "- üí¨ Ask the **Databricks AI Assistant**\n",
    "- üåê Visit [Databricks Community](https://community.databricks.com/)\n",
    "- üìß Contact your Databricks account team\n",
    "- üìñ Check [Stack Overflow](https://stackoverflow.com/questions/tagged/databricks)\n",
    "\n",
    "### Share Your Success\n",
    "- ‚≠ê Share insights with your colleagues\n",
    "- üìä Create dashboards for stakeholders\n",
    "- üé§ Present your work at team meetings\n",
    "- üèÜ Contribute to the Databricks community\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: October 7, 2025*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f241a39f-e379-43c2-bf07-a69604fe8fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ Quick Reference Guide\n",
    "\n",
    "## Common PySpark Operations\n",
    "\n",
    "### Reading Data\n",
    "```python\n",
    "# From Delta table\n",
    "df = spark.table(\"catalog.schema.table\")\n",
    "\n",
    "# From CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/file.csv\")\n",
    "\n",
    "# From JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# From Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Writing Data\n",
    "```python\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "# Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"table_name\")\n",
    "\n",
    "# With partitioning\n",
    "df.write.format(\"delta\").partitionBy(\"date_col\").saveAsTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Select columns\n",
    "df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Filter rows\n",
    "df.filter(col(\"amount\") > 100)\n",
    "df.where(\"amount > 100\")\n",
    "\n",
    "# Add new column\n",
    "df.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# Rename column\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "# Drop column\n",
    "df.drop(\"col_name\")\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    avg(\"amount\").alias(\"average\")\n",
    ")\n",
    "\n",
    "# Join tables\n",
    "df1.join(df2, \"key_column\")\n",
    "df1.join(df2, df1.key == df2.key, \"left\")\n",
    "\n",
    "# Sort\n",
    "df.orderBy(\"col_name\")\n",
    "df.orderBy(col(\"col_name\").desc())\n",
    "\n",
    "# Remove duplicates\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "### Common Functions\n",
    "```python\n",
    "# String functions\n",
    "upper(\"col_name\")\n",
    "lower(\"col_name\")\n",
    "trim(\"col_name\")\n",
    "concat(\"col1\", \"col2\")\n",
    "substring(\"col_name\", 1, 5)\n",
    "\n",
    "# Date functions\n",
    "current_date()\n",
    "current_timestamp()\n",
    "date_format(\"date_col\", \"yyyy-MM-dd\")\n",
    "year(\"date_col\")\n",
    "month(\"date_col\")\n",
    "datediff(\"date1\", \"date2\")\n",
    "\n",
    "# Math functions\n",
    "round(\"col_name\", 2)\n",
    "abs(\"col_name\")\n",
    "ceil(\"col_name\")\n",
    "floor(\"col_name\")\n",
    "\n",
    "# Conditional logic\n",
    "when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    "\n",
    "# Null handling\n",
    "col(\"col_name\").isNull()\n",
    "col(\"col_name\").isNotNull()\n",
    "coalesce(\"col1\", \"col2\", lit(0))\n",
    "```\n",
    "\n",
    "## Common SQL Operations\n",
    "\n",
    "### DDL Commands\n",
    "```sql\n",
    "-- Create database\n",
    "CREATE DATABASE IF NOT EXISTS database_name;\n",
    "\n",
    "-- Drop database\n",
    "DROP DATABASE IF EXISTS database_name CASCADE;\n",
    "\n",
    "-- Create table\n",
    "CREATE TABLE table_name (\n",
    "    id STRING,\n",
    "    amount DOUBLE,\n",
    "    date DATE\n",
    ");\n",
    "\n",
    "-- Drop table\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "-- Describe table\n",
    "DESCRIBE EXTENDED table_name;\n",
    "SHOW COLUMNS FROM table_name;\n",
    "```\n",
    "\n",
    "### DML Commands\n",
    "```sql\n",
    "-- Insert data\n",
    "INSERT INTO table_name VALUES (1, 'value1', 100);\n",
    "\n",
    "-- Update data (Delta Lake)\n",
    "UPDATE table_name SET amount = 200 WHERE id = 1;\n",
    "\n",
    "-- Delete data (Delta Lake)\n",
    "DELETE FROM table_name WHERE id = 1;\n",
    "\n",
    "-- Merge (Upsert)\n",
    "MERGE INTO target_table\n",
    "USING source_table\n",
    "ON target_table.id = source_table.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "### Query Commands\n",
    "```sql\n",
    "-- Basic SELECT\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "\n",
    "-- With WHERE clause\n",
    "SELECT * FROM table_name WHERE amount > 100;\n",
    "\n",
    "-- Aggregations\n",
    "SELECT category, COUNT(*), SUM(amount), AVG(amount)\n",
    "FROM table_name\n",
    "GROUP BY category;\n",
    "\n",
    "-- Joins\n",
    "SELECT a.*, b.name\n",
    "FROM table_a a\n",
    "INNER JOIN table_b b ON a.id = b.id;\n",
    "\n",
    "-- Window functions\n",
    "SELECT \n",
    "    *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as rank\n",
    "FROM table_name;\n",
    "\n",
    "-- CTE (Common Table Expression)\n",
    "WITH summary AS (\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM table_name\n",
    "    GROUP BY category\n",
    ")\n",
    "SELECT * FROM summary WHERE total > 1000;\n",
    "```\n",
    "\n",
    "## Databricks Utilities\n",
    "```python\n",
    "# File system operations\n",
    "dbutils.fs.ls(\"path/\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "dbutils.fs.rm(\"path/\", recurse=True)\n",
    "dbutils.fs.mkdirs(\"path/\")\n",
    "\n",
    "# Widgets (parameters)\n",
    "dbutils.widgets.text(\"param_name\", \"default_value\")\n",
    "param_value = dbutils.widgets.get(\"param_name\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, {\"param\": \"value\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this reference handy as you build your data pipelines!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7199164455271059,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBSQL_Workshop_ETL and Analytics_10072025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "catalog": {
     "currentValue": "my_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
