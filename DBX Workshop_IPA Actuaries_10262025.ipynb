{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Databricks for Actuaries: Healthcare Analytics Workshop\n",
    "**A Hands-On Guide for SAS Users Transitioning to Databricks**\n",
    "\n",
    "---\n",
    "\n",
    "## üëã Welcome Actuaries & Analysts!\n",
    "\n",
    "This workshop is designed specifically for **actuaries and analysts** who are familiar with **SAS** and want to learn Databricks. Don't worry if you're new to Python or SQL - we'll guide you step by step!\n",
    "\n",
    "## üìö Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. ‚úÖ Understand how Databricks compares to your SAS workflows\n",
    "2. ‚úÖ Load and query healthcare payer data using **simple SQL**\n",
    "3. ‚úÖ Perform actuarial analyses you're familiar with (loss ratios, trends, reserving)\n",
    "4. ‚úÖ Create interactive visualizations without complex code\n",
    "5. ‚úÖ Build analytics tables for pricing, reserving, and risk management\n",
    "\n",
    "---\n",
    "\n",
    "### üè• Dataset Overview\n",
    "\n",
    "We'll work with **healthcare payer data** including:\n",
    "- **Members**: Health plan enrollees\n",
    "- **Claims**: Medical claim submissions\n",
    "- **Providers**: Healthcare providers\n",
    "- **Diagnoses**: Diagnosis codes from claims\n",
    "- **Procedures**: Medical procedures performed\n",
    "\n",
    "**Think of it as**: Claims = Losses, Members = Policies, Providers = Service Providers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b67ad5a-ac8b-4034-8885-cf6ed972bdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìë Table of Contents\n",
    "\n",
    "### Part 1: Getting Started (Quick Setup)\n",
    "1. **[Introduction for Actuaries](#introduction)**\n",
    "   - Databricks vs SAS: What's Different?\n",
    "   - Quick Tour of the Platform\n",
    "   - Your First Query\n",
    "\n",
    "2. **[Setup (We'll Do This Together)](#setup)**\n",
    "   - Loading Your Data (Bronze & Silver - Simplified)\n",
    "   - Checking Your Tables\n",
    "\n",
    "### Part 2: Actuarial Analytics (The Fun Part! üéâ)\n",
    "\n",
    "3. **[Gold Layer - Actuarial Analytics](#gold-layer)**\n",
    "   - **Loss Ratio Analysis** ‚≠ê\n",
    "   - **Claims Development & Trending** ‚≠ê\n",
    "   - **Frequency & Severity Metrics** ‚≠ê\n",
    "   - **Risk Segmentation & Scoring** ‚≠ê\n",
    "   - **IBNR Indicators** ‚≠ê\n",
    "\n",
    "4. **[Interactive Exercises](#exercises)**\n",
    "   - üéØ Exercise 1: Calculate Loss Ratios by Segment\n",
    "   - üéØ Exercise 2: Identify High-Risk Members\n",
    "   - üéØ Exercise 3: Month-over-Month Growth Analysis\n",
    "   - üéØ Exercise 4: Provider Network Cost Drivers\n",
    "   - üîç Exercise 5: Data Quality Checks ‚≠ê NEW\n",
    "   - ‚öñÔ∏è Exercise 6: Bias Detection in Healthcare Data ‚≠ê NEW\n",
    "\n",
    "5. **[Visualizations for Actuaries](#visualizations)**\n",
    "   - Loss Ratio Charts\n",
    "   - Development Triangles\n",
    "   - Trend Lines\n",
    "   - Distribution Analysis\n",
    "\n",
    "6. **[SAS to Databricks Quick Reference](#sas-reference)**\n",
    "   - Common SAS Procedures ‚Üí SQL/PySpark\n",
    "   - PROC SQL ‚Üí Databricks SQL\n",
    "   - PROC MEANS ‚Üí Aggregations\n",
    "   - PROC FREQ ‚Üí GROUP BY\n",
    "   - Data Steps ‚Üí Transformations\n",
    "\n",
    "7. **[Next Steps](#next-steps)**\n",
    "   - Taking This Back to Your Work\n",
    "   - Resources for Actuaries\n",
    "   - Getting Help\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Workshop Flow**: We'll move quickly through setup, then spend most of our time on actuarial analytics. Feel free to ask questions anytime!\n",
    "    - Continue Learning\n",
    "    - Resources & Certifications\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Workshop Tip**: This is a hands-on workshop! Execute each cell as you go and experiment with the code. Don't hesitate to ask questions or use the Databricks AI Assistant.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe39bed2-7a8e-4992-8c17-3df323aec010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction for Actuaries\n",
    "\n",
    "## ü§î Why Databricks for Actuaries?\n",
    "\n",
    "If you're coming from **SAS**, you might be wondering: \"Why learn another tool?\"\n",
    "\n",
    "### Here's Why:\n",
    "- **Scalability**: Handle millions of claims instantly (no more waiting for PROC SQL!)\n",
    "- **Modern Analytics**: Built-in ML, real-time dashboards, and collaboration\n",
    "- **Cost-Effective**: Cloud-based, pay only for what you use\n",
    "- **Still Use SQL**: 90% of your SAS PROC SQL knowledge transfers directly!\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ SAS vs Databricks: Quick Comparison\n",
    "\n",
    "| **What You Do in SAS** | **How You Do It in Databricks** | **Difficulty** |\n",
    "|------------------------|----------------------------------|----------------|\n",
    "| `PROC SQL` | SQL queries (almost identical!) | ‚≠ê Easy |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregate functions | ‚≠ê Easy |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | ‚≠ê Easy |\n",
    "| `DATA` step | SQL `SELECT` or simple Python | ‚≠ê‚≠ê Moderate |\n",
    "| `PROC EXPAND` (trending) | Window functions | ‚≠ê‚≠ê Moderate |\n",
    "| Macros | Parameters + reusable queries | ‚≠ê‚≠ê‚≠ê Learning curve |\n",
    "\n",
    "**Good News**: Most of what you do can be done with **SQL alone**!\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Quick Concept: Medallion Architecture (Simplified)\n",
    "\n",
    "Think of it like your SAS workflow:\n",
    "\n",
    "```\n",
    "üì• BRONZE (Raw Data)        ‚Üí  Like your input datasets from source systems\n",
    "   ‚Üì\n",
    "üîß SILVER (Clean Data)      ‚Üí  Like your cleaned/standardized datasets  \n",
    "   ‚Üì\n",
    "‚≠ê GOLD (Analytics Tables)   ‚Üí  Like your final reporting/analysis datasets\n",
    "```\n",
    "\n",
    "**Today's Focus**: We'll quickly load Bronze/Silver, then spend most time on **Gold** (actuarial analytics)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üè† What is a Lakehouse? (Simple Explanation)\n",
    "\n",
    "**For Actuaries**: Think of it as a **super-powered SAS library** that:\n",
    "- Stores all your data in one place (claims, policies, members)\n",
    "- Lets you analyze it with SQL (like PROC SQL)\n",
    "- Handles millions of rows instantly\n",
    "- Keeps track of all changes (audit trail)\n",
    "- Lets multiple people work at once (no locking issues!)\n",
    "\n",
    "**Key Benefit**: Unlike SAS datasets, you can query **billions** of claims in seconds!\n",
    "\n",
    "<img src=\"https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png\" alt=\"Lakehouse\" width=\"500\" height=\"350\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìö Unity Catalog (Data Organization - Like SAS Libraries)\n",
    "\n",
    "**For Actuaries**: Think of Unity Catalog as your **SAS library structure**, but better organized:\n",
    "\n",
    "```\n",
    "In SAS:                    In Databricks:\n",
    "LIBNAME.DATASET            CATALOG.SCHEMA.TABLE\n",
    "  ‚Üì                           ‚Üì\n",
    "work.claims        ‚Üí       my_catalog.payer_bronze.claims\n",
    "actuarial.loss_ratios ‚Üí    my_catalog.payer_gold.loss_ratios\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Everyone sees the same data (no duplicate datasets!)\n",
    "- ‚úÖ Built-in security (control who can see PHI/PII)\n",
    "- ‚úÖ Audit trail (track all data access)\n",
    "- ‚úÖ Easy to find data (searchable catalog)\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086\" alt=\"Unity Catalog\" width=\"500\" height=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ü•âü•àü•á Medallion Architecture (Your Data Quality Layers)\n",
    "\n",
    "**For Actuaries**: This is like your SAS data prep workflow, but organized into layers:\n",
    "\n",
    "### üì• Bronze (Raw Data) \n",
    "- **Like**: Your raw claims extracts from source systems\n",
    "- **Contains**: Data exactly as received (CSV, database extracts)\n",
    "- **Example**: Raw claims file from claims system\n",
    "- **Today**: We'll load this quickly!\n",
    "\n",
    "### üîß Silver (Cleaned Data)\n",
    "- **Like**: Your cleaned/standardized SAS datasets\n",
    "- **Contains**: Deduplicated, standardized data\n",
    "- **Example**: Claims with proper data types, duplicates removed\n",
    "- **Today**: We'll auto-clean this!\n",
    "\n",
    "### ‚≠ê Gold (Analytics Tables)\n",
    "- **Like**: Your final analysis datasets (loss triangles, premium summaries)\n",
    "- **Contains**: Business-ready tables for actuarial analysis\n",
    "- **Example**: Loss ratios, development factors, IBNR estimates\n",
    "- **Today**: This is where we'll spend most time! üéâ\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Medallion Architecture\" width=\"500\" height=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Modeling Concepts\n",
    "\n",
    "Databricks fully supports both **dimensional modeling** (Kimball/star schema) and **Inmon-style, Data Vault architectures (hubs, satellites, links)** on the Lakehouse platform. For dimensional models, you can build classic star and snowflake schemas directly with SQL, benefiting from ACID transactions and scalable Delta Lake tables.\n",
    "\n",
    "For Inmon/Data Vault use cases, Databricks provides rich support for hub-and-satellite models that address core enterprise needs for history, auditability, and extensibility‚Äîfind end.\n",
    "\n",
    "The Lakehouse approach lets you mix these styles as needed within a single platform, so you can incrementally land data in Raw Vault/EDW structures and later expose it as dimensional marts‚Äîall with Delta Live Tables, fine-grained security, and open formats.\n",
    "\n",
    "Key blog resources:\n",
    "\n",
    "[Implementing Dimensional Modeling](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "[Implementing Data Vault/Hub-Satellite](https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html) \n",
    "\n",
    "[Data Vault Best Practices](https://www.databricks.com/blog/data-vault-best-practice-implementation-lakehouse)\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043442-a66b-47f2-9f2c-c6890d2de349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data Model\n",
    "\n",
    "For a payer, commonly used tables include:\n",
    "\n",
    "- **Members**: members enrolled in a health plan\n",
    "- **Claims**: medical claim submissions\n",
    "- **Providers**: healthcare providers (doctors, clinics)\n",
    "- **Diagnoses**: claim diagnosis codes\n",
    "- **Procedures**: procedures/services performed\n",
    "\n",
    "Each table should have at least 50 rows.\n",
    "\n",
    "<img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/bdd54dc0-f3c7-4975-80a3-0017ebdb121c.png\" alt=\"Managed Tables\" width=\"400\" height=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP\n",
    "Just run next couple of cells for setup! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"my_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3beab112-8351-412b-89dd-b7d12bed854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1346d2f0-da56-4664-9549-ca712a979960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee52692-6c62-4c15-b115-e05aca2d260f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3974a340-43ce-4065-bf06-4d899386100b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f529a21-c047-4a8c-81f1-4d6c452dee52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "üì• Bronze Layer    ‚Üí    üîß Silver Layer    ‚Üí    ‚≠ê Gold Layer    ‚Üí    üìä Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8747d067-1569-4679-a336-41dcc5479627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Set the catalog and schema\n",
    "# CREATE CATALOG IF NOT EXISTS my_catalog;\n",
    "# USE CATALOG my_catalog;\n",
    "\n",
    "# -- Create bronze schema\n",
    "# CREATE SCHEMA IF NOT EXISTS payer_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze Layer ‚Äì Ingest Raw Data\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Bronze Layer?\n",
    "\n",
    "The **Bronze Layer** is the landing zone for raw data. Here we:\n",
    "- üìÇ Load data \"as-is\" from source files (CSV, JSON, Parquet, etc.)\n",
    "- üíæ Store in Delta Lake format for ACID transactions\n",
    "- üìù Apply minimal transformation (just schema inference)\n",
    "- ‚è±Ô∏è Keep historical data for audit and reprocessing\n",
    "\n",
    "> **üí° Best Practice**: Use `COPY INTO` for incremental, idempotent loading from raw data files into Delta Lake tables. It automatically skips already-loaded files!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/my_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### üìñ Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- ‚úÖ **Idempotent**: Safely re-run without duplicating data\n",
    "- ‚úÖ **Incremental**: Only loads new files automatically\n",
    "- ‚úÖ **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- ‚úÖ **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "üìö **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load Claims Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files.\n",
    "\n",
    "\n",
    "-- Load Diagnosis Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Members Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Procedures Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "\n",
    "-- Load Providers Data into Bronze Table\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### üêç Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/my_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîß Silver Layer ‚Äì Transform, Clean, and Join\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Silver Layer?\n",
    "\n",
    "The **Silver Layer** is where we transform raw data into clean, validated, and enriched datasets. Here we:\n",
    "\n",
    "- üßπ **Clean**: Remove nulls, trim whitespace, fix data quality issues\n",
    "- üîÑ **Transform**: Cast data types, standardize formats\n",
    "- üóëÔ∏è **Deduplicate**: Remove duplicate records based on business keys\n",
    "- üîç **Validate**: Apply business rules and data quality checks\n",
    "- üìä **Enrich**: Join related tables, calculate derived columns\n",
    "\n",
    "> **üí° Best Practice**: Silver tables should be \"analytics-ready\" ‚Äì cleaned, validated, and properly typed!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cb09e1-fe4c-4c5b-bcba-fbff4eb501fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Exercise: Data Quality Checks\n",
    "\n",
    "Let's verify our Silver transformations worked correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e90cd4-2b05-41cd-9fe9-3de6a242c6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Data Quality Check 1: Check for nulls in key columns\n",
    "print(\"=== NULL CHECK ===\")\n",
    "members_df = spark.table(\"payer_silver.members\")\n",
    "null_counts = members_df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in members_df.columns\n",
    "])\n",
    "display(null_counts)\n",
    "\n",
    "# Data Quality Check 2: Check for duplicates\n",
    "print(\"\\n=== DUPLICATE CHECK ===\")\n",
    "claims_df = spark.table(\"payer_silver.claims\")\n",
    "total_rows = claims_df.count()\n",
    "distinct_rows = claims_df.select(\"claim_id\").distinct().count()\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct claim_ids: {distinct_rows}\")\n",
    "print(f\"Duplicates: {total_rows - distinct_rows}\")\n",
    "\n",
    "# Data Quality Check 3: Value range checks\n",
    "print(\"\\n=== VALUE RANGE CHECK ===\")\n",
    "claims_stats = claims_df.agg(\n",
    "    {\"total_charge\": \"min\", \"total_charge\": \"max\", \"total_charge\": \"avg\"}\n",
    ")\n",
    "display(claims_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6466f560-5e31-4b0d-b8bd-9ed70734a0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü§ñ Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"What kind of aggregations can I do with table payer_gold.claims_enriched?\"\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚≠ê Gold Layer ‚Äì Actuarial Analytics (The Fun Part!)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is the Gold Layer? (For Actuaries)\n",
    "\n",
    "**This is where YOU spend most of your time!** The Gold Layer is like your **final SAS analysis datasets** - ready for actuarial work.\n",
    "\n",
    "### What We'll Build (Actuarial Examples):\n",
    "\n",
    "1. **üìä Loss Ratios** - By state, specialty, plan (like your quarterly reports)\n",
    "2. **üìà Claims Trends** - Frequency & severity trending (PROC EXPAND style)\n",
    "3. **üî∫ Development Triangles** - Age-to-age factors (for reserving)\n",
    "4. **üí∞ Premium Adequacy** - Earned vs Incurred ratios\n",
    "5. **üé≤ Risk Scoring** - Member risk segmentation\n",
    "6. **üìâ IBNR Indicators** - Late claims reporting patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How This Compares to SAS\n",
    "\n",
    "| **Your SAS Workflow** | **In Databricks Gold Layer** |\n",
    "|----------------------|------------------------------|\n",
    "| Create final analysis dataset | Create Gold table |\n",
    "| PROC SQL with aggregations | SQL SELECT with GROUP BY |\n",
    "| PROC MEANS for summary stats | Aggregate functions (AVG, SUM, etc.) |\n",
    "| Multiple DATA steps for calcs | Single SQL statement with CTEs |\n",
    "| Macros for repeated calcs | Parameterized queries |\n",
    "| Export to Excel for viz | Built-in interactive charts! |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Your Actuarial Toolbox\n",
    "\n",
    "Today you'll learn SQL equivalents for common actuarial analyses:\n",
    "\n",
    "- **Loss Ratios**: `SUM(claims)/SUM(premium)`\n",
    "- **Trending**: Window functions (`LAG`, `LEAD`)\n",
    "- **Development Factors**: `GROUP BY` claim year + development period\n",
    "- **Percentiles**: `PERCENTILE_CONT` function\n",
    "- **Risk Scores**: `CASE WHEN` logic\n",
    "\n",
    "**Ready?** Let's start building! üöÄ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9923eb4-1dec-4ecd-923c-7a3759104c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Actuarial Example 1: Loss Ratios by Segment\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What is our loss ratio by provider specialty and state?\"**\n",
    "\n",
    "This is a **fundamental actuarial metric** - you probably calculate this quarterly or annually!\n",
    "\n",
    "### üìù SAS vs Databricks\n",
    "\n",
    "**In SAS, you might write:**\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE loss_ratios AS\n",
    "    SELECT \n",
    "        specialty,\n",
    "        state,\n",
    "        COUNT(*) as claim_count,\n",
    "        SUM(total_charge) as incurred_losses,\n",
    "        CALCULATED incurred_losses / (SELECT SUM(premium) FROM policies) as loss_ratio\n",
    "    FROM claims c\n",
    "    LEFT JOIN providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY specialty, state;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "**In Databricks, we write:**\n",
    "```sql\n",
    "-- Very similar! Most SQL transfers directly.\n",
    "```\n",
    "\n",
    "Let's build this now! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7d5a93-952d-44a7-86ac-7e1483d72e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üìä ACTUARIAL ANALYSIS: Loss Ratios by Specialty and State\n",
    "-- This calculates incurred claims / exposure (using claim count as proxy for premium)\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.loss_ratios_by_segment AS\n",
    "SELECT \n",
    "    p.specialty,\n",
    "    p.state,\n",
    "    COUNT(DISTINCT c.claim_id) AS claim_count,\n",
    "    COUNT(DISTINCT c.member_id) AS member_count,\n",
    "    SUM(c.total_charge) AS total_incurred,\n",
    "    ROUND(AVG(c.total_charge), 2) AS avg_claim_size,\n",
    "    \n",
    "    -- Loss Ratio Calculation (using member count as premium proxy)\n",
    "    -- In real life, you'd join to a premium table!\n",
    "    ROUND(SUM(c.total_charge) / COUNT(DISTINCT c.member_id), 2) AS loss_ratio_per_member,\n",
    "    \n",
    "    -- Claim Frequency (claims per member)\n",
    "    ROUND(COUNT(c.claim_id) * 1.0 / COUNT(DISTINCT c.member_id), 2) AS frequency,\n",
    "    \n",
    "    -- Severity (avg cost per claim)\n",
    "    ROUND(SUM(c.total_charge) / COUNT(c.claim_id), 2) AS severity\n",
    "    \n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "GROUP BY p.specialty, p.state\n",
    "HAVING COUNT(c.claim_id) >= 5  -- Filter out small segments\n",
    "ORDER BY total_incurred DESC;\n",
    "\n",
    "-- Display results\n",
    "SELECT * FROM payer_gold.loss_ratios_by_segment LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc30919-0b70-489a-afa7-d319aa89c642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ YOUR TURN! Exercise 1: Specialty Analysis\n",
    "\n",
    "**Task**: Modify the query above to answer: **\"Which specialty has the highest severity (cost per claim)?\"**\n",
    "\n",
    "**Hints:**\n",
    "1. You already have the `severity` column!\n",
    "2. Just change the `ORDER BY` clause\n",
    "3. Maybe add `LIMIT 10` to see top specialties\n",
    "\n",
    "**Try it below:** (Click the cell and modify the SQL)\n",
    "\n",
    "```sql\n",
    "-- YOUR CODE HERE\n",
    "-- Hint: Copy the query above and modify the ORDER BY\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output**: You should see specialties ranked by average claim cost.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee97453-ce7c-42ce-a7bb-3ca47d797784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìà Actuarial Example 2: Claims Trending Analysis\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"What are our monthly claim trends? Are claims trending up or down?\"**\n",
    "\n",
    "This is crucial for:\n",
    "- **Rate making** (applying trend factors)\n",
    "- **Reserving** (projecting ultimate losses)\n",
    "- **Budgeting** (forecasting next year's costs)\n",
    "\n",
    "### üìù What We're Calculating\n",
    "\n",
    "```\n",
    "Month-over-Month Growth = (This Month - Last Month) / Last Month\n",
    "Year-over-Year Growth = (This Month - Same Month Last Year) / Same Month Last Year\n",
    "```\n",
    "\n",
    "### üîß SAS Equivalent\n",
    "In SAS, you might use **PROC EXPAND** or **LAG functions** in a DATA step. \n",
    "\n",
    "In Databricks, we use **Window Functions** - specifically `LAG()` and `LEAD()`.\n",
    "\n",
    "Let's build it! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4681eb-8014-47c8-9257-21bdcad9fed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üìà ACTUARIAL ANALYSIS: Monthly Claims Trending\n",
    "-- Window functions for MoM and YoY calculations\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_trend_analysis AS\n",
    "WITH monthly_claims AS (\n",
    "    -- Step 1: Aggregate claims by month\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        YEAR(claim_date) AS claim_year,\n",
    "        MONTH(claim_date) AS claim_month_num,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(total_charge) AS total_incurred,\n",
    "        ROUND(AVG(total_charge), 2) AS avg_claim_cost\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month, claim_year, claim_month_num\n",
    ")\n",
    "SELECT \n",
    "    claim_month,\n",
    "    claim_count,\n",
    "    total_incurred,\n",
    "    avg_claim_cost,\n",
    "    \n",
    "    -- Month-over-Month Comparison\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY claim_month) AS prior_month_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY claim_month) * 100, 2) AS mom_growth_pct,\n",
    "    \n",
    "    -- Year-over-Year Comparison (12 months ago)\n",
    "    LAG(total_incurred, 12) OVER (ORDER BY claim_month) AS prior_year_incurred,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 12) OVER (ORDER BY claim_month)) / \n",
    "          LAG(total_incurred, 12) OVER (ORDER BY claim_month) * 100, 2) AS yoy_growth_pct,\n",
    "    \n",
    "    -- 3-Month Moving Average (for smoothing)\n",
    "    ROUND(AVG(total_incurred) OVER (ORDER BY claim_month \n",
    "                                     ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), 2) AS moving_avg_3mo\n",
    "    \n",
    "FROM monthly_claims\n",
    "ORDER BY claim_month;\n",
    "\n",
    "-- Display the results\n",
    "SELECT * FROM payer_gold.claims_trend_analysis;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327b52b8-6664-4f4d-b08d-3727a57c49b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üî∫ Actuarial Example 3: Claims Development Triangle\n",
    "\n",
    "### üéØ Business Question\n",
    "**\"How do claims develop over time? What are our age-to-age factors?\"**\n",
    "\n",
    "This is **THE fundamental tool** for actuarial reserving!\n",
    "\n",
    "### üìù What We're Building\n",
    "\n",
    "A **development triangle** shows:\n",
    "- **Rows**: Accident/Policy Year\n",
    "- **Columns**: Development Period (months since occurrence)\n",
    "- **Values**: Cumulative claims at each development point\n",
    "\n",
    "Then we calculate:\n",
    "- **Age-to-Age Factors** (e.g., 12-to-24 month factor)\n",
    "- **Ultimate Loss Projections**\n",
    "\n",
    "### üîß Why This Matters\n",
    "- **IBNR Reserves**: Estimate unreported claims\n",
    "- **Case Reserve Adequacy**: Check if reserves are sufficient\n",
    "- **Rate Adequacy**: Are our prices sufficient?\n",
    "\n",
    "Let's build a simple development view! üëá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee4375b-8f59-47be-ba8b-329520778bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- üî∫ ACTUARIAL ANALYSIS: Claims Development Pattern\n",
    "-- Shows how claims emerge over time (by months since occurrence)\n",
    "\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_development AS\n",
    "WITH claim_development AS (\n",
    "    SELECT \n",
    "        c.claim_id,\n",
    "        DATE_TRUNC('YEAR', c.claim_date) AS accident_year,\n",
    "        c.claim_date,\n",
    "        c.total_charge,\n",
    "        m.effective_date AS member_effective_date,\n",
    "        \n",
    "        -- Development period in months (months between member effective date and claim date)\n",
    "        -- This simulates months since policy inception\n",
    "        DATEDIFF(MONTH, m.effective_date, c.claim_date) AS development_months,\n",
    "        \n",
    "        -- Group into development periods (quarterly for simplicity)\n",
    "        CASE \n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 3 THEN '0-3 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 6 THEN '4-6 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 12 THEN '7-12 months'\n",
    "            WHEN DATEDIFF(MONTH, m.effective_date, c.claim_date) <= 24 THEN '13-24 months'\n",
    "            ELSE '24+ months'\n",
    "        END AS development_period\n",
    "        \n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "    WHERE c.claim_date >= m.effective_date  -- Claims after member enrollment\n",
    ")\n",
    "SELECT \n",
    "    accident_year,\n",
    "    development_period,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS cumulative_incurred,\n",
    "    ROUND(AVG(total_charge), 2) AS avg_claim_size,\n",
    "    \n",
    "    -- Calculate % of claims reported in each period\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY accident_year), 2) AS pct_of_total_claims\n",
    "    \n",
    "FROM claim_development\n",
    "GROUP BY accident_year, development_period\n",
    "ORDER BY accident_year, development_period;\n",
    "\n",
    "-- Display the triangle\n",
    "SELECT * FROM payer_gold.claims_development;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13d12f5-3034-45a6-b62a-9c4b6a965065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ YOUR TURN! Exercise 2: Development Analysis\n",
    "\n",
    "**Task**: Calculate the age-to-age development factor from 0-3 months to 4-6 months.\n",
    "\n",
    "**What You Need:**\n",
    "1. Sum of cumulative incurred for \"0-3 months\"\n",
    "2. Sum of cumulative incurred for \"4-6 months\"  \n",
    "3. Factor = (4-6 months) / (0-3 months)\n",
    "\n",
    "**Hint**: Use the table we just created!\n",
    "\n",
    "```sql\n",
    "-- YOUR CODE HERE\n",
    "SELECT \n",
    "    '0-3 to 4-6 month factor' AS factor_name,\n",
    "    -- Calculate the ratio here\n",
    "FROM payer_gold.claims_development\n",
    "WHERE development_period IN ('0-3 months', '4-6 months');\n",
    "```\n",
    "\n",
    "**Why This Matters**: Development factors help you project ultimate losses!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e918f47-3a29-4706-ae9e-90d4b0892911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ SAS to Databricks Quick Reference for Actuaries\n",
    "\n",
    "This section shows you **side-by-side comparisons** of common SAS code and Databricks equivalents.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Basic Data Aggregation\n",
    "\n",
    "### SAS: PROC MEANS\n",
    "```sas\n",
    "PROC MEANS DATA=claims NOPRINT;\n",
    "    CLASS specialty;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=summary \n",
    "        N=claim_count \n",
    "        SUM=total_incurred \n",
    "        MEAN=avg_claim;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    COUNT(*) AS claim_count,\n",
    "    SUM(total_charge) AS total_incurred,\n",
    "    AVG(total_charge) AS avg_claim\n",
    "FROM claims\n",
    "GROUP BY specialty;\n",
    "```\n",
    "\n",
    "**üéØ Key Difference**: In Databricks, it's all in one SELECT statement!\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Frequency Tables\n",
    "\n",
    "### SAS: PROC FREQ\n",
    "```sas\n",
    "PROC FREQ DATA=claims;\n",
    "    TABLES specialty * state / NOCOL NOROW;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: GROUP BY with COUNT\n",
    "```sql\n",
    "SELECT \n",
    "    specialty,\n",
    "    state,\n",
    "    COUNT(*) AS frequency\n",
    "FROM claims\n",
    "GROUP BY specialty, state\n",
    "ORDER BY frequency DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Conditional Logic\n",
    "\n",
    "### SAS: DATA Step with IF-THEN\n",
    "```sas\n",
    "DATA claims_categorized;\n",
    "    SET claims;\n",
    "    IF total_charge < 1000 THEN risk_category = 'Low';\n",
    "    ELSE IF total_charge < 5000 THEN risk_category = 'Medium';\n",
    "    ELSE risk_category = 'High';\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: CASE WHEN\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN total_charge < 1000 THEN 'Low'\n",
    "        WHEN total_charge < 5000 THEN 'Medium'\n",
    "        ELSE 'High'\n",
    "    END AS risk_category\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Joining Tables\n",
    "\n",
    "### SAS: PROC SQL Join\n",
    "```sas\n",
    "PROC SQL;\n",
    "    CREATE TABLE enriched_claims AS\n",
    "    SELECT c.*, p.specialty, p.provider_name\n",
    "    FROM claims c\n",
    "    LEFT JOIN providers p \n",
    "        ON c.provider_id = p.provider_id;\n",
    "QUIT;\n",
    "```\n",
    "\n",
    "### Databricks: SQL Join (Identical!)\n",
    "```sql\n",
    "SELECT c.*, p.specialty, p.provider_name\n",
    "FROM claims c\n",
    "LEFT JOIN providers p \n",
    "    ON c.provider_id = p.provider_id;\n",
    "```\n",
    "\n",
    "**üéØ Great News**: If you know SAS PROC SQL, you already know Databricks SQL!\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Lagging and Leading Values (Trending)\n",
    "\n",
    "### SAS: LAG Function\n",
    "```sas\n",
    "DATA trends;\n",
    "    SET monthly_data;\n",
    "    prior_month = LAG(total_incurred);\n",
    "    growth_pct = (total_incurred - prior_month) / prior_month * 100;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: LAG Window Function\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    LAG(total_incurred, 1) OVER (ORDER BY month) AS prior_month,\n",
    "    ROUND((total_incurred - LAG(total_incurred, 1) OVER (ORDER BY month)) / \n",
    "          LAG(total_incurred, 1) OVER (ORDER BY month) * 100, 2) AS growth_pct\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Percentiles and Quantiles\n",
    "\n",
    "### SAS: PROC UNIVARIATE\n",
    "```sas\n",
    "PROC UNIVARIATE DATA=claims;\n",
    "    VAR total_charge;\n",
    "    OUTPUT OUT=percentiles \n",
    "        PCTLPTS=25 50 75 90 95 99\n",
    "        PCTLPRE=P;\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: PERCENTILE_CONT\n",
    "```sql\n",
    "SELECT \n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_charge) AS P25,\n",
    "    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_charge) AS P50,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_charge) AS P75,\n",
    "    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY total_charge) AS P90,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_charge) AS P95,\n",
    "    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY total_charge) AS P99\n",
    "FROM claims;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Moving Averages (Smoothing)\n",
    "\n",
    "### SAS: Rolling Average\n",
    "```sas\n",
    "DATA moving_avg;\n",
    "    SET monthly_data;\n",
    "    avg_3mo = MEAN(total_incurred, LAG(total_incurred), LAG2(total_incurred));\n",
    "RUN;\n",
    "```\n",
    "\n",
    "### Databricks: Window Function with ROWS\n",
    "```sql\n",
    "SELECT \n",
    "    *,\n",
    "    AVG(total_incurred) OVER (\n",
    "        ORDER BY month \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) AS avg_3mo\n",
    "FROM monthly_data;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Quick Translation Guide\n",
    "\n",
    "| **SAS** | **Databricks** | **Notes** |\n",
    "|---------|----------------|-----------|\n",
    "| `PROC SQL` | SQL queries | Almost identical! |\n",
    "| `PROC MEANS` | `GROUP BY` + aggregations | Very similar |\n",
    "| `PROC FREQ` | `GROUP BY` + `COUNT()` | Same logic |\n",
    "| `DATA` step | `SELECT` with transformations | Different syntax, same result |\n",
    "| `LAG()` | `LAG() OVER (ORDER BY)` | Window function needed |\n",
    "| `RETAIN` | Window functions | Use cumulative sums |\n",
    "| `MERGE` | `JOIN` | SQL joins |\n",
    "| `WHERE` | `WHERE` | Identical! |\n",
    "| `IF-THEN-ELSE` | `CASE WHEN` | Different syntax |\n",
    "| Macros | Widgets + parameters | Similar concept |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips for SAS Users\n",
    "\n",
    "1. **PROC SQL knowledge transfers 90%**: If you're comfortable with SAS PROC SQL, you'll pick up Databricks quickly!\n",
    "\n",
    "2. **Window functions = LAG/LEAD on steroids**: More powerful than SAS LAG functions.\n",
    "\n",
    "3. **No DATA step needed**: Most transformations can be done in SQL with `CASE WHEN`.\n",
    "\n",
    "4. **CTEs are your friend**: Use `WITH` clauses instead of creating intermediate datasets.\n",
    "\n",
    "5. **Display > PROC PRINT**: Just use `display()` in Python cells or `SELECT` in SQL.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27243b6a-b8c1-4400-93f3-88476753529f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Exercise 5: Data Quality Checks for Actuarial Analysis\n",
    "\n",
    "**Objective**: Identify data quality issues that could impact your actuarial analysis.\n",
    "\n",
    "As actuaries, you know that **garbage in = garbage out**. Before any analysis, you must check your data quality!\n",
    "\n",
    "### üéØ Common Data Quality Issues in Healthcare:\n",
    "1. **Completeness**: Missing critical fields (claim amounts, dates, member IDs)\n",
    "2. **Accuracy**: Negative claim amounts, future dates, invalid codes\n",
    "3. **Consistency**: Duplicate claims, conflicting information\n",
    "4. **Timeliness**: Lag in claim reporting\n",
    "\n",
    "### üìù What to Check:\n",
    "\n",
    "**Part A: Completeness Check**\n",
    "```sql\n",
    "-- YOUR TURN: Find records with missing critical data\n",
    "SELECT \n",
    "    'Missing claim_id' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE claim_id IS NULL\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Missing total_charge' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge IS NULL\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Missing member_id' AS issue,\n",
    "    COUNT(*) AS record_count\n",
    "FROM payer_silver.claims\n",
    "WHERE member_id IS NULL;\n",
    "```\n",
    "\n",
    "**Part B: Accuracy Check**\n",
    "```sql\n",
    "-- YOUR TURN: Find data accuracy issues\n",
    "SELECT \n",
    "    'Negative charges' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    SUM(total_charge) AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge < 0\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Future claim dates' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    NULL AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE claim_date > CURRENT_DATE()\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Zero dollar claims' AS issue,\n",
    "    COUNT(*) AS record_count,\n",
    "    0 AS total_amount\n",
    "FROM payer_silver.claims\n",
    "WHERE total_charge = 0;\n",
    "```\n",
    "\n",
    "**Part C: Duplicate Check**\n",
    "```sql\n",
    "-- YOUR TURN: Find duplicate claims (same claim_id appearing multiple times)\n",
    "SELECT \n",
    "    claim_id,\n",
    "    COUNT(*) AS duplicate_count,\n",
    "    SUM(total_charge) AS total_duplicate_amount\n",
    "FROM payer_silver.claims\n",
    "GROUP BY claim_id\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY duplicate_count DESC;\n",
    "```\n",
    "\n",
    "### üéØ YOUR CHALLENGE:\n",
    "Write a **comprehensive data quality report** that shows:\n",
    "1. % of records with complete data\n",
    "2. % of records with data quality issues\n",
    "3. Financial impact of quality issues (e.g., sum of negative charges)\n",
    "\n",
    "**Starter Code:**\n",
    "```sql\n",
    "-- Calculate data quality metrics\n",
    "WITH total_records AS (\n",
    "    SELECT COUNT(*) AS total_count FROM payer_silver.claims\n",
    "),\n",
    "quality_issues AS (\n",
    "    SELECT \n",
    "        SUM(CASE WHEN claim_id IS NULL THEN 1 ELSE 0 END) AS missing_claim_id,\n",
    "        SUM(CASE WHEN total_charge IS NULL THEN 1 ELSE 0 END) AS missing_amount,\n",
    "        SUM(CASE WHEN total_charge < 0 THEN 1 ELSE 0 END) AS negative_amounts,\n",
    "        SUM(CASE WHEN claim_date > CURRENT_DATE() THEN 1 ELSE 0 END) AS future_dates,\n",
    "        -- YOUR TURN: Add more checks here\n",
    "        SUM(CASE WHEN _____ THEN 1 ELSE 0 END) AS _____\n",
    "    FROM payer_silver.claims\n",
    ")\n",
    "SELECT \n",
    "    t.total_count,\n",
    "    q.*,\n",
    "    -- YOUR TURN: Calculate percentages\n",
    "    ROUND(q.missing_claim_id * 100.0 / t.total_count, 2) AS pct_missing_id\n",
    "FROM total_records t, quality_issues q;\n",
    "```\n",
    "\n",
    "### üí° Actuarial Insight:\n",
    "Data quality issues can significantly impact:\n",
    "- **Loss ratio calculations** (missing claims = underestimated losses)\n",
    "- **Trend analysis** (inconsistent reporting lags)\n",
    "- **IBNR estimates** (late-reported claims)\n",
    "- **Rate filings** (regulators scrutinize data quality!)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed176b2-a929-4979-829b-455fdc9348dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öñÔ∏è Exercise 6: Bias Detection in Healthcare Data\n",
    "\n",
    "**Objective**: Detect potential biases in your data that could lead to unfair pricing or discriminatory practices.\n",
    "\n",
    "As actuaries, you have an **ethical and regulatory obligation** to ensure your analyses are fair and unbiased. This is especially critical in healthcare!\n",
    "\n",
    "### üéØ Types of Bias to Watch For:\n",
    "\n",
    "1. **Selection Bias**: Are certain populations underrepresented?\n",
    "2. **Geographic Bias**: Do certain regions have systematically different patterns?\n",
    "3. **Temporal Bias**: Has data collection changed over time?\n",
    "4. **Provider Bias**: Do certain providers have unusual claim patterns?\n",
    "5. **Demographic Bias**: Are outcomes correlated with protected characteristics?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 6A: Demographic Representation Bias\n",
    "\n",
    "**Check**: Are all demographic groups adequately represented?\n",
    "\n",
    "```sql\n",
    "-- Analyze member distribution by demographics\n",
    "SELECT \n",
    "    gender,\n",
    "    CASE \n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 18 THEN 'Under 18'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 35 THEN '18-34'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 50 THEN '35-49'\n",
    "        WHEN YEAR(CURRENT_DATE()) - YEAR(birth_date) < 65 THEN '50-64'\n",
    "        ELSE '65+'\n",
    "    END AS age_group,\n",
    "    COUNT(*) AS member_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS pct_of_total\n",
    "FROM payer_silver.members\n",
    "GROUP BY gender, age_group\n",
    "ORDER BY gender, age_group;\n",
    "```\n",
    "\n",
    "**What to Look For:**\n",
    "- Are any groups severely underrepresented (< 2% of population)?\n",
    "- Are certain age/gender combinations missing?\n",
    "- Could this skew your loss ratio analysis?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 6B: Geographic Bias Detection\n",
    "\n",
    "**Check**: Are certain states or cities systematically excluded or overrepresented?\n",
    "\n",
    "```sql\n",
    "-- YOUR TURN: Analyze geographic distribution\n",
    "WITH state_stats AS (\n",
    "    SELECT \n",
    "        p.state,\n",
    "        COUNT(DISTINCT c.claim_id) AS claim_count,\n",
    "        COUNT(DISTINCT c.member_id) AS member_count,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost,\n",
    "        ROUND(SUM(c.total_charge), 2) AS total_incurred\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.state\n",
    "),\n",
    "overall_avg AS (\n",
    "    SELECT AVG(avg_claim_cost) AS overall_avg_cost\n",
    "    FROM state_stats\n",
    ")\n",
    "SELECT \n",
    "    s.state,\n",
    "    s.claim_count,\n",
    "    s.member_count,\n",
    "    s.avg_claim_cost,\n",
    "    o.overall_avg_cost,\n",
    "    -- Deviation from average\n",
    "    ROUND((s.avg_claim_cost - o.overall_avg_cost) / o.overall_avg_cost * 100, 2) AS pct_deviation_from_avg,\n",
    "    -- Flag for significant deviation\n",
    "    CASE \n",
    "        WHEN ABS((s.avg_claim_cost - o.overall_avg_cost) / o.overall_avg_cost) > 0.25 \n",
    "        THEN '‚ö†Ô∏è HIGH DEVIATION'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS bias_flag\n",
    "FROM state_stats s, overall_avg o\n",
    "ORDER BY ABS(s.avg_claim_cost - o.overall_avg_cost) DESC;\n",
    "```\n",
    "\n",
    "**Actuarial Questions:**\n",
    "- Are high-cost states being excluded from your analysis?\n",
    "- Could this bias your rate setting?\n",
    "- Should you stratify by geography?\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 6C: Temporal Bias (Claims Reporting Lag)\n",
    "\n",
    "**Check**: Has claim reporting behavior changed over time?\n",
    "\n",
    "```sql\n",
    "-- Detect if recent months have unusual patterns (incomplete data?)\n",
    "WITH monthly_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        COUNT(*) AS claim_count,\n",
    "        ROUND(AVG(total_charge), 2) AS avg_claim_size,\n",
    "        COUNT(DISTINCT member_id) AS unique_members\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month\n",
    "),\n",
    "stats AS (\n",
    "    SELECT \n",
    "        AVG(claim_count) AS avg_monthly_claims,\n",
    "        STDDEV(claim_count) AS stddev_claims\n",
    "    FROM monthly_metrics\n",
    "    WHERE claim_month < DATE_TRUNC('MONTH', ADD_MONTHS(CURRENT_DATE(), -1))  -- Exclude current month\n",
    ")\n",
    "SELECT \n",
    "    m.claim_month,\n",
    "    m.claim_count,\n",
    "    m.avg_claim_size,\n",
    "    s.avg_monthly_claims,\n",
    "    -- Flag months with unusually low counts (potential incomplete data)\n",
    "    CASE \n",
    "        WHEN m.claim_count < (s.avg_monthly_claims - 2 * s.stddev_claims) \n",
    "        THEN '‚ö†Ô∏è UNUSUALLY LOW - POTENTIAL BIAS'\n",
    "        WHEN m.claim_count > (s.avg_monthly_claims + 2 * s.stddev_claims)\n",
    "        THEN '‚ö†Ô∏è UNUSUALLY HIGH - INVESTIGATE'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS completeness_flag\n",
    "FROM monthly_metrics m, stats s\n",
    "ORDER BY m.claim_month DESC\n",
    "LIMIT 12;  -- Last 12 months\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- Recent months may have incomplete data (IBNR!)\n",
    "- Including incomplete months will **understate** your loss ratios\n",
    "- Critical for reserving and trend analysis\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exercise 6D: Provider Bias Detection\n",
    "\n",
    "**Check**: Are certain providers outliers? Could this indicate fraud, coding issues, or data errors?\n",
    "\n",
    "```sql\n",
    "-- YOUR TURN: Identify provider outliers\n",
    "WITH provider_metrics AS (\n",
    "    SELECT \n",
    "        p.provider_id,\n",
    "        p.provider_name,\n",
    "        p.specialty,\n",
    "        COUNT(c.claim_id) AS claim_count,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost,\n",
    "        ROUND(SUM(c.total_charge), 2) AS total_billed\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.provider_id, p.provider_name, p.specialty\n",
    "    HAVING COUNT(c.claim_id) >= 10  -- Minimum credibility\n",
    "),\n",
    "specialty_benchmarks AS (\n",
    "    SELECT \n",
    "        specialty,\n",
    "        AVG(avg_claim_cost) AS specialty_avg,\n",
    "        STDDEV(avg_claim_cost) AS specialty_stddev\n",
    "    FROM provider_metrics\n",
    "    GROUP BY specialty\n",
    ")\n",
    "SELECT \n",
    "    pm.provider_id,\n",
    "    pm.provider_name,\n",
    "    pm.specialty,\n",
    "    pm.claim_count,\n",
    "    pm.avg_claim_cost,\n",
    "    sb.specialty_avg,\n",
    "    -- Calculate Z-score (standard deviations from mean)\n",
    "    ROUND((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0), 2) AS z_score,\n",
    "    -- Flag outliers\n",
    "    CASE \n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) > 3 \n",
    "        THEN 'üö® EXTREME HIGH - INVESTIGATE'\n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) > 2 \n",
    "        THEN '‚ö†Ô∏è HIGH OUTLIER'\n",
    "        WHEN (pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0) < -2 \n",
    "        THEN '‚ö†Ô∏è LOW OUTLIER'\n",
    "        ELSE '‚úÖ Normal'\n",
    "    END AS outlier_flag\n",
    "FROM provider_metrics pm\n",
    "INNER JOIN specialty_benchmarks sb ON pm.specialty = sb.specialty\n",
    "WHERE ABS((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0)) > 2  -- Only show outliers\n",
    "ORDER BY ABS((pm.avg_claim_cost - sb.specialty_avg) / NULLIF(sb.specialty_stddev, 0)) DESC;\n",
    "```\n",
    "\n",
    "**Actuarial Actions:**\n",
    "- Investigate extreme outliers (fraud? coding errors?)\n",
    "- Consider excluding outliers from benchmarks\n",
    "- Document your methodology for rate filings\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ YOUR CHALLENGE: Comprehensive Bias Report\n",
    "\n",
    "Create a single query that produces a **Bias Detection Dashboard**:\n",
    "\n",
    "```sql\n",
    "-- Create a bias summary report\n",
    "SELECT \n",
    "    'BIAS CHECK' AS metric_category,\n",
    "    'Gender Distribution' AS metric_name,\n",
    "    CASE \n",
    "        WHEN MAX(gender_pct) - MIN(gender_pct) > 30 THEN '‚ö†Ô∏è HIGH IMBALANCE'\n",
    "        ELSE '‚úÖ Balanced'\n",
    "    END AS status\n",
    "FROM (\n",
    "    SELECT \n",
    "        gender,\n",
    "        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS gender_pct\n",
    "    FROM payer_silver.members\n",
    "    GROUP BY gender\n",
    ")\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'DATA QUALITY',\n",
    "    'Completeness Check',\n",
    "    -- YOUR TURN: Add your data quality checks here\n",
    "    \n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'TEMPORAL BIAS',\n",
    "    'Recent Month Completeness',\n",
    "    -- YOUR TURN: Check for recent month bias\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Actuarial Ethics & Compliance:\n",
    "\n",
    "**Why This Matters:**\n",
    "1. **Regulatory Compliance**: ACA prohibits discrimination based on protected characteristics\n",
    "2. **Actuarial Standards of Practice (ASOP)**: Require documentation of data quality and potential biases\n",
    "3. **Rate Filing Requirements**: Regulators will question biased or incomplete data\n",
    "4. **Professional Ethics**: Actuaries must ensure fairness in pricing and risk selection\n",
    "\n",
    "**Best Practices:**\n",
    "- ‚úÖ Document all data limitations and potential biases\n",
    "- ‚úÖ Stratify analysis by key demographics to detect disparities\n",
    "- ‚úÖ Use credibility weighting for small segments\n",
    "- ‚úÖ Clearly communicate assumptions and limitations to stakeholders\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcfd424a-5c99-444d-a3a6-e63c834f2ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéØ Interactive Exercises - Test Your Skills!\n",
    "\n",
    "Now it's time to practice! These exercises cover common actuarial analyses.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Calculate Loss Ratios by Plan Type\n",
    "\n",
    "**Objective**: Find which plan has the worst loss ratio (highest incurred per member).\n",
    "\n",
    "**What to Calculate**:\n",
    "- Claims count by plan_id\n",
    "- Total incurred by plan_id\n",
    "- Average claim per member by plan_id\n",
    "- Sort by worst loss ratio first\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "-- YOUR TURN! Complete this query\n",
    "SELECT \n",
    "    m.plan_id,\n",
    "    COUNT(c.claim_id) AS _____,\n",
    "    SUM(c.total_charge) AS _____,\n",
    "    COUNT(DISTINCT m.member_id) AS member_count,\n",
    "    ROUND(SUM(c.total_charge) / COUNT(DISTINCT m.member_id), 2) AS loss_ratio_per_member\n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "GROUP BY _____\n",
    "ORDER BY _____ DESC;\n",
    "```\n",
    "\n",
    "**Hints**:\n",
    "- Fill in the blanks (_____)\n",
    "- Think about what metrics you need\n",
    "- How should you order the results?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Identify High-Risk Members\n",
    "\n",
    "**Objective**: Find members with unusually high claims (potential high-risk individuals).\n",
    "\n",
    "**Criteria**:\n",
    "- Members with total claims > 95th percentile\n",
    "- Include member demographics\n",
    "- Calculate their total incurred and claim count\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "-- Step 1: Find the 95th percentile threshold\n",
    "WITH percentile_threshold AS (\n",
    "    SELECT PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_charge) AS p95\n",
    "    FROM payer_silver.claims\n",
    "),\n",
    "-- Step 2: Identify high-cost members\n",
    "high_cost_members AS (\n",
    "    SELECT \n",
    "        member_id,\n",
    "        COUNT(*) AS claim_count,\n",
    "        SUM(total_charge) AS total_incurred\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY member_id\n",
    "    HAVING SUM(total_charge) > (SELECT p95 FROM percentile_threshold)\n",
    ")\n",
    "-- Step 3: YOUR TURN - Join with member details and display results\n",
    "SELECT \n",
    "    -- Add your columns here\n",
    "FROM high_cost_members h\n",
    "INNER JOIN payer_silver.members m ON h.member_id = m.member_id\n",
    "ORDER BY total_incurred DESC;\n",
    "```\n",
    "\n",
    "**What This Tells You**: These are your high-risk members who need care management!\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Month-over-Month Growth Rate\n",
    "\n",
    "**Objective**: Calculate which month had the highest growth in claims.\n",
    "\n",
    "**What to Find**:\n",
    "- Monthly total incurred\n",
    "- Prior month incurred (use LAG)\n",
    "- Month-over-month growth %\n",
    "- Which month had highest growth?\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "WITH monthly_totals AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', claim_date) AS claim_month,\n",
    "        SUM(total_charge) AS monthly_incurred\n",
    "    FROM payer_silver.claims\n",
    "    GROUP BY claim_month\n",
    ")\n",
    "SELECT \n",
    "    claim_month,\n",
    "    monthly_incurred,\n",
    "    -- YOUR TURN: Add LAG function to get prior month\n",
    "    LAG(monthly_incurred, ___) OVER (ORDER BY ___) AS prior_month,\n",
    "    -- YOUR TURN: Calculate growth percentage\n",
    "    ROUND((monthly_incurred - LAG(_____, ___) OVER (ORDER BY _____)) / \n",
    "          LAG(_____, ___) OVER (ORDER BY _____) * 100, 2) AS mom_growth_pct\n",
    "FROM monthly_totals\n",
    "ORDER BY mom_growth_pct DESC  -- Highest growth first\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Actuarial Insight**: Sudden spikes might indicate unusual events (epidemics, policy changes, fraud).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: Provider Network Analysis\n",
    "\n",
    "**Objective**: Which providers/specialties drive the most costs?\n",
    "\n",
    "**Analysis Steps**:\n",
    "1. Group by provider specialty\n",
    "2. Calculate total incurred, claim count, avg claim size\n",
    "3. Calculate % of total costs\n",
    "4. Identify top cost drivers\n",
    "\n",
    "**Starter Code**:\n",
    "```sql\n",
    "WITH specialty_costs AS (\n",
    "    SELECT \n",
    "        p.specialty,\n",
    "        COUNT(c.claim_id) AS claim_count,\n",
    "        SUM(c.total_charge) AS total_incurred,\n",
    "        ROUND(AVG(c.total_charge), 2) AS avg_claim_cost\n",
    "    FROM payer_silver.claims c\n",
    "    INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id\n",
    "    GROUP BY p.specialty\n",
    ")\n",
    "SELECT \n",
    "    specialty,\n",
    "    claim_count,\n",
    "    total_incurred,\n",
    "    avg_claim_cost,\n",
    "    -- YOUR TURN: Calculate % of total costs\n",
    "    ROUND(total_incurred * 100.0 / SUM(total_incurred) OVER (), 2) AS pct_of_total_cost\n",
    "FROM specialty_costs\n",
    "ORDER BY total_incurred DESC;\n",
    "```\n",
    "\n",
    "**Why This Matters**: Understanding cost drivers helps with:\n",
    "- Network contracting\n",
    "- Provider steerage\n",
    "- Benefit design\n",
    "- Rate setting\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Challenge Exercise: Build Your Own Analysis!\n",
    "\n",
    "Pick one of these actuarial questions and build the SQL to answer it:\n",
    "\n",
    "1. **Geographic Analysis**: Which states have the best/worst loss ratios?\n",
    "2. **Gender Analysis**: Do claims patterns differ by gender?\n",
    "3. **Age Analysis**: How do claims trend with member age? (Use birth_date to calculate age)\n",
    "4. **Seasonality**: Are there seasonal patterns in claims? (Monthly analysis)\n",
    "5. **Large Claims**: Identify claims above $10,000 - what specialties are they from?\n",
    "\n",
    "**Your Code Here**:\n",
    "```sql\n",
    "-- Pick a question and write your analysis!\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ee3f51-2d4f-4127-94e9-54d034af7144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa71c33b-47d1-4b46-9940-2fdfd3cfb439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae305e5-7f23-43f9-83a3-6ebe62a4d4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéì Workshop Summary - You Did It!\n",
    "\n",
    "## üéâ Congratulations, Actuaries!\n",
    "\n",
    "You've just completed your first Databricks workshop! Let's review what you learned.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Accomplished Today\n",
    "\n",
    "### 1. **Transitioned from SAS to Databricks** üîÑ\n",
    "- Learned how your SAS knowledge transfers\n",
    "- Ran SQL queries (very similar to PROC SQL!)\n",
    "- Used window functions (LAG, LEAD for trending)\n",
    "\n",
    "### 2. **Built Actuarial Analytics** üìä\n",
    "- ‚úÖ **Loss Ratios** by segment\n",
    "- ‚úÖ **Claims Trending** (month-over-month growth)\n",
    "- ‚úÖ **Development Patterns** (claims emergence)\n",
    "- ‚úÖ **Risk Segmentation** (high-cost members)\n",
    "- ‚úÖ **Provider Analysis** (cost drivers)\n",
    "\n",
    "### 3. **Learned Key SQL Techniques** üíª\n",
    "- `GROUP BY` for aggregations (like PROC MEANS)\n",
    "- `JOIN` for combining tables\n",
    "- `CASE WHEN` for conditional logic (like IF-THEN)\n",
    "- `LAG/LEAD` for trending (like SAS LAG functions)\n",
    "- `PERCENTILE_CONT` for quantiles (like PROC UNIVARIATE)\n",
    "- `WINDOW FUNCTIONS` for running calculations\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How to Use This at Work\n",
    "\n",
    "### Immediate Applications:\n",
    "\n",
    "1. **Quarterly Loss Ratio Reports**\n",
    "   - Use the loss ratio queries we built\n",
    "   - Group by state, specialty, plan type\n",
    "   - Export to dashboards (no more Excel!)\n",
    "\n",
    "2. **Monthly Trending Analysis**\n",
    "   - Monitor claims frequency and severity trends\n",
    "   - Identify unusual spikes early\n",
    "   - Feed into your pricing models\n",
    "\n",
    "3. **Reserving Support**\n",
    "   - Build development triangles\n",
    "   - Calculate age-to-age factors\n",
    "   - Track IBNR emergence patterns\n",
    "\n",
    "4. **Risk Management**\n",
    "   - Identify high-risk members\n",
    "   - Segment populations for care management\n",
    "   - Calculate risk scores\n",
    "\n",
    "5. **Rate Filings**\n",
    "   - Trend historical claims\n",
    "   - Support rate change justifications\n",
    "   - Build exhibits for regulators\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üÜò Getting Help\n",
    "\n",
    "### When You're Stuck:\n",
    "\n",
    "1. **Use the AI Assistant** ü§ñ\n",
    "   - Click the AI icon in any cell\n",
    "   - Ask: \"How do I calculate a loss ratio?\"\n",
    "   - Ask: \"Convert this SAS code to SQL\"\n",
    "\n",
    "2. **Databricks Documentation**\n",
    "   - [SQL Reference](https://docs.databricks.com/sql/language-manual/)\n",
    "   - [Window Functions](https://docs.databricks.com/sql/language-manual/sql-ref-window-functions.html)\n",
    "   - [Date Functions](https://docs.databricks.com/sql/language-manual/sql-ref-datetime-functions.html)\n",
    "\n",
    "3. **Community Resources**\n",
    "   - [Databricks Community Forums](https://community.databricks.com/)\n",
    "   - [Stack Overflow - Databricks Tag](https://stackoverflow.com/questions/tagged/databricks)\n",
    "   - Internal company Databricks experts\n",
    "\n",
    "4. **Your Colleagues**\n",
    "   - Share this notebook with other actuaries!\n",
    "   - Form a study group\n",
    "   - Practice together\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "### 1. **You Already Know More Than You Think!**\n",
    "If you know SAS PROC SQL, you know 90% of Databricks SQL. The syntax is almost identical!\n",
    "\n",
    "### 2. **Start Simple**\n",
    "Don't try to learn everything at once. Start with:\n",
    "- Basic `SELECT` queries\n",
    "- Simple aggregations (`GROUP BY`)\n",
    "- Joins\n",
    "\n",
    "Then gradually add:\n",
    "- Window functions\n",
    "- CTEs (WITH clauses)\n",
    "- Advanced analytics\n",
    "\n",
    "### 3. **SQL is Enough for Most Actuarial Work**\n",
    "You don't need to learn Python/PySpark right away. Most actuarial analyses can be done with SQL alone!\n",
    "\n",
    "### 4. **Iterate and Improve**\n",
    "Your first queries won't be perfect. That's okay! \n",
    "- Start with something that works\n",
    "- Refine it over time\n",
    "- Ask for feedback\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop!\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: October 26, 2025*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafd9a2a-eca0-430e-8cd9-edfa1595df17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìö Best Practices & Performance Tips\n",
    "\n",
    "## üöÄ Performance Optimization\n",
    "\n",
    "### 1. **Use Partitioning for Large Tables**\n",
    "```python\n",
    "# Partition by date for time-series data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"claim_date\") \\\n",
    "    .saveAsTable(\"payer_gold.claims_partitioned\")\n",
    "```\n",
    "\n",
    "### 2. **Enable Z-Ordering for Common Filters**\n",
    "```sql\n",
    "OPTIMIZE payer_gold.claims_enriched\n",
    "ZORDER BY (member_id, claim_date);\n",
    "```\n",
    "\n",
    "### 3. **Use Caching for Frequently Accessed DataFrames**\n",
    "```python\n",
    "claims_df = spark.table(\"payer_silver.claims\").cache()\n",
    "# Now use claims_df multiple times without re-reading\n",
    "```\n",
    "\n",
    "### 4. **Broadcast Small Tables in Joins**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîí Data Quality Best Practices\n",
    "\n",
    "### 1. **Always Validate Data**\n",
    "```python\n",
    "# Add constraints\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE payer_silver.claims \n",
    "    ADD CONSTRAINT valid_charge CHECK (total_charge > 0)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. **Use Schema Evolution Carefully**\n",
    "```python\n",
    "# Explicitly define schema for production\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"total_charge\", DoubleType(), True),\n",
    "    # ... more fields\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. **Implement Data Quality Checks**\n",
    "```python\n",
    "def validate_claims(df):\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    checks = {\n",
    "        \"null_claim_ids\": df.filter(col(\"claim_id\").isNull()).count(),\n",
    "        \"negative_charges\": df.filter(col(\"total_charge\") < 0).count(),\n",
    "        \"future_dates\": df.filter(col(\"claim_date\") > current_date()).count()\n",
    "    }\n",
    "    return checks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ Delta Lake Best Practices\n",
    "\n",
    "### 1. **Regular Maintenance**\n",
    "```sql\n",
    "-- Compact small files\n",
    "OPTIMIZE payer_gold.claims_enriched;\n",
    "\n",
    "-- Remove old versions (keep 7 days)\n",
    "VACUUM payer_gold.claims_enriched RETAIN 168 HOURS;\n",
    "\n",
    "-- Update statistics\n",
    "ANALYZE TABLE payer_gold.claims_enriched COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "### 2. **Use Time Travel for Auditing**\n",
    "```sql\n",
    "-- Query previous version\n",
    "SELECT * FROM payer_gold.claims_enriched VERSION AS OF 1;\n",
    "\n",
    "-- Query as of timestamp\n",
    "SELECT * FROM payer_gold.claims_enriched TIMESTAMP AS OF '2025-01-01';\n",
    "```\n",
    "\n",
    "### 3. **Enable Change Data Feed**\n",
    "```sql\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Best Practices\n",
    "\n",
    "### 1. **Medallion Layer Guidelines**\n",
    "- **Bronze**: Keep all source data, minimal transformation\n",
    "- **Silver**: One source system = one silver table (usually)\n",
    "- **Gold**: Many silver tables ‚Üí one gold table (join/aggregate)\n",
    "\n",
    "### 2. **Naming Conventions**\n",
    "```\n",
    "Catalog: <organization>_<environment>\n",
    "Schema: <domain>_<layer>\n",
    "Table: <entity>_<descriptor>\n",
    "\n",
    "Examples:\n",
    "- acme_prod.payer_bronze.claims_raw\n",
    "- acme_dev.payer_silver.claims_cleaned\n",
    "- acme_prod.payer_gold.member_360_view\n",
    "```\n",
    "\n",
    "### 3. **Documentation**\n",
    "```sql\n",
    "-- Add table comments\n",
    "COMMENT ON TABLE payer_gold.claims_enriched IS \n",
    "'Enriched claims with member and provider details for analytics';\n",
    "\n",
    "-- Add column comments\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "ALTER COLUMN total_charge COMMENT 'Total charged amount in USD';\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f241a39f-e379-43c2-bf07-a69604fe8fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìñ Quick Reference Guide\n",
    "\n",
    "## Common PySpark Operations\n",
    "\n",
    "### Reading Data\n",
    "```python\n",
    "# From Delta table\n",
    "df = spark.table(\"catalog.schema.table\")\n",
    "\n",
    "# From CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/file.csv\")\n",
    "\n",
    "# From JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# From Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Writing Data\n",
    "```python\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "# Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"table_name\")\n",
    "\n",
    "# With partitioning\n",
    "df.write.format(\"delta\").partitionBy(\"date_col\").saveAsTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Select columns\n",
    "df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Filter rows\n",
    "df.filter(col(\"amount\") > 100)\n",
    "df.where(\"amount > 100\")\n",
    "\n",
    "# Add new column\n",
    "df.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# Rename column\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "# Drop column\n",
    "df.drop(\"col_name\")\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    avg(\"amount\").alias(\"average\")\n",
    ")\n",
    "\n",
    "# Join tables\n",
    "df1.join(df2, \"key_column\")\n",
    "df1.join(df2, df1.key == df2.key, \"left\")\n",
    "\n",
    "# Sort\n",
    "df.orderBy(\"col_name\")\n",
    "df.orderBy(col(\"col_name\").desc())\n",
    "\n",
    "# Remove duplicates\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "### Common Functions\n",
    "```python\n",
    "# String functions\n",
    "upper(\"col_name\")\n",
    "lower(\"col_name\")\n",
    "trim(\"col_name\")\n",
    "concat(\"col1\", \"col2\")\n",
    "substring(\"col_name\", 1, 5)\n",
    "\n",
    "# Date functions\n",
    "current_date()\n",
    "current_timestamp()\n",
    "date_format(\"date_col\", \"yyyy-MM-dd\")\n",
    "year(\"date_col\")\n",
    "month(\"date_col\")\n",
    "datediff(\"date1\", \"date2\")\n",
    "\n",
    "# Math functions\n",
    "round(\"col_name\", 2)\n",
    "abs(\"col_name\")\n",
    "ceil(\"col_name\")\n",
    "floor(\"col_name\")\n",
    "\n",
    "# Conditional logic\n",
    "when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    "\n",
    "# Null handling\n",
    "col(\"col_name\").isNull()\n",
    "col(\"col_name\").isNotNull()\n",
    "coalesce(\"col1\", \"col2\", lit(0))\n",
    "```\n",
    "\n",
    "## Common SQL Operations\n",
    "\n",
    "### DDL Commands\n",
    "```sql\n",
    "-- Create database\n",
    "CREATE DATABASE IF NOT EXISTS database_name;\n",
    "\n",
    "-- Drop database\n",
    "DROP DATABASE IF EXISTS database_name CASCADE;\n",
    "\n",
    "-- Create table\n",
    "CREATE TABLE table_name (\n",
    "    id STRING,\n",
    "    amount DOUBLE,\n",
    "    date DATE\n",
    ");\n",
    "\n",
    "-- Drop table\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "-- Describe table\n",
    "DESCRIBE EXTENDED table_name;\n",
    "SHOW COLUMNS FROM table_name;\n",
    "```\n",
    "\n",
    "### DML Commands\n",
    "```sql\n",
    "-- Insert data\n",
    "INSERT INTO table_name VALUES (1, 'value1', 100);\n",
    "\n",
    "-- Update data (Delta Lake)\n",
    "UPDATE table_name SET amount = 200 WHERE id = 1;\n",
    "\n",
    "-- Delete data (Delta Lake)\n",
    "DELETE FROM table_name WHERE id = 1;\n",
    "\n",
    "-- Merge (Upsert)\n",
    "MERGE INTO target_table\n",
    "USING source_table\n",
    "ON target_table.id = source_table.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "### Query Commands\n",
    "```sql\n",
    "-- Basic SELECT\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "\n",
    "-- With WHERE clause\n",
    "SELECT * FROM table_name WHERE amount > 100;\n",
    "\n",
    "-- Aggregations\n",
    "SELECT category, COUNT(*), SUM(amount), AVG(amount)\n",
    "FROM table_name\n",
    "GROUP BY category;\n",
    "\n",
    "-- Joins\n",
    "SELECT a.*, b.name\n",
    "FROM table_a a\n",
    "INNER JOIN table_b b ON a.id = b.id;\n",
    "\n",
    "-- Window functions\n",
    "SELECT \n",
    "    *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as rank\n",
    "FROM table_name;\n",
    "\n",
    "-- CTE (Common Table Expression)\n",
    "WITH summary AS (\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM table_name\n",
    "    GROUP BY category\n",
    ")\n",
    "SELECT * FROM summary WHERE total > 1000;\n",
    "```\n",
    "\n",
    "## Databricks Utilities\n",
    "```python\n",
    "# File system operations\n",
    "dbutils.fs.ls(\"path/\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "dbutils.fs.rm(\"path/\", recurse=True)\n",
    "dbutils.fs.mkdirs(\"path/\")\n",
    "\n",
    "# Widgets (parameters)\n",
    "dbutils.widgets.text(\"param_name\", \"default_value\")\n",
    "param_value = dbutils.widgets.get(\"param_name\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, {\"param\": \"value\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this reference handy as you build your data pipelines!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8256874346672629,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBX Workshop_IPA Actuaries_10262025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "my_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
