{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febfa2bd-7981-4064-8bd4-687c26364d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸŽ“ Databricks Workshop: Healthcare Payer Analytics\n",
    "**Hands-on Introduction to Databricks ETL and Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Workshop Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. âœ… Understand the **Medallion Architecture** (Bronze, Silver, Gold layers)\n",
    "2. âœ… Build data pipelines using **SQL** and **PySpark**\n",
    "3. âœ… Load raw data using `COPY INTO` commands\n",
    "4. âœ… Transform and clean data with best practices\n",
    "5. âœ… Create analytics-ready tables for business insights\n",
    "6. âœ… Visualize data using Databricks native capabilities\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¥ Dataset Overview\n",
    "\n",
    "We'll work with **healthcare payer data** including:\n",
    "- **Members**: Health plan enrollees\n",
    "- **Claims**: Medical claim submissions\n",
    "- **Providers**: Healthcare providers (doctors, clinics)\n",
    "- **Diagnoses**: Diagnosis codes from claims\n",
    "- **Procedures**: Medical procedures performed\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b67ad5a-ac8b-4034-8885-cf6ed972bdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“‘ Table of Contents\n",
    "\n",
    "1. **[Introduction](#introduction)**\n",
    "   - What is a Lakehouse?\n",
    "   - Unity Catalog Overview\n",
    "   - Medallion Architecture\n",
    "   - Managed Tables & Predictive Optimization\n",
    "\n",
    "2. **[Modeling Concepts](#modeling-concepts)**\n",
    "   - Dimensional Modeling\n",
    "   - Data Vault Architecture\n",
    "   - Sample Data Model\n",
    "\n",
    "3. **[Setup](#setup)**\n",
    "   - Configure Widgets & Parameters\n",
    "   - Create Catalog & Schemas\n",
    "   - Setup Unity Catalog Volumes\n",
    "   - Download Sample Data\n",
    "\n",
    "4. **[Bronze Layer - Ingest Raw Data](#bronze-layer)**\n",
    "   - Understanding Bronze Layer\n",
    "   - Using COPY INTO\n",
    "   - Loading with PySpark\n",
    "   - Data Exploration Exercises\n",
    "\n",
    "5. **[Silver Layer - Transform & Clean](#silver-layer)**\n",
    "   - Understanding Silver Layer\n",
    "   - SQL Transformations\n",
    "   - PySpark Transformations\n",
    "   - Data Quality Checks\n",
    "\n",
    "6. **[Gold Layer - Analytics Ready](#gold-layer)**\n",
    "   - Understanding Gold Layer\n",
    "   - Enriched Fact Tables\n",
    "   - Provider Performance Dashboard\n",
    "   - Time-Series Analysis\n",
    "   - Cohort Analysis\n",
    "\n",
    "7. **[Analytics & Visualization](#analytics-visualization)**\n",
    "   - Using Databricks AI Assistant\n",
    "   - Statistical Analysis\n",
    "   - Creating Visualizations (6+ types)\n",
    "   - Advanced Analytics Examples\n",
    "\n",
    "8. **[Hands-On Exercises](#exercises)**\n",
    "   - Exercise 1: Provider Specialty Analysis\n",
    "   - Exercise 2: Time-Based Claims Analysis\n",
    "   - Exercise 3: Cost Outlier Detection\n",
    "   - Exercise 4: Member Health Risk Scoring\n",
    "\n",
    "9. **[Best Practices & Performance](#best-practices)**\n",
    "   - Performance Optimization\n",
    "   - Data Quality Best Practices\n",
    "   - Delta Lake Maintenance\n",
    "   - Architecture Guidelines\n",
    "\n",
    "10. **[Quick Reference Guide](#reference-guide)**\n",
    "    - PySpark Operations\n",
    "    - SQL Commands\n",
    "    - Databricks Utilities\n",
    "\n",
    "11. **[Workshop Summary & Next Steps](#summary)**\n",
    "    - What You Accomplished\n",
    "    - Continue Learning\n",
    "    - Resources & Certifications\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸ’¡ Workshop Tip**: This is a hands-on workshop! Execute each cell as you go and experiment with the code. Don't hesitate to ask questions or use the Databricks AI Assistant.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe39bed2-7a8e-4992-8c17-3df323aec010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "This guide shows how to design a **Databricks Medallion Architecture** (Bronze, Silver, Gold) pipeline with SQL, using sample tables and realistic transformations relevant to a **Healthcare Payer**. All code is written to work in Databricks SQL notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979a9246-402c-479e-aef6-f29943d5f664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is a lakehouse?\n",
    "\n",
    "1. **Hybrid Architecture:**  \n",
    "   A lakehouse combines the best of data lakes (flexible, cheap storage) and data warehouses (structured, fast analytics), providing transactional and governance features on top of open cloud storage.\n",
    "\n",
    "2. **ACID Transactions and Schema Governance:**  \n",
    "   Lakehouses support ACID transactions for consistent concurrent data access and enforce schema management, which is essential for data integrity and compliance.\n",
    "\n",
    "3. **Open and Decoupled:**  \n",
    "   They use open file formats (like Parquet), decouple compute from storage for flexible scalability, and allow access by a variety of analytics, BI, and machine learning tools.\n",
    "\n",
    "4. **Supports All Workloads and Data Types:**  \n",
    "   The architecture enables SQL analytics, data science, machine learning, and can handle structured, semi-structured, and unstructured data (including images, text, video).\n",
    "\n",
    "5. **Single Platform, Enterprise Ready:**  \n",
    "   With features like real-time streaming, end-to-end governance, access control, and data discovery tools, lakehouses reduce complexityâ€”allowing enterprises to manage all data and analytics needs in one unified system.\n",
    "![](https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc272da-63ab-456c-bec7-9ef87b0cdcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog\n",
    "\n",
    "[Unified and open governance for data and AI in the Lakehouse](https://www.databricks.com/product/unity-catalog#features)\n",
    "\n",
    "Eliminate silos, simplify governance and accelerate insights at scale:\n",
    "\n",
    "- Centralizes governance, access control, auditing, and data discovery for all data and AI assets across Databricks workspaces.\n",
    "- Enables fine-grained, consistent data access policies (row- and column-level), defined once and applied everywhere.\n",
    "- Provides comprehensive data lineage and audit logs, showing how and by whom data is accessed and transformed.\n",
    "- Supports data discovery, tagging, and documentation, making it easier to find and understand datasets and models.\n",
    "- Works across multiple clouds and supports open formats (Delta, Parquet, etc.), avoiding vendor lock-in and enabling broad interoperability.\n",
    "- Allows secure data and AI sharing within and outside the organization, including clean rooms and partner collaborations.\n",
    "- Provides built-in monitoring for data quality, freshness, and usage, helping ensure compliance and rapid troubleshooting.\n",
    "- Integrates tightly with the catalog/schema/object model, enhancing organization and security for all managed data assets.\n",
    "\n",
    "![](https://www.databricks.com/sites/default/files/2025-05/header-unity-catalog.png?v=1748513086)\n",
    "\n",
    "[Unity Catalog Search & Data Explorer](https://app.getreprise.com/launch/96mpAqy/)\n",
    "\n",
    "[Exploring Lineage and Governance with Unity Catalog](https://app.getreprise.com/launch/MnqjQDX/)\n",
    "\n",
    "[A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2024-08/comprehensive-guide-to-data-and-ai-governance.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5571fe0-a829-411d-b1ee-b827fd1b4c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Medallion lakehouse architecture\n",
    "\n",
    "In this example, we will be following the **medallion lakehouse architecture**. The medallion architecture is a data design pattern to organize data in a lakehouse. The goal is to progressively improve the quality and structure of the data as it flows through each layer (Bronze [**raw**] â†’ Silver [**staging**] â†’ Gold [**main**]).\n",
    "\n",
    "1. **Bronze layer**: the raw, unvalidated data\n",
    "2. **Silver**: cleansed and conformed data\n",
    "3. **Gold**: curated business-level tables\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/building-data-pipelines-with-delta-lake-120823.png?v=1702318922\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8292153-2b0a-47dd-b6c6-b1cf4c45cdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Managed tables\n",
    "\n",
    "[How Unity Catalog Managed Tables Automate Performance at Scale](https://www.databricks.com/blog/how-unity-catalog-managed-tables-automate-performance-scale) with [Predictive Optimization](https://learn.microsoft.com/en-us/azure/databricks/optimizations/predictive-optimization)\n",
    "\n",
    "\n",
    "<!-- ![](https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384) -->\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/image2_48.png?v=1751297384\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9115bf6-333a-4cb8-965c-5e73a2d204d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Faster Queries: 20X query latency reduction](https://www.databricks.com/blog/predictive-optimization-automatically-delivers-faster-queries-and-lower-tco)\n",
    "\n",
    "**Predictive Optimization** in Databricks automates table management by leveraging Unity Catalog and the Data Intelligence Platform. This innovative feature currently runs the following optimizations for Unity Catalog managed tables:\n",
    "\n",
    "* **OPTIMIZE** - Triggers incremental clustering for enabled tables. Improves query performance by optimizing file sizes.\n",
    "* **VACUUM** - Reduces storage costs by deleting data files no longer referenced by the table.\n",
    "* **ANALYZE** - Triggers incremental update of statistics to improve query performance. \n",
    "\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/styles/max_1000x1000/public/2024-05/db-976-blog-img-og.png?itok=qWBT8VA-&v=1717158571\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n",
    "\n",
    "**Compaction** - This enhances query performance by optimizing file sizes, ensuring that data retrieval is efficient.\n",
    "\n",
    "**Liquid Clustering** - This technique incrementally clusters incoming data, enabling optimal data layout and efficient data skipping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce4ebf7-dc51-44bc-8bf2-804fec60b0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Medallion Pipeline for a Healthcare Payer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75be99a-8ae5-4056-8ef9-6d1763695c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Modeling Concepts\n",
    "\n",
    "Databricks fully supports both **dimensional modeling** (Kimball/star schema) and **Inmon-style, Data Vault architectures (hubs, satellites, links)** on the Lakehouse platform. For dimensional models, you can build classic star and snowflake schemas directly with SQL, benefiting from ACID transactions and scalable Delta Lake tables.\n",
    "\n",
    "For Inmon/Data Vault use cases, Databricks provides rich support for hub-and-satellite models that address core enterprise needs for history, auditability, and extensibilityâ€”find end.\n",
    "\n",
    "The Lakehouse approach lets you mix these styles as needed within a single platform, so you can incrementally land data in Raw Vault/EDW structures and later expose it as dimensional martsâ€”all with Delta Live Tables, fine-grained security, and open formats.\n",
    "\n",
    "Key blog resources:\n",
    "\n",
    "[Implementing Dimensional Modeling](https://www.databricks.com/blog/implementing-dimensional-data-warehouse-databricks-sql-part-1)\n",
    "\n",
    "[Implementing Data Vault/Hub-Satellite](https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html) \n",
    "\n",
    "[Data Vault Best Practices](https://www.databricks.com/blog/data-vault-best-practice-implementation-lakehouse)\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/5c87faea-3e60-4f71-826d-42d04f6cdc0b.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "  <img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/6826c275-d462-4c07-a978-43fe9c40f3ed.png\" alt=\"Managed Tables\" width=\"400\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043442-a66b-47f2-9f2c-c6890d2de349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data Model\n",
    "\n",
    "For a payer, commonly used tables include:\n",
    "\n",
    "- **Members**: members enrolled in a health plan\n",
    "- **Claims**: medical claim submissions\n",
    "- **Providers**: healthcare providers (doctors, clinics)\n",
    "- **Diagnoses**: claim diagnosis codes\n",
    "- **Procedures**: procedures/services performed\n",
    "\n",
    "Each table should have at least 50 rows.\n",
    "\n",
    "<img src=\"https://user-gen-media-assets.s3.amazonaws.com/gpt4o_images/bdd54dc0-f3c7-4975-80a3-0017ebdb121c.png\" alt=\"Managed Tables\" width=\"400\" height=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bd63ca-5638-422f-a991-0d6c3d37a2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table\tKey Columns\n",
    "\n",
    "**Members**\tmember_id, first_name, last_name, birth_date, gender, plan_id, effective_date\n",
    "\n",
    "**Claims**\tclaim_id, member_id, provider_id, claim_date, total_charge, claim_status\n",
    "\n",
    "**Providers**\tprovider_id, npi, provider_name, specialty, address, city, state\n",
    "\n",
    "**Diagnoses**\tclaim_id, diagnosis_code, diagnosis_desc\n",
    "\n",
    "**Procedures**\tclaim_id, procedure_code, procedure_desc, amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b310730-d71e-40c6-ae23-7618308dfd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877474a-5f0b-4528-bafa-436396ede8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"my_catalog\", \"Catalog\")\n",
    "dbutils.widgets.text(\"bronze_db\", \"payer_bronze\", \"Bronze DB\")\n",
    "dbutils.widgets.text(\"silver_db\", \"payer_silver\", \"Silver DB\")\n",
    "dbutils.widgets.text(\"gold_db\", \"payer_gold\", \"Gold DB\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
    "silver_db = dbutils.widgets.get(\"silver_db\")\n",
    "gold_db = dbutils.widgets.get(\"gold_db\")\n",
    "\n",
    "path = f\"/Volumes/{catalog}/{bronze_db}/payer/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3beab112-8351-412b-89dd-b7d12bed854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Bronze DB: {bronze_db}\")\n",
    "print(f\"Silver DB: {silver_db}\")\n",
    "print(f\"Gold DB: {gold_db}\")\n",
    "print(f\"Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45166e53-a9ae-4494-b817-653adfe484f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1346d2f0-da56-4664-9549-ca712a979960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc54c890-6b43-4031-ae41-39e8aff39654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create new **Volumes** as below and upload shared files to your volumes.\n",
    "\n",
    "(schema) payer_bronze \\\n",
    "|--- payer/files/ \\\n",
    "|------ claims \\\n",
    "|------ members \\\n",
    "|------ providers \\\n",
    "|------ diagnoses \\\n",
    "|------ procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee52692-6c62-4c15-b115-e05aca2d260f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.payer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3974a340-43ce-4065-bf06-4d899386100b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the volume and folders\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/members\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers\")\n",
    "dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc7297a-1005-4168-9f62-b43322f98751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = \"https://github.com/bigdatavik/databricksfirststeps/blob/6b225621c3c010a2734ab604efd79c15ec6c71b8/data/Payor_Archive.zip?raw=true\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Define the base path\n",
    "base_path = f\"/Volumes/{catalog}/{bronze_db}/payer/downloads\" \n",
    "\n",
    "# Extract the ZIP file to the base path\n",
    "zip_file.extractall(base_path)\n",
    "\n",
    "# Define the paths\n",
    "paths = {\n",
    "    \"claims.csv\": f\"{base_path}/claims\",\n",
    "    \"diagnoses.csv\": f\"{base_path}/diagnosis\",\n",
    "    \"procedures.csv\": f\"{base_path}/procedures\",\n",
    "    \"member.csv\": f\"{base_path}/members\",\n",
    "    \"providers.csv\": f\"{base_path}/providers\"\n",
    "}\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "for dest_path in paths.values():\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "# Move the files to the respective directories\n",
    "for file_name, dest_path in paths.items():\n",
    "    source_file = f\"{base_path}/{file_name}\"\n",
    "    if os.path.exists(source_file):\n",
    "        os.rename(source_file, f\"{dest_path}/{file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f529a21-c047-4a8c-81f1-4d6c452dee52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Copy the files to the specified directories and print the paths\n",
    "shutil.copy(f\"{base_path}/claims/claims.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/claims/claims.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/diagnosis/diagnoses.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/diagnosis/diagnosis.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/procedures/procedures.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/procedures/procedures.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/members/member.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/members/members.csv\")\n",
    "\n",
    "shutil.copy(f\"{base_path}/providers/providers.csv\", f\"/Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")\n",
    "print(f\"Copied to /Volumes/{catalog}/{bronze_db}/payer/files/providers/providers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd9129d-4144-45a7-92df-4b44d1a85f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸš€ Let's Build Your First Data Pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Roadmap\n",
    "\n",
    "```\n",
    "ðŸ“¥ Bronze Layer    â†’    ðŸ”§ Silver Layer    â†’    â­ Gold Layer    â†’    ðŸ“Š Analytics\n",
    "   (Raw Data)          (Cleaned Data)        (Business Tables)      (Insights)\n",
    "```\n",
    "\n",
    "In the following sections, we'll build a complete data pipeline following the **Medallion Architecture**:\n",
    "\n",
    "1. **Bronze Layer**: Ingest raw CSV files into Delta tables\n",
    "2. **Silver Layer**: Clean, deduplicate, and transform data\n",
    "3. **Gold Layer**: Create enriched analytics tables\n",
    "4. **Analytics**: Generate insights and visualizations\n",
    "\n",
    "Let's get started! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8747d067-1569-4679-a336-41dcc5479627",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Catalog and Create Bronze Schema"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Set the catalog and schema\n",
    "# CREATE CATALOG IF NOT EXISTS my_catalog;\n",
    "# USE CATALOG my_catalog;\n",
    "\n",
    "# -- Create bronze schema\n",
    "# CREATE SCHEMA IF NOT EXISTS payer_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209c8fdf-7b40-41fb-87a2-3b7ae5835233",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“¥ Bronze Layer â€“ Ingest Raw Data\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Bronze Layer?\n",
    "\n",
    "The **Bronze Layer** is the landing zone for raw data. Here we:\n",
    "- ðŸ“‚ Load data \"as-is\" from source files (CSV, JSON, Parquet, etc.)\n",
    "- ðŸ’¾ Store in Delta Lake format for ACID transactions\n",
    "- ðŸ“ Apply minimal transformation (just schema inference)\n",
    "- â±ï¸ Keep historical data for audit and reprocessing\n",
    "\n",
    "> **ðŸ’¡ Best Practice**: Use `COPY INTO` for incremental, idempotent loading. It automatically skips already-loaded files!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91c7230-8541-4084-b852-93919095631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Verify Source Files\n",
    "\n",
    "Let's first check that our source files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242023e4-0a88-4c9c-8da6-04d5088dc662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Files in Payer Data Directory"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST '/Volumes/my_catalog/payer_bronze/payer/files/claims/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee0a970-02b6-4be4-8675-d0219d62d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load Data with COPY INTO\n",
    "\n",
    "### ðŸ“– Understanding COPY INTO\n",
    "\n",
    "`COPY INTO` is Databricks' recommended command for loading data from cloud storage into Delta tables.\n",
    "\n",
    "**Key Benefits:**\n",
    "- âœ… **Idempotent**: Safely re-run without duplicating data\n",
    "- âœ… **Incremental**: Only loads new files automatically\n",
    "- âœ… **Schema Evolution**: Can merge new columns with `mergeSchema` option\n",
    "- âœ… **Atomic**: Either succeeds completely or rolls back\n",
    "\n",
    "**Syntax:**\n",
    "```sql\n",
    "COPY INTO <table_name>\n",
    "FROM '<source_path>'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS('mergeSchema' = 'true')\n",
    "```\n",
    "\n",
    "ðŸ“š **Learn More:**\n",
    "- [COPY INTO Documentation](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-copy-into)\n",
    "- [COPY INTO Examples](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/copy-into/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383fea56-06e8-4bdd-aa01-35ea9cc2f69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398367f4-130e-4cd0-8faf-74859a61e741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Claims Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.claims_raw;\n",
    "COPY INTO payer_bronze.claims_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/claims/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true', 'force' = 'true');\n",
    "\n",
    "-- NOTE: 'force = true' is used here for demo purposes only to reload all files every time. In production, omit this option so COPY INTO only processes new data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c87df9-f460-4e15-bd6d-c9e3da090f48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Diagnosis Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.diagnosis_raw;\n",
    "COPY INTO payer_bronze.diagnosis_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/diagnosis/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863a1bfd-e455-4e93-bdd1-a66c1ede59af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Members Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.members_raw;\n",
    "COPY INTO payer_bronze.members_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/members/')\n",
    "\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398146a1-96d1-45a2-be1d-ff80834d7e37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Procedures Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.procedures_raw;\n",
    "COPY INTO payer_bronze.procedures_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/procedures/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c10dc9-2a3f-4f1f-a9ef-113259d7bae7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Providers Data into Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS payer_bronze.providers_raw;\n",
    "COPY INTO payer_bronze.providers_raw FROM\n",
    "(SELECT\n",
    "*\n",
    "FROM '/Volumes/my_catalog/payer_bronze/payer/files/providers/')\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS('header' = 'true',\n",
    "               'inferSchema' = 'true',\n",
    "               'delimiter' = ',')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3617a3bc-f19f-4580-aa9e-a43be0ee1c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ðŸ Alternative: Loading Data with PySpark\n",
    "\n",
    "While SQL is great for batch loading, PySpark gives you more programmatic control. Here's how to load the same data using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ef4290-c63f-4884-a5b4-1ac7725dfcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Load data using PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Option 1: Let Spark infer the schema\n",
    "claims_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/my_catalog/payer_bronze/payer/files/claims/\")\n",
    "\n",
    "# Display first 10 rows\n",
    "display(claims_df.limit(10))\n",
    "\n",
    "# Show schema\n",
    "print(\"Claims Schema:\")\n",
    "claims_df.printSchema()\n",
    "\n",
    "# Get row count\n",
    "print(f\"\\nTotal rows loaded: {claims_df.count()}\")\n",
    "\n",
    "# Write to Delta table (this creates or replaces the table)\n",
    "# claims_df.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .saveAsTable(\"payer_bronze.claims_raw_pyspark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9506c098-f429-4193-91a8-bd5696c4ae03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Exercise: Query Bronze Tables\n",
    "\n",
    "Now that we've loaded data into Bronze tables, let's explore what we have:\n",
    "\n",
    "**Try these queries yourself:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b757d-bb2d-476e-a8cc-b880ef7a965c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query: Count records in each bronze table using PySpark\n",
    "from pyspark.sql.functions import lit, count\n",
    "\n",
    "tables = ['claims_raw', 'members_raw', 'providers_raw', 'diagnosis_raw', 'procedures_raw']\n",
    "row_counts = []\n",
    "\n",
    "for table in tables:\n",
    "    cnt = spark.table(f\"payer_bronze.{table}\").count()\n",
    "    row_counts.append((table, cnt))\n",
    "    \n",
    "# Create DataFrame to display results\n",
    "result_df = spark.createDataFrame(row_counts, [\"table_name\", \"row_count\"])\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb123759-ef4a-4c75-ae91-9ce33afe32ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ”§ Silver Layer â€“ Transform, Clean, and Join\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Silver Layer?\n",
    "\n",
    "The **Silver Layer** is where we transform raw data into clean, validated, and enriched datasets. Here we:\n",
    "\n",
    "- ðŸ§¹ **Clean**: Remove nulls, trim whitespace, fix data quality issues\n",
    "- ðŸ”„ **Transform**: Cast data types, standardize formats\n",
    "- ðŸ—‘ï¸ **Deduplicate**: Remove duplicate records based on business keys\n",
    "- ðŸ” **Validate**: Apply business rules and data quality checks\n",
    "- ðŸ“Š **Enrich**: Join related tables, calculate derived columns\n",
    "\n",
    "> **ðŸ’¡ Best Practice**: Silver tables should be \"analytics-ready\" â€“ cleaned, validated, and properly typed!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c2cea8-7251-4817-8af5-c09889d6dc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Transform Bronze to Silver (SQL)\n",
    "\n",
    "Let's clean and transform our Bronze tables. We'll demonstrate with multiple examples using both **SQL** and **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6db0c47-1b8c-4fa4-821c-230f3a15a7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Silver Schema and Deduplicate Data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create silver schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_silver;\n",
    "\n",
    "\n",
    "-- Members: select relevant fields, cast types, remove duplicates\n",
    "CREATE OR REPLACE TABLE payer_silver.members AS\n",
    "SELECT\n",
    "  DISTINCT CAST(member_id AS STRING) AS member_id,\n",
    "  TRIM(first_name) AS first_name,\n",
    "  TRIM(last_name) AS last_name,\n",
    "  CAST(birth_date AS DATE) AS birth_date,\n",
    "  gender,\n",
    "  plan_id,\n",
    "  CAST(effective_date AS DATE) AS effective_date\n",
    "FROM payer_bronze.members_raw\n",
    "WHERE member_id IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Claims: remove duplicates, prepare data\n",
    "CREATE OR REPLACE TABLE payer_silver.claims AS\n",
    "SELECT\n",
    "  DISTINCT claim_id,\n",
    "  member_id,\n",
    "  provider_id,\n",
    "  CAST(claim_date AS DATE) AS claim_date,\n",
    "  ROUND(total_charge, 2) AS total_charge,\n",
    "  LOWER(claim_status) AS claim_status\n",
    "FROM payer_bronze.claims_raw\n",
    "WHERE claim_id IS NOT NULL AND total_charge > 0;\n",
    "\n",
    "\n",
    "-- Providers: deduplicate\n",
    "CREATE OR REPLACE TABLE payer_silver.providers AS\n",
    "SELECT\n",
    "  DISTINCT provider_id,\n",
    "  npi,\n",
    "  provider_name,\n",
    "  specialty,\n",
    "  address,\n",
    "  city,\n",
    "  state\n",
    "FROM payer_bronze.providers_raw\n",
    "WHERE provider_id IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03f4f06-9cc5-4509-aa52-ea96f802396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform with PySpark\n",
    "\n",
    "Now let's see how to do the same transformations using PySpark. This approach is more flexible for complex business logic.\n",
    "\n",
    "### Example: Transform Procedures Table with PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb23dcb-68e4-44ee-b5ee-710ad8fb8b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, round as spark_round, when, regexp_replace\n",
    "\n",
    "# Read from Bronze\n",
    "procedures_bronze = spark.table(\"payer_bronze.procedures_raw\")\n",
    "\n",
    "# Clean and cast the amount column\n",
    "procedures_bronze_clean = procedures_bronze.withColumn(\n",
    "    \"amount_clean\",\n",
    "    regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "procedures_silver = procedures_bronze_clean \\\n",
    "    .dropDuplicates(['claim_id', 'procedure_code']) \\\n",
    "    .filter(col(\"claim_id\").isNotNull()) \\\n",
    "    .filter(col(\"amount_clean\") > 0) \\\n",
    "    .select(\n",
    "        col(\"claim_id\"),\n",
    "        upper(trim(col(\"procedure_code\"))).alias(\"procedure_code\"),\n",
    "        trim(col(\"procedure_desc\")).alias(\"procedure_desc\"),\n",
    "        spark_round(col(\"amount_clean\"), 2).alias(\"amount\"),\n",
    "        when(col(\"amount_clean\") < 100, \"Low\")\n",
    "        .when(col(\"amount_clean\") < 500, \"Medium\")\n",
    "        .when(col(\"amount_clean\") < 1000, \"High\")\n",
    "        .otherwise(\"Very High\").alias(\"cost_category\")\n",
    "    )\n",
    "\n",
    "# Show sample data\n",
    "print(\"Transformed Procedures (first 10 rows):\")\n",
    "display(procedures_silver.limit(10))\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nCost Category Distribution:\")\n",
    "display(procedures_silver.groupBy(\"cost_category\").count().orderBy(\"cost_category\"))\n",
    "\n",
    "# Write to Silver table\n",
    "procedures_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_silver.procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cb09e1-fe4c-4c5b-bcba-fbff4eb501fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸŽ¯ Exercise: Data Quality Checks\n",
    "\n",
    "Let's verify our Silver transformations worked correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e90cd4-2b05-41cd-9fe9-3de6a242c6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Data Quality Check 1: Check for nulls in key columns\n",
    "print(\"=== NULL CHECK ===\")\n",
    "members_df = spark.table(\"payer_silver.members\")\n",
    "null_counts = members_df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in members_df.columns\n",
    "])\n",
    "display(null_counts)\n",
    "\n",
    "# Data Quality Check 2: Check for duplicates\n",
    "print(\"\\n=== DUPLICATE CHECK ===\")\n",
    "claims_df = spark.table(\"payer_silver.claims\")\n",
    "total_rows = claims_df.count()\n",
    "distinct_rows = claims_df.select(\"claim_id\").distinct().count()\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct claim_ids: {distinct_rows}\")\n",
    "print(f\"Duplicates: {total_rows - distinct_rows}\")\n",
    "\n",
    "# Data Quality Check 3: Value range checks\n",
    "print(\"\\n=== VALUE RANGE CHECK ===\")\n",
    "claims_stats = claims_df.agg(\n",
    "    {\"total_charge\": \"min\", \"total_charge\": \"max\", \"total_charge\": \"avg\"}\n",
    ")\n",
    "display(claims_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91106e3-b2d1-4a46-bdec-ce066badb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# â­ Gold Layer â€“ Aggregate, Model, Ready for Analytics\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Gold Layer?\n",
    "\n",
    "The **Gold Layer** contains curated, business-level tables optimized for analytics and reporting. Here we:\n",
    "\n",
    "- ðŸ“Š **Aggregate**: Create summary metrics and KPIs\n",
    "- ðŸ”— **Join**: Combine related tables into wide, denormalized views\n",
    "- ðŸŽ¯ **Model**: Build fact tables, dimension tables, or star schemas\n",
    "- ðŸš€ **Optimize**: Structure data for fast query performance\n",
    "- ðŸ“ˆ **Business Logic**: Apply complex calculations and business rules\n",
    "\n",
    "> **ðŸ’¡ Best Practice**: Gold tables should answer specific business questions directly!\n",
    "\n",
    "---\n",
    "\n",
    "## Common Gold Table Patterns\n",
    "\n",
    "1. **Enriched Fact Tables**: Claims joined with members, providers, diagnoses\n",
    "2. **Aggregate Summaries**: Member-level or provider-level rollups\n",
    "3. **Time-Series Analytics**: Trends over time (daily, monthly, yearly)\n",
    "4. **Dimensional Tables**: Lookup tables for reporting tools\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b0153-df99-4566-982a-745e97ff0888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Enriched Tables with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da30fc9-0891-4c8a-8bbd-aae8687f1372",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Enriched Claims, Members, and Providers Summary Tables"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create gold schema\n",
    "CREATE SCHEMA IF NOT EXISTS payer_gold;\n",
    "\n",
    "-- Gold: Claims with member and provider details\n",
    "CREATE OR REPLACE TABLE payer_gold.claims_enriched AS\n",
    "SELECT\n",
    "  c.claim_id,\n",
    "  c.claim_date,\n",
    "  c.total_charge,\n",
    "  c.claim_status,\n",
    "  m.member_id,\n",
    "  m.first_name,\n",
    "  m.last_name,\n",
    "  m.gender,\n",
    "  m.plan_id,\n",
    "  p.provider_id,\n",
    "  p.provider_name,\n",
    "  p.specialty,\n",
    "  p.city,\n",
    "  p.state\n",
    "FROM payer_silver.claims c\n",
    "INNER JOIN payer_silver.members m ON c.member_id = m.member_id\n",
    "INNER JOIN payer_silver.providers p ON c.provider_id = p.provider_id;\n",
    "\n",
    "\n",
    "-- Gold: Claim Aggregates per Member\n",
    "CREATE OR REPLACE TABLE payer_gold.member_claim_summary AS\n",
    "SELECT\n",
    "  member_id,\n",
    "  COUNT(DISTINCT claim_id) AS total_claims,\n",
    "  SUM(total_charge) AS sum_claims,\n",
    "  MAX(total_charge) AS max_claim,\n",
    "  MIN(total_charge) AS min_claim\n",
    "FROM payer_silver.claims\n",
    "GROUP BY member_id;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67f5872-4eed-41cb-a33d-20b41fa85b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Advanced Analytics with PySpark\n",
    "\n",
    "Let's create more sophisticated analytics tables using PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd070c5e-2128-43c3-bb65-cb5fec3566f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1: Provider Performance Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fc395e-70a3-43f4-a0e4-0698fcf46d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, min, max, stddev, countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load tables\n",
    "claims = spark.table(\"payer_silver.claims\")\n",
    "providers = spark.table(\"payer_silver.providers\")\n",
    "members = spark.table(\"payer_silver.members\")\n",
    "\n",
    "# Create provider performance metrics\n",
    "provider_performance = claims.join(providers, \"provider_id\") \\\n",
    "    .groupBy(\n",
    "        \"provider_id\", \n",
    "        \"provider_name\", \n",
    "        \"specialty\", \n",
    "        \"city\", \n",
    "        \"state\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        countDistinct(\"member_id\").alias(\"unique_patients\"),\n",
    "        sum(\"total_charge\").alias(\"total_revenue\"),\n",
    "        avg(\"total_charge\").alias(\"avg_claim_amount\"),\n",
    "        min(\"total_charge\").alias(\"min_claim_amount\"),\n",
    "        max(\"total_charge\").alias(\"max_claim_amount\"),\n",
    "        stddev(\"total_charge\").alias(\"stddev_claim_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"Top 10 Providers by Revenue:\")\n",
    "display(provider_performance.limit(10))\n",
    "\n",
    "# Save to Gold table\n",
    "provider_performance.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.provider_performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ff1561-f6e2-43db-91a2-bef5a7e9ccd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 2: Time-Series Analysis - Monthly Claims Trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0060180a-6efb-4de0-bc2f-31e90ba6a9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, date_format, lag, round as spark_round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create monthly trends\n",
    "monthly_claims = claims \\\n",
    "    .withColumn(\"year\", year(\"claim_date\")) \\\n",
    "    .withColumn(\"month\", month(\"claim_date\")) \\\n",
    "    .withColumn(\"month_year\", date_format(\"claim_date\", \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year\", \"month\", \"month_year\", \"claim_status\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"claim_count\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge\")\n",
    "    ) \\\n",
    "    .orderBy(\"year\", \"month\", \"claim_status\")\n",
    "\n",
    "# Calculate month-over-month growth\n",
    "window_spec = Window.partitionBy(\"claim_status\").orderBy(\"year\", \"month\")\n",
    "\n",
    "monthly_trends = monthly_claims \\\n",
    "    .withColumn(\"prev_month_charges\", lag(\"total_charges\").over(window_spec)) \\\n",
    "    .withColumn(\n",
    "        \"mom_growth_pct\", \n",
    "        spark_round(\n",
    "            ((col(\"total_charges\") - col(\"prev_month_charges\")) / col(\"prev_month_charges\") * 100), \n",
    "            2\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Monthly Claims Trends with Growth:\")\n",
    "display(monthly_trends)\n",
    "\n",
    "# Save to Gold\n",
    "monthly_trends.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.monthly_claims_trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dc7637-0cbc-45aa-830a-6eef7f338892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ¤– Using Databricks AI Assistant\n",
    "\n",
    "---\n",
    "\n",
    "Databricks AI Assistant can help you write code, understand data, and troubleshoot issues!\n",
    "\n",
    "### How to Use AI Assistant:\n",
    "1. Click the AI Assistant icon\n",
    "2. Ask questions in natural language\n",
    "3. Get code suggestions and explanations\n",
    "\n",
    "### Example Prompts to Try:\n",
    "- \"What kind of aggregations can I do with table payer_gold.claims_enriched?\"\n",
    "- \"How do I calculate the total claims by specialty?\"\n",
    "- \"Show me how to create a window function for running totals\"\n",
    "- \"What does spark.table() command do?\"\n",
    "- \"Help me debug this PySpark error\"\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713ffea-c715-46f9-a267-75093518164d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Š Analytics & Visualization\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Databricks Visualizations\n",
    "\n",
    "Databricks provides powerful built-in visualization capabilities. You can create:\n",
    "- ðŸ“Š Bar charts, line charts, scatter plots\n",
    "- ðŸ—ºï¸ Geographic maps\n",
    "- ðŸ“ˆ Histograms and box plots\n",
    "- ðŸ¥§ Pie charts and area charts\n",
    "\n",
    "> **ðŸ’¡ Tip**: Use `display()` function to automatically generate interactive visualizations!\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Analysis with PySpark\n",
    "\n",
    "Let's explore our data using statistical analysis and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d43e2-a24b-4243-9566-3e3d8c733913",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Payer Claims Enriched Table"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"payer_gold.claims_enriched\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6513d65d-db72-4aa1-89a8-ffb2de886d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27af84c5-623f-44d6-9a93-019c3f90fb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Claims by Status (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55eeef7f-0571-40ba-9269-87c1388958c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges by Claim Status"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHN1bSwgY291bnQsIGF2ZwoKIyBBZ2dyZWdhdGUgdG90YWwgY2hhcmdlcyBieSBjbGFpbSBzdGF0dXMKY2xhaW1zX2J5X3N0YXR1cyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJjbGFpbV9zdGF0dXMiKSBcCiAgICAuYWdnKAogICAgICAgIHN1bSgidG90YWxfY2hhcmdlIikuYWxpYXMoInRvdGFsX2NoYXJnZXMiKSwKICAgICAgICBjb3VudCgiY2xhaW1faWQiKS5hbGlhcygiY2xhaW1fY291bnQiKSwKICAgICAgICBhdmcoInRvdGFsX2NoYXJnZSIpLmFsaWFzKCJhdmdfY2hhcmdlIikKICAgICkgXAogICAgLm9yZGVyQnkoInRvdGFsX2NoYXJnZXMiLCBhc2NlbmRpbmc9RmFsc2UpCgojIFZpc3VhbGl6ZSBhcyBhIGJhciBjaGFydDogWC1heGlzID0gY2xhaW1fc3RhdHVzLCBZLWF4aXMgPSB0b3RhbF9jaGFyZ2VzCmRpc3BsYXkoY2xhaW1zX2J5X3N0YXR1cykKCiMgVGlwOiBJbiB0aGUgdmlzdWFsaXphdGlvbiBvcHRpb25zLCBzZWxlY3QgIkJhciBDaGFydCIsIHNldCBYLWF4aXMgdG8gJ2NsYWltX3N0YXR1cycsIFktYXhpcyB0byAndG90YWxfY2hhcmdlcyc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd7a2487\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd7a2487\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd7a2487\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd7a2487) SELECT `claim_status`,SUM(`total_charges`) `column_3c6cda842464` FROM q GROUP BY `claim_status`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd7a2487\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_status",
             "id": "column_3c6cda842463"
            },
            "y": [
             {
              "column": "total_charges",
              "id": "column_3c6cda842464",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842464": {
             "name": "sum(total_charge)",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "516b639a-ca4d-4846-837e-77f7620816f5",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_status",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_status",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842464",
           "args": [
            {
             "column": "total_charges",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg\n",
    "\n",
    "# Aggregate total charges by claim status\n",
    "claims_by_status = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"claim_status\") \\\n",
    "    .agg(\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        count(\"claim_id\").alias(\"claim_count\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_charges\", ascending=False)\n",
    "\n",
    "# Visualize as a bar chart: X-axis = claim_status, Y-axis = total_charges\n",
    "display(claims_by_status)\n",
    "\n",
    "# Tip: In the visualization options, select \"Bar Chart\", set X-axis to 'claim_status', Y-axis to 'total_charges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bd3b72-1e38-4861-a90a-b1c0155d02d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Gender Distribution (Pie Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a772eaa0-abc2-43a6-8185-6bbcb61ba3d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count of Claims Grouped by Gender"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBBbmFseXplIGNsYWltcyBkaXN0cmlidXRpb24gYnkgZ2VuZGVyCmdlbmRlcl9hbmFseXNpcyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJnZW5kZXIiKSBcCiAgICAuYWdnKAogICAgICAgIGNvdW50KCJjbGFpbV9pZCIpLmFsaWFzKCJ0b3RhbF9jbGFpbXMiKSwKICAgICAgICBzdW0oInRvdGFsX2NoYXJnZSIpLmFsaWFzKCJ0b3RhbF9jaGFyZ2VzIiksCiAgICAgICAgYXZnKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygiYXZnX2NoYXJnZV9wZXJfY2xhaW0iKQogICAgKQoKcHJpbnQoIvCfkaUgQ2xhaW1zIEFuYWx5c2lzIGJ5IEdlbmRlcjoiKQpkaXNwbGF5KGdlbmRlcl9hbmFseXNpcykKCiMgVGlwOiBUcnkgY2hhbmdpbmcgdGhlIHZpc3VhbGl6YXRpb24gdG8gYSBQaWUgQ2hhcnQgdG8gc2VlIHRoZSBkaXN0cmlidXRpb24h\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView775de78\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView775de78\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView775de78\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView775de78) SELECT `gender`,COUNT(`gender`) `column_3c6cda842478` FROM q GROUP BY `gender`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView775de78\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "gender",
             "id": "column_3c6cda842477"
            },
            "y": [
             {
              "column": "gender",
              "id": "column_3c6cda842478",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842478": {
             "name": "count",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c7d6472d-90b5-4136-813a-67b5fb01f6b8",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "gender",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "gender",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842478",
           "args": [
            {
             "column": "gender",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze claims distribution by gender\n",
    "gender_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"gender\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        avg(\"total_charge\").alias(\"avg_charge_per_claim\")\n",
    "    )\n",
    "\n",
    "print(\"ðŸ‘¥ Claims Analysis by Gender:\")\n",
    "display(gender_analysis)\n",
    "\n",
    "# Tip: Try changing the visualization to a Pie Chart to see the distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fb4408-de01-4392-b809-162cdc6e3cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Time Series - Claims Over Time (Line Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85dff0f-de3d-4084-bff0-4e8d1c11a22f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum of Total Charges Grouped by Claim Date"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBUaW1lIHNlcmllcyBhbmFseXNpcyBvZiBjbGFpbXMKdGltZV9zZXJpZXMgPSBzcGFyay50YWJsZSgicGF5ZXJfZ29sZC5jbGFpbXNfZW5yaWNoZWQiKSBcCiAgICAuZ3JvdXBCeSgiY2xhaW1fZGF0ZSIpIFwKICAgIC5hZ2coCiAgICAgICAgc3VtKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygiZGFpbHlfY2hhcmdlcyIpLAogICAgICAgIGNvdW50KCJjbGFpbV9pZCIpLmFsaWFzKCJkYWlseV9jbGFpbV9jb3VudCIpCiAgICApIFwKICAgIC5vcmRlckJ5KCJjbGFpbV9kYXRlIikKCnByaW50KCLwn5OIIERhaWx5IENsYWltcyBUcmVuZHM6IikKZGlzcGxheSh0aW1lX3NlcmllcykKCiMgVGlwOiBTZWxlY3QgTGluZSBDaGFydCB2aXN1YWxpemF0aW9uIHdpdGggY2xhaW1fZGF0ZSBhcyBYLWF4aXMgYW5kIGRhaWx5X2NoYXJnZXMgYXMgWS1heGlzIQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd55ec8e\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd55ec8e\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd55ec8e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd55ec8e) SELECT `claim_date`,SUM(`daily_charges`) `column_3c6cda842489` FROM q GROUP BY `claim_date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd55ec8e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842488"
            },
            "y": [
             {
              "column": "daily_charges",
              "id": "column_3c6cda842489",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842489": {
             "name": "sum(total_charge)",
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "a82fd1a1-168b-4171-b219-ca56315cf563",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842489",
           "args": [
            {
             "column": "daily_charges",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time series analysis of claims\n",
    "time_series = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"claim_date\") \\\n",
    "    .agg(\n",
    "        sum(\"total_charge\").alias(\"daily_charges\"),\n",
    "        count(\"claim_id\").alias(\"daily_claim_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"claim_date\")\n",
    "\n",
    "print(\"ðŸ“ˆ Daily Claims Trends:\")\n",
    "display(time_series)\n",
    "\n",
    "# Tip: Select Line Chart visualization with claim_date as X-axis and daily_charges as Y-axis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7439bb5-34bd-47c3-a604-99b8471ab7a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Geographic Analysis - Claims by City (Map/Bar Chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6923f450-56f2-46da-a356-dbb09b5a5ec1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group Claims by City and Count"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBHZW9ncmFwaGljIGRpc3RyaWJ1dGlvbiBvZiBjbGFpbXMKY2l0eV9hbmFseXNpcyA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5ncm91cEJ5KCJjaXR5IiwgInN0YXRlIikgXAogICAgLmFnZygKICAgICAgICBjb3VudCgiY2xhaW1faWQiKS5hbGlhcygidG90YWxfY2xhaW1zIiksCiAgICAgICAgc3VtKCJ0b3RhbF9jaGFyZ2UiKS5hbGlhcygidG90YWxfY2hhcmdlcyIpLAogICAgICAgIGNvdW50RGlzdGluY3QoIm1lbWJlcl9pZCIpLmFsaWFzKCJ1bmlxdWVfbWVtYmVycyIpCiAgICApIFwKICAgIC5vcmRlckJ5KCJ0b3RhbF9jaGFyZ2VzIiwgYXNjZW5kaW5nPUZhbHNlKQoKcHJpbnQoIvCfl7rvuI8gQ2xhaW1zIERpc3RyaWJ1dGlvbiBieSBDaXR5OiIpCmRpc3BsYXkoY2l0eV9hbmFseXNpcy5saW1pdCgyMCkpCgojIFRpcDogVHJ5IE1hcCB2aXN1YWxpemF0aW9uIGlmIHlvdXIgZGF0YSBoYXMgcHJvcGVyIGdlb2dyYXBoaWMgZmllbGRzIQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewc1dd994\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewc1dd994\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewc1dd994\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewc1dd994) SELECT `city`,SUM(`count`) `column_3c6cda842507` FROM q GROUP BY `city`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewc1dd994\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Bar Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "city",
             "id": "column_3c6cda842506"
            },
            "y": [
             {
              "column": "count",
              "id": "column_3c6cda842507",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842507": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "448dac26-5767-4652-830e-d0e81c3424cc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "city",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "city",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842507",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "# Geographic distribution of claims\ncity_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n    .groupBy(\"city\", \"state\") \\\n    .agg(\n        count(\"claim_id\").alias(\"total_claims\"),\n        sum(\"total_charge\").alias(\"total_charges\"),\n        countDistinct(\"member_id\").alias(\"unique_members\")\n    ) \\\n    .orderBy(\"total_charges\", ascending=False)\n\nprint(\"ðŸ—ºï¸ Claims Distribution by City:\")\ndisplay(city_analysis.limit(20))\n\n# Tip: Try Map visualization if your data has proper geographic fields!",
       "commandTitle": "Map",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHOROPLETH"
         },
         {
          "key": "options",
          "value": {
           "clusteringMode": "e",
           "colors": {
            "background": "#ffffff",
            "borders": "#ffffff",
            "max": "#002FB4",
            "min": "#799CFF",
            "noValue": "#dddddd"
           },
           "keyColumn": "city",
           "legend": {
            "alignText": "right",
            "position": "bottom-left",
            "visible": true
           },
           "mapType": "usa",
           "noValuePlaceholder": "N/A",
           "popup": {
            "enabled": true,
            "template": "Region: <b>{{ @@name }}</b>\n<br>\nValue: <b>{{ @@value }}</b>"
           },
           "steps": 5,
           "targetField": "name",
           "tooltip": {
            "enabled": true,
            "template": "<b>{{ @@name }}</b>: {{ @@value }}"
           },
           "valueColumn": "total_charges",
           "valueFormat": "0,0.00"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b62534d2-5c77-4526-bffd-6ac36cb1e5b4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Geographic distribution of claims\n",
    "city_analysis = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .groupBy(\"city\", \"state\") \\\n",
    "    .agg(\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\"),\n",
    "        countDistinct(\"member_id\").alias(\"unique_members\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_charges\", ascending=False)\n",
    "\n",
    "print(\"ðŸ—ºï¸ Claims Distribution by City:\")\n",
    "display(city_analysis.limit(20))\n",
    "\n",
    "# Tip: Try Map visualization if your data has proper geographic fields!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e0c6dc-e3a0-451a-930f-f39c928a55c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Distribution Analysis - Charge Amount Histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ee3c88-20a6-4788-95b2-7bc433d43534",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Total Charges from Payer Claims Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBBbmFseXplIHRoZSBkaXN0cmlidXRpb24gb2YgY2xhaW0gY2hhcmdlcwpjaGFyZ2VzID0gc3BhcmsudGFibGUoInBheWVyX2dvbGQuY2xhaW1zX2VucmljaGVkIikuc2VsZWN0KCJ0b3RhbF9jaGFyZ2UiKQoKIyBHZXQgc3RhdGlzdGljYWwgc3VtbWFyeQpwcmludCgi8J+TiiBTdGF0aXN0aWNhbCBTdW1tYXJ5IG9mIENsYWltIENoYXJnZXM6IikKY2hhcmdlcy5kZXNjcmliZSgpLnNob3coKQoKIyBEaXNwbGF5IGhpc3RvZ3JhbQpwcmludCgiXG7wn5K1IENoYXJnZSBEaXN0cmlidXRpb24gKEhpc3RvZ3JhbSk6IikKZGlzcGxheShjaGFyZ2VzKQoKIyBUaXA6IFNlbGVjdCBIaXN0b2dyYW0gdmlzdWFsaXphdGlvbiB0byBzZWUgdGhlIGRpc3RyaWJ1dGlvbiBvZiBjaGFyZ2VzIQojIFlvdSBjYW4gYWRqdXN0IHRoZSBudW1iZXIgb2YgYmlucyBmb3IgYmV0dGVyIGdyYW51bGFyaXR5Lg==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView613bdce\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView613bdce\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView613bdce\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView613bdce) ,min_max AS (SELECT `total_charge`,(SELECT MAX(`total_charge`) FROM q) `target_column_max`,(SELECT MIN(`total_charge`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `total_charge`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 10 `step` FROM min_max) SELECT IF(ISNULL(`total_charge`),NULL,LEAST(WIDTH_BUCKET(`total_charge`,`min_value`,`max_value`,10),10)) `total_charge_BIN`,FIRST(`min_value` + ((IF(ISNULL(`total_charge`),NULL,LEAST(WIDTH_BUCKET(`total_charge`,`min_value`,`max_value`,10),10)) - 1) * `step`)) `total_charge_BIN_LOWER_BOUND`,FIRST(`step`) `total_charge_BIN_STEP`,COUNT(`total_charge`) `COUNT` FROM histogram_meta GROUP BY `total_charge_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView613bdce\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Bar Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "total_charge",
             "id": "column_3c6cda842509"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "5641bb40-65b6-4acd-aafb-5f180b0ec2e7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "total_charge_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "total_charge_BIN",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "total_charge_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "total_charge_BIN_STEP",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze the distribution of claim charges\n",
    "charges = spark.table(\"payer_gold.claims_enriched\").select(\"total_charge\")\n",
    "\n",
    "# Get statistical summary\n",
    "print(\"ðŸ“Š Statistical Summary of Claim Charges:\")\n",
    "charges.describe().show()\n",
    "\n",
    "# Display histogram\n",
    "print(\"\\nðŸ’µ Charge Distribution (Histogram):\")\n",
    "display(charges)\n",
    "\n",
    "# Tip: Select Histogram visualization to see the distribution of charges!\n",
    "# You can adjust the number of bins for better granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c481c9d-e2db-4f9d-bb8d-7a059e0fd3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Correlation Analysis - Scatter Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b953e3-6eba-43c5-8157-48245b421e22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Claim Date and Total Charge from Payer Claims"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTY2F0dGVyIHBsb3QgdG8gZmluZCByZWxhdGlvbnNoaXBzIGJldHdlZW4gdmFyaWFibGVzCnNjYXR0ZXJfZGF0YSA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5zZWxlY3QoImNsYWltX2RhdGUiLCAidG90YWxfY2hhcmdlIiwgImNsYWltX3N0YXR1cyIpCgpwcmludCgi8J+TjSBTY2F0dGVyIFBsb3Q6IENsYWltIERhdGUgdnMgQ2hhcmdlIEFtb3VudCIpCmRpc3BsYXkoc2NhdHRlcl9kYXRhKQoKIyBUaXA6IFNlbGVjdCBTY2F0dGVyIFBsb3QgdmlzdWFsaXphdGlvbgojIFgtYXhpczogY2xhaW1fZGF0ZSwgWS1heGlzOiB0b3RhbF9jaGFyZ2UKIyBHcm91cCBieTogY2xhaW1fc3RhdHVzIGZvciBjb2xvciBjb2Rpbmc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewded76e1\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewded76e1\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewded76e1\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewded76e1) SELECT `claim_date`,`total_charge` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewded76e1\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Scatter Plot",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842514"
            },
            "y": [
             {
              "column": "total_charge",
              "id": "column_3c6cda842515"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842515": {
             "name": "total_charge",
             "type": "scatter",
             "yAxis": 0
            },
            "total_charge": {
             "name": "total_charge",
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8e59f8fe-b565-4c7a-aad2-48446055c509",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.45947265625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "column": "total_charge",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTY2F0dGVyIHBsb3QgdG8gZmluZCByZWxhdGlvbnNoaXBzIGJldHdlZW4gdmFyaWFibGVzCnNjYXR0ZXJfZGF0YSA9IHNwYXJrLnRhYmxlKCJwYXllcl9nb2xkLmNsYWltc19lbnJpY2hlZCIpIFwKICAgIC5zZWxlY3QoImNsYWltX2RhdGUiLCAidG90YWxfY2hhcmdlIiwgImNsYWltX3N0YXR1cyIpCgpwcmludCgi8J+TjSBTY2F0dGVyIFBsb3Q6IENsYWltIERhdGUgdnMgQ2hhcmdlIEFtb3VudCIpCmRpc3BsYXkoc2NhdHRlcl9kYXRhKQoKIyBUaXA6IFNlbGVjdCBTY2F0dGVyIFBsb3QgdmlzdWFsaXphdGlvbgojIFgtYXhpczogY2xhaW1fZGF0ZSwgWS1heGlzOiB0b3RhbF9jaGFyZ2UKIyBHcm91cCBieTogY2xhaW1fc3RhdHVzIGZvciBjb2xvciBjb2Rpbmc=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd2549a7\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd2549a7\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd2549a7\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd2549a7) SELECT `claim_date`,SUM(`total_charge`) `column_3c6cda842511` FROM q GROUP BY `claim_date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd2549a7\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Line Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "claim_date",
             "id": "column_3c6cda842510"
            },
            "y": [
             {
              "column": "total_charge",
              "id": "column_3c6cda842511",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_3c6cda842511": {
             "name": "total_charge",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "093ecb4c-2a74-40a8-9b46-2d675add2207",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.9189453125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "claim_date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "claim_date",
           "type": "column"
          },
          {
           "alias": "column_3c6cda842511",
           "args": [
            {
             "column": "total_charge",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot to find relationships between variables\n",
    "scatter_data = spark.table(\"payer_gold.claims_enriched\") \\\n",
    "    .select(\"claim_date\", \"total_charge\", \"claim_status\")\n",
    "\n",
    "print(\"ðŸ“ Scatter Plot: Claim Date vs Charge Amount\")\n",
    "display(scatter_data)\n",
    "\n",
    "# Tip: Select Scatter Plot visualization\n",
    "# X-axis: claim_date, Y-axis: total_charge\n",
    "# Group by: claim_status for color coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b7166b-70ab-4b63-8055-51c5d42cc956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ðŸŽ¯ Advanced Analytics Example: Cohort Analysis\n",
    "\n",
    "Let's create a more complex analysis - member cohort analysis based on their enrollment date:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0836fe4a-ef44-4b1d-bea7-f8458e833451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, floor\n",
    "\n",
    "# Join members with their claims\n",
    "member_claims = spark.table(\"payer_silver.members\") \\\n",
    "    .join(spark.table(\"payer_silver.claims\"), \"member_id\", \"left\")\n",
    "\n",
    "# Create cohorts based on enrollment month\n",
    "cohort_analysis = member_claims \\\n",
    "    .withColumn(\"enrollment_month\", date_format(\"effective_date\", \"yyyy-MM\")) \\\n",
    "    .withColumn(\"claim_month\", date_format(\"claim_date\", \"yyyy-MM\")) \\\n",
    "    .withColumn(\n",
    "        \"months_since_enrollment\", \n",
    "        floor(months_between(\"claim_date\", \"effective_date\"))\n",
    "    ) \\\n",
    "    .groupBy(\"enrollment_month\", \"months_since_enrollment\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"member_id\").alias(\"active_members\"),\n",
    "        count(\"claim_id\").alias(\"total_claims\"),\n",
    "        sum(\"total_charge\").alias(\"total_charges\")\n",
    "    ) \\\n",
    "    .filter(col(\"months_since_enrollment\").isNotNull()) \\\n",
    "    .orderBy(\"enrollment_month\", \"months_since_enrollment\")\n",
    "\n",
    "print(\"ðŸ“… Member Cohort Analysis:\")\n",
    "display(cohort_analysis)\n",
    "\n",
    "# Save as a Gold table for future analysis\n",
    "cohort_analysis.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"payer_gold.member_cohort_analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a066c3-a0c9-41e8-bdd4-475282fb0950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI\n",
    "\n",
    "Intelligent analytics for everyone!\n",
    "\n",
    "Databricks AI/BI is a new type of business intelligence product designed to provide a deep understanding of your data's semantics, enabling self-service data analysis for everyone in your organization. AI/BI is built on a compound AI system that draws insights from the full lifecycle of your data across the Databricks platform, including ETL pipelines, lineage, and other queries.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-05/hero-image-ai-bi-v2-2x.png?v=1748417271\" alt=\"Managed Tables\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa71c33b-47d1-4b46-9940-2fdfd3cfb439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie\n",
    "\n",
    "Talk with your data\n",
    "\n",
    "Now everyone can get insights from data simply by asking questions in natural language.\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2025-06/ai-bi-genie-hero.png?v=1749162682\" alt=\"Managed Tables\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafd9a2a-eca0-430e-8cd9-edfa1595df17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“š Best Practices & Performance Tips\n",
    "\n",
    "## ðŸš€ Performance Optimization\n",
    "\n",
    "### 1. **Use Partitioning for Large Tables**\n",
    "```python\n",
    "# Partition by date for time-series data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"claim_date\") \\\n",
    "    .saveAsTable(\"payer_gold.claims_partitioned\")\n",
    "```\n",
    "\n",
    "### 2. **Enable Z-Ordering for Common Filters**\n",
    "```sql\n",
    "OPTIMIZE payer_gold.claims_enriched\n",
    "ZORDER BY (member_id, claim_date);\n",
    "```\n",
    "\n",
    "### 3. **Use Caching for Frequently Accessed DataFrames**\n",
    "```python\n",
    "claims_df = spark.table(\"payer_silver.claims\").cache()\n",
    "# Now use claims_df multiple times without re-reading\n",
    "```\n",
    "\n",
    "### 4. **Broadcast Small Tables in Joins**\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”’ Data Quality Best Practices\n",
    "\n",
    "### 1. **Always Validate Data**\n",
    "```python\n",
    "# Add constraints\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE payer_silver.claims \n",
    "    ADD CONSTRAINT valid_charge CHECK (total_charge > 0)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 2. **Use Schema Evolution Carefully**\n",
    "```python\n",
    "# Explicitly define schema for production\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"claim_id\", StringType(), False),\n",
    "    StructField(\"total_charge\", DoubleType(), True),\n",
    "    # ... more fields\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. **Implement Data Quality Checks**\n",
    "```python\n",
    "def validate_claims(df):\n",
    "    \"\"\"Run data quality checks\"\"\"\n",
    "    checks = {\n",
    "        \"null_claim_ids\": df.filter(col(\"claim_id\").isNull()).count(),\n",
    "        \"negative_charges\": df.filter(col(\"total_charge\") < 0).count(),\n",
    "        \"future_dates\": df.filter(col(\"claim_date\") > current_date()).count()\n",
    "    }\n",
    "    return checks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¾ Delta Lake Best Practices\n",
    "\n",
    "### 1. **Regular Maintenance**\n",
    "```sql\n",
    "-- Compact small files\n",
    "OPTIMIZE payer_gold.claims_enriched;\n",
    "\n",
    "-- Remove old versions (keep 7 days)\n",
    "VACUUM payer_gold.claims_enriched RETAIN 168 HOURS;\n",
    "\n",
    "-- Update statistics\n",
    "ANALYZE TABLE payer_gold.claims_enriched COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "### 2. **Use Time Travel for Auditing**\n",
    "```sql\n",
    "-- Query previous version\n",
    "SELECT * FROM payer_gold.claims_enriched VERSION AS OF 1;\n",
    "\n",
    "-- Query as of timestamp\n",
    "SELECT * FROM payer_gold.claims_enriched TIMESTAMP AS OF '2025-01-01';\n",
    "```\n",
    "\n",
    "### 3. **Enable Change Data Feed**\n",
    "```sql\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Architecture Best Practices\n",
    "\n",
    "### 1. **Medallion Layer Guidelines**\n",
    "- **Bronze**: Keep all source data, minimal transformation\n",
    "- **Silver**: One source system = one silver table (usually)\n",
    "- **Gold**: Many silver tables â†’ one gold table (join/aggregate)\n",
    "\n",
    "### 2. **Naming Conventions**\n",
    "```\n",
    "Catalog: <organization>_<environment>\n",
    "Schema: <domain>_<layer>\n",
    "Table: <entity>_<descriptor>\n",
    "\n",
    "Examples:\n",
    "- acme_prod.payer_bronze.claims_raw\n",
    "- acme_dev.payer_silver.claims_cleaned\n",
    "- acme_prod.payer_gold.member_360_view\n",
    "```\n",
    "\n",
    "### 3. **Documentation**\n",
    "```sql\n",
    "-- Add table comments\n",
    "COMMENT ON TABLE payer_gold.claims_enriched IS \n",
    "'Enriched claims with member and provider details for analytics';\n",
    "\n",
    "-- Add column comments\n",
    "ALTER TABLE payer_gold.claims_enriched \n",
    "ALTER COLUMN total_charge COMMENT 'Total charged amount in USD';\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a98a2-db41-4a69-b0ec-9876ee60ccfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸŽ“ Workshop Summary & Next Steps\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the Databricks Healthcare Payer Analytics Workshop! Let's review what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What You Accomplished\n",
    "\n",
    "### 1. **Medallion Architecture**\n",
    "- âœ… Built a complete **Bronze â†’ Silver â†’ Gold** pipeline\n",
    "- âœ… Understood data quality improvement at each layer\n",
    "- âœ… Created analytics-ready datasets\n",
    "\n",
    "### 2. **Data Engineering Skills**\n",
    "- âœ… Loaded data using **COPY INTO**\n",
    "- âœ… Transformed data with **SQL and PySpark**\n",
    "- âœ… Applied data quality checks and validations\n",
    "- âœ… Created aggregations and derived metrics\n",
    "\n",
    "### 3. **Analytics & Visualization**\n",
    "- âœ… Generated business insights from data\n",
    "- âœ… Created interactive visualizations\n",
    "- âœ… Performed statistical analysis\n",
    "- âœ… Built executive dashboards\n",
    "\n",
    "### 4. **Databricks Platform**\n",
    "- âœ… Worked with **Unity Catalog**\n",
    "- âœ… Used **Delta Lake** for reliable data storage\n",
    "- âœ… Leveraged **AI Assistant** for code help\n",
    "- âœ… Applied performance optimization techniques\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. â­ **Bookmark this notebook** for future reference\n",
    "2. ðŸ“– Complete the hands-on exercises\n",
    "3. ðŸ”„ Try modifying the code with your own logic\n",
    "4. ðŸ’¾ Export your results and share with your team\n",
    "\n",
    "### Continue Learning\n",
    "\n",
    "#### ðŸ“š Advanced Topics to Explore\n",
    "- **Delta Live Tables (DLT)**: Declarative pipeline framework\n",
    "- **Databricks Workflows**: Orchestration and scheduling\n",
    "- **Unity Catalog**: Advanced governance features\n",
    "- **Databricks SQL**: Performance tuning and optimization\n",
    "- **Machine Learning**: MLflow and Feature Store\n",
    "- **Streaming**: Structured Streaming with Delta Lake\n",
    "\n",
    "#### ðŸ”— Helpful Resources\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training)\n",
    "- [Community Forums](https://community.databricks.com/)\n",
    "- [Databricks Blog](https://www.databricks.com/blog)\n",
    "\n",
    "#### ðŸŽ¯ Recommended Certifications\n",
    "- **Databricks Lakehouse Platform Fundamentals**\n",
    "- **Databricks Certified Data Engineer Associate**\n",
    "- **Databricks Certified Data Analyst Associate**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Real-World Applications\n",
    "\n",
    "Apply these skills to:\n",
    "- ðŸ¥ **Healthcare**: Claims processing, patient analytics, risk scoring\n",
    "- ðŸ¦ **Finance**: Fraud detection, transaction analysis, risk management\n",
    "- ðŸ›’ **Retail**: Customer analytics, inventory optimization, sales forecasting\n",
    "- ðŸ“± **Technology**: User behavior analysis, product metrics, A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤ Get Help & Share\n",
    "\n",
    "### Need Help?\n",
    "- ðŸ’¬ Ask the **Databricks AI Assistant**\n",
    "- ðŸŒ Visit [Databricks Community](https://community.databricks.com/)\n",
    "- ðŸ“§ Contact your Databricks account team\n",
    "- ðŸ“– Check [Stack Overflow](https://stackoverflow.com/questions/tagged/databricks)\n",
    "\n",
    "### Share Your Success\n",
    "- â­ Share insights with your colleagues\n",
    "- ðŸ“Š Create dashboards for stakeholders\n",
    "- ðŸŽ¤ Present your work at team meetings\n",
    "- ðŸ† Contribute to the Databricks community\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Feedback\n",
    "\n",
    "We'd love to hear your thoughts on this workshop!\n",
    "\n",
    "**What worked well?** What could be improved? **What topics do you want to learn next?**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ™ Thank You!\n",
    "\n",
    "Thank you for participating in this workshop. We hope you found it valuable and are excited to continue your Databricks journey! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: October 7, 2025*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f241a39f-e379-43c2-bf07-a69604fe8fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“– Quick Reference Guide\n",
    "\n",
    "## Common PySpark Operations\n",
    "\n",
    "### Reading Data\n",
    "```python\n",
    "# From Delta table\n",
    "df = spark.table(\"catalog.schema.table\")\n",
    "\n",
    "# From CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/file.csv\")\n",
    "\n",
    "# From JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# From Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```\n",
    "\n",
    "### Writing Data\n",
    "```python\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"table_name\")\n",
    "\n",
    "# Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"table_name\")\n",
    "\n",
    "# With partitioning\n",
    "df.write.format(\"delta\").partitionBy(\"date_col\").saveAsTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Select columns\n",
    "df.select(\"col1\", \"col2\")\n",
    "\n",
    "# Filter rows\n",
    "df.filter(col(\"amount\") > 100)\n",
    "df.where(\"amount > 100\")\n",
    "\n",
    "# Add new column\n",
    "df.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# Rename column\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "\n",
    "# Drop column\n",
    "df.drop(\"col_name\")\n",
    "\n",
    "# Group by and aggregate\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"total\"),\n",
    "    avg(\"amount\").alias(\"average\")\n",
    ")\n",
    "\n",
    "# Join tables\n",
    "df1.join(df2, \"key_column\")\n",
    "df1.join(df2, df1.key == df2.key, \"left\")\n",
    "\n",
    "# Sort\n",
    "df.orderBy(\"col_name\")\n",
    "df.orderBy(col(\"col_name\").desc())\n",
    "\n",
    "# Remove duplicates\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "### Common Functions\n",
    "```python\n",
    "# String functions\n",
    "upper(\"col_name\")\n",
    "lower(\"col_name\")\n",
    "trim(\"col_name\")\n",
    "concat(\"col1\", \"col2\")\n",
    "substring(\"col_name\", 1, 5)\n",
    "\n",
    "# Date functions\n",
    "current_date()\n",
    "current_timestamp()\n",
    "date_format(\"date_col\", \"yyyy-MM-dd\")\n",
    "year(\"date_col\")\n",
    "month(\"date_col\")\n",
    "datediff(\"date1\", \"date2\")\n",
    "\n",
    "# Math functions\n",
    "round(\"col_name\", 2)\n",
    "abs(\"col_name\")\n",
    "ceil(\"col_name\")\n",
    "floor(\"col_name\")\n",
    "\n",
    "# Conditional logic\n",
    "when(col(\"amount\") > 100, \"High\").otherwise(\"Low\")\n",
    "\n",
    "# Null handling\n",
    "col(\"col_name\").isNull()\n",
    "col(\"col_name\").isNotNull()\n",
    "coalesce(\"col1\", \"col2\", lit(0))\n",
    "```\n",
    "\n",
    "## Common SQL Operations\n",
    "\n",
    "### DDL Commands\n",
    "```sql\n",
    "-- Create database\n",
    "CREATE DATABASE IF NOT EXISTS database_name;\n",
    "\n",
    "-- Drop database\n",
    "DROP DATABASE IF EXISTS database_name CASCADE;\n",
    "\n",
    "-- Create table\n",
    "CREATE TABLE table_name (\n",
    "    id STRING,\n",
    "    amount DOUBLE,\n",
    "    date DATE\n",
    ");\n",
    "\n",
    "-- Drop table\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "-- Describe table\n",
    "DESCRIBE EXTENDED table_name;\n",
    "SHOW COLUMNS FROM table_name;\n",
    "```\n",
    "\n",
    "### DML Commands\n",
    "```sql\n",
    "-- Insert data\n",
    "INSERT INTO table_name VALUES (1, 'value1', 100);\n",
    "\n",
    "-- Update data (Delta Lake)\n",
    "UPDATE table_name SET amount = 200 WHERE id = 1;\n",
    "\n",
    "-- Delete data (Delta Lake)\n",
    "DELETE FROM table_name WHERE id = 1;\n",
    "\n",
    "-- Merge (Upsert)\n",
    "MERGE INTO target_table\n",
    "USING source_table\n",
    "ON target_table.id = source_table.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "### Query Commands\n",
    "```sql\n",
    "-- Basic SELECT\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "\n",
    "-- With WHERE clause\n",
    "SELECT * FROM table_name WHERE amount > 100;\n",
    "\n",
    "-- Aggregations\n",
    "SELECT category, COUNT(*), SUM(amount), AVG(amount)\n",
    "FROM table_name\n",
    "GROUP BY category;\n",
    "\n",
    "-- Joins\n",
    "SELECT a.*, b.name\n",
    "FROM table_a a\n",
    "INNER JOIN table_b b ON a.id = b.id;\n",
    "\n",
    "-- Window functions\n",
    "SELECT \n",
    "    *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY category ORDER BY amount DESC) as rank\n",
    "FROM table_name;\n",
    "\n",
    "-- CTE (Common Table Expression)\n",
    "WITH summary AS (\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM table_name\n",
    "    GROUP BY category\n",
    ")\n",
    "SELECT * FROM summary WHERE total > 1000;\n",
    "```\n",
    "\n",
    "## Databricks Utilities\n",
    "```python\n",
    "# File system operations\n",
    "dbutils.fs.ls(\"path/\")\n",
    "dbutils.fs.cp(\"source\", \"destination\")\n",
    "dbutils.fs.rm(\"path/\", recurse=True)\n",
    "dbutils.fs.mkdirs(\"path/\")\n",
    "\n",
    "# Widgets (parameters)\n",
    "dbutils.widgets.text(\"param_name\", \"default_value\")\n",
    "param_value = dbutils.widgets.get(\"param_name\")\n",
    "\n",
    "# Notebooks\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, {\"param\": \"value\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Keep this reference handy as you build your data pipelines!*\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7199164455271059,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBSQL_Workshop_ETL and Analytics_Oct 2025",
   "widgets": {
    "bronze_db": {
     "currentValue": "payer_bronze",
     "nuid": "963c4fe4-97b6-41e6-a579-6b2238f8e54c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_bronze",
      "label": "Bronze DB",
      "name": "bronze_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "my_catalog",
     "nuid": "3f153351-0558-4599-81f8-0fe0154412b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "my_catalog",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gold_db": {
     "currentValue": "payer_gold",
     "nuid": "1b336a25-137d-4b7e-9fca-faa32b3f4aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_gold",
      "label": "Gold DB",
      "name": "gold_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "silver_db": {
     "currentValue": "payer_silver",
     "nuid": "6aba1384-5512-4e3e-ae51-27c321916f57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "payer_silver",
      "label": "Silver DB",
      "name": "silver_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
