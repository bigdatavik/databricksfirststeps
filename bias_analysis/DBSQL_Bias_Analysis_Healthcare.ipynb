{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bias Analysis in Healthcare Data\n",
        "## Actuarial Continuing Education Module\n",
        "\n",
        "This notebook demonstrates bias detection and mitigation techniques in healthcare data, specifically designed for actuarial continuing education requirements.\n",
        "\n",
        "### Learning Objectives\n",
        "- Identify statistical biases in healthcare claims data\n",
        "- Recognize cognitive biases in actuarial decision-making  \n",
        "- Understand social biases in healthcare data collection\n",
        "- Apply bias detection techniques to machine learning models\n",
        "- Implement fairness testing in healthcare analytics\n",
        "\n",
        "### Bias Categories Covered\n",
        "1. **Statistical Bias**: Survivorship bias, selection bias, data bias\n",
        "2. **Cognitive Bias**: Anchoring bias, confirmation bias\n",
        "3. **Social Bias**: Racial bias, gender bias, age bias\n",
        "4. **Modeling Bias**: Fairness metrics, disparate impact analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/Workspace/Repos/bigdatavik/databricksfirststeps/bias_analysis')\n",
        "\n",
        "from bias_detection_utils import BiasDetector, create_bias_report, visualize_bias_analysis\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"BiasAnalysis\").getOrCreate()\n",
        "\n",
        "# Initialize bias detector\n",
        "bias_detector = BiasDetector(spark)\n",
        "\n",
        "print(\"Bias Analysis Environment Initialized Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Statistical Bias Analysis\n",
        "\n",
        "Statistical bias occurs when there are systematic errors in data collection, sampling, or analysis that lead to incorrect conclusions.\n",
        "\n",
        "### 1.1 Survivorship Bias\n",
        "Survivorship bias occurs when only \"surviving\" data points are included in analysis, leading to overly optimistic conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load healthcare payer data for bias analysis\n",
        "# Try to load from Unity Catalog first, then fall back to sample data\n",
        "try:\n",
        "    # Load from your existing data structure\n",
        "    claims_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/your_catalog/your_schema/data/claims.csv\")\n",
        "    members_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/your_catalog/your_schema/data/member.csv\")\n",
        "    providers_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/your_catalog/your_schema/data/providers.csv\")\n",
        "    diagnoses_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/your_catalog/your_schema/data/diagnoses.csv\")\n",
        "    procedures_df = spark.read.option(\"header\", \"true\").csv(\"/Volumes/your_catalog/your_schema/data/procedures.csv\")\n",
        "    \n",
        "    print(\"Real healthcare payer data loaded successfully\")\n",
        "    print(f\"Claims: {claims_df.count()}, Members: {members_df.count()}, Providers: {providers_df.count()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Could not load from Unity Catalog: {e}\")\n",
        "    print(\"Creating realistic sample data for bias analysis demonstration...\")\n",
        "    \n",
        "    # Create realistic healthcare payer sample data with demographic information\n",
        "    import random\n",
        "    from datetime import datetime, timedelta\n",
        "    import uuid\n",
        "    \n",
        "    # Set seed for reproducible results\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate realistic member demographics\n",
        "    member_data = []\n",
        "    for i in range(1000):\n",
        "        member_id = f\"100{i+1:03d}\"\n",
        "        gender = random.choice(['M', 'F', 'Other'])\n",
        "        birth_year = random.randint(1940, 2005)\n",
        "        age = 2024 - birth_year\n",
        "        \n",
        "        # Create realistic demographic distribution with some bias patterns\n",
        "        if gender == 'M':\n",
        "            race_ethnicity = random.choices(['White', 'Black', 'Hispanic', 'Asian', 'Other'], \n",
        "                                          weights=[0.6, 0.15, 0.12, 0.08, 0.05])[0]\n",
        "        elif gender == 'F':\n",
        "            race_ethnicity = random.choices(['White', 'Black', 'Hispanic', 'Asian', 'Other'], \n",
        "                                          weights=[0.55, 0.18, 0.15, 0.08, 0.04])[0]\n",
        "        else:\n",
        "            race_ethnicity = random.choices(['White', 'Black', 'Hispanic', 'Asian', 'Other'], \n",
        "                                          weights=[0.5, 0.2, 0.15, 0.1, 0.05])[0]\n",
        "        \n",
        "        # Income bias - certain demographics have lower income\n",
        "        if race_ethnicity in ['Black', 'Hispanic']:\n",
        "            income_level = random.choices(['Low', 'Medium', 'High'], weights=[0.4, 0.45, 0.15])[0]\n",
        "        else:\n",
        "            income_level = random.choices(['Low', 'Medium', 'High'], weights=[0.2, 0.5, 0.3])[0]\n",
        "        \n",
        "        # Geographic bias - certain areas have different healthcare access\n",
        "        region = random.choice(['Urban', 'Suburban', 'Rural'])\n",
        "        if region == 'Rural':\n",
        "            income_level = random.choices(['Low', 'Medium', 'High'], weights=[0.5, 0.4, 0.1])[0]\n",
        "        \n",
        "        member_data.append((\n",
        "            member_id,\n",
        "            f\"Member_{i+1}\",\n",
        "            f\"LastName_{i+1}\",\n",
        "            f\"{birth_year}-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
        "            gender,\n",
        "            race_ethnicity,\n",
        "            income_level,\n",
        "            region,\n",
        "            f\"PLN{random.randint(101, 110)}\",\n",
        "            \"2020-01-01\"\n",
        "        ))\n",
        "    \n",
        "    # Generate realistic claims data with bias patterns\n",
        "    claims_data = []\n",
        "    diagnosis_codes = ['I10', 'E11', 'M79', 'F32', 'K21', 'G47', 'M25', 'R06', 'Z00', 'I25']\n",
        "    procedure_codes = ['99213', '99214', '99215', '99281', '99282', '99283', '99284', '99285', '36415', '93000']\n",
        "    \n",
        "    for i in range(2000):\n",
        "        claim_id = f\"CLM{i+1:06d}\"\n",
        "        member_id = f\"100{random.randint(1, 1000):03d}\"\n",
        "        provider_id = f\"200{random.randint(1, 50):03d}\"\n",
        "        \n",
        "        # Create date with some temporal bias\n",
        "        claim_date = (datetime(2023, 1, 1) + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        # Create cost bias based on demographics (simulate real-world patterns)\n",
        "        member_demo = next((m for m in member_data if m[0] == member_id), None)\n",
        "        base_cost = random.uniform(50, 2000)\n",
        "        \n",
        "        # Apply demographic bias to costs\n",
        "        if member_demo:\n",
        "            if member_demo[5] in ['Black', 'Hispanic']:  # Race bias\n",
        "                base_cost *= random.uniform(0.8, 1.1)  # Slight variation\n",
        "            if member_demo[6] == 'Low':  # Income bias\n",
        "                base_cost *= random.uniform(0.7, 1.0)\n",
        "            if member_demo[7] == 'Rural':  # Geographic bias\n",
        "                base_cost *= random.uniform(0.6, 0.9)\n",
        "        \n",
        "        # Add some missing data bias (survivorship bias)\n",
        "        if random.random() < 0.05:  # 5% missing data\n",
        "            total_charge = None\n",
        "        else:\n",
        "            total_charge = round(base_cost, 2)\n",
        "        \n",
        "        # Claim status bias\n",
        "        if member_demo and member_demo[6] == 'Low':\n",
        "            claim_status = random.choices(['PAID', 'DENIED', 'PENDING'], weights=[0.6, 0.3, 0.1])[0]\n",
        "        else:\n",
        "            claim_status = random.choices(['PAID', 'DENIED', 'PENDING'], weights=[0.8, 0.15, 0.05])[0]\n",
        "        \n",
        "        claims_data.append((\n",
        "            claim_id,\n",
        "            member_id,\n",
        "            provider_id,\n",
        "            claim_date,\n",
        "            total_charge,\n",
        "            claim_status,\n",
        "            random.choice(diagnosis_codes),\n",
        "            random.choice(procedure_codes)\n",
        "        ))\n",
        "    \n",
        "    # Create DataFrames\n",
        "    member_schema = StructType([\n",
        "        StructField(\"member_id\", StringType(), True),\n",
        "        StructField(\"first_name\", StringType(), True),\n",
        "        StructField(\"last_name\", StringType(), True),\n",
        "        StructField(\"birth_date\", StringType(), True),\n",
        "        StructField(\"gender\", StringType(), True),\n",
        "        StructField(\"race_ethnicity\", StringType(), True),\n",
        "        StructField(\"income_level\", StringType(), True),\n",
        "        StructField(\"region\", StringType(), True),\n",
        "        StructField(\"plan_id\", StringType(), True),\n",
        "        StructField(\"effective_date\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    claims_schema = StructType([\n",
        "        StructField(\"claim_id\", StringType(), True),\n",
        "        StructField(\"member_id\", StringType(), True),\n",
        "        StructField(\"provider_id\", StringType(), True),\n",
        "        StructField(\"claim_date\", StringType(), True),\n",
        "        StructField(\"total_charge\", DoubleType(), True),\n",
        "        StructField(\"claim_status\", StringType(), True),\n",
        "        StructField(\"diagnosis_code\", StringType(), True),\n",
        "        StructField(\"procedure_code\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    members_df = spark.createDataFrame(member_data, member_schema)\n",
        "    claims_df = spark.createDataFrame(claims_data, claims_schema)\n",
        "    \n",
        "    # Create provider data\n",
        "    provider_data = []\n",
        "    specialties = ['Family Practice', 'Internal Medicine', 'Cardiology', 'Oncology', 'Pediatrics', \n",
        "                  'Orthopedics', 'Dermatology', 'Psychiatry', 'Neurology', 'Emergency Medicine']\n",
        "    \n",
        "    for i in range(50):\n",
        "        provider_id = f\"200{i+1:03d}\"\n",
        "        specialty = random.choice(specialties)\n",
        "        # Geographic bias in provider distribution\n",
        "        region = random.choices(['Urban', 'Suburban', 'Rural'], weights=[0.6, 0.3, 0.1])[0]\n",
        "        \n",
        "        provider_data.append((\n",
        "            provider_id,\n",
        "            f\"Dr. {random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis'])}\",\n",
        "            specialty,\n",
        "            region\n",
        "        ))\n",
        "    \n",
        "    provider_schema = StructType([\n",
        "        StructField(\"provider_id\", StringType(), True),\n",
        "        StructField(\"provider_name\", StringType(), True),\n",
        "        StructField(\"specialty\", StringType(), True),\n",
        "        StructField(\"region\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    providers_df = spark.createDataFrame(provider_data, provider_schema)\n",
        "    \n",
        "    print(\"Realistic healthcare payer sample data created!\")\n",
        "    print(f\"Members: {members_df.count()}, Claims: {claims_df.count()}, Providers: {providers_df.count()}\")\n",
        "    print(\"\\nSample member data:\")\n",
        "    members_df.show(5)\n",
        "    print(\"\\nSample claims data:\")\n",
        "    claims_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join claims with member demographics for comprehensive bias analysis\n",
        "claims_with_demographics = claims_df.join(members_df, \"member_id\", \"left\")\n",
        "\n",
        "print(\"=== HEALTHCARE PAYER BIAS ANALYSIS ===\")\n",
        "print(f\"Analyzing {claims_with_demographics.count()} claims across {members_df.count()} members\")\n",
        "print(f\"Demographic distribution:\")\n",
        "members_df.groupBy(\"race_ethnicity\", \"gender\", \"income_level\", \"region\").count().orderBy(\"count\").show()\n",
        "\n",
        "# Detect statistical biases in healthcare claims\n",
        "print(\"\\n=== STATISTICAL BIAS ANALYSIS ===\")\n",
        "statistical_bias = bias_detector.detect_statistical_bias(\n",
        "    df=claims_with_demographics, \n",
        "    target_column=\"total_charge\",\n",
        "    group_columns=[\"race_ethnicity\", \"gender\", \"income_level\"]\n",
        ")\n",
        "\n",
        "# Display results\n",
        "for bias_type, results in statistical_bias.items():\n",
        "    print(f\"\\n{bias_type.upper()}:\")\n",
        "    for key, value in results.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"  {key}:\")\n",
        "            for sub_key, sub_value in value.items():\n",
        "                print(f\"    {sub_key}: {sub_value}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "# Analyze claim approval rates by demographics (realistic healthcare scenario)\n",
        "print(\"\\n=== CLAIM APPROVAL BIAS ANALYSIS ===\")\n",
        "approval_by_demo = claims_with_demographics.groupBy(\"race_ethnicity\", \"gender\", \"income_level\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Claim approval rates by demographic groups:\")\n",
        "approval_by_demo.orderBy(\"approval_rate\").show()\n",
        "\n",
        "# Calculate age from birth_date for age bias analysis\n",
        "claims_with_age = claims_with_demographics.withColumn(\n",
        "    \"age\", \n",
        "    year(current_date()) - year(to_date(col(\"birth_date\"), \"yyyy-MM-dd\"))\n",
        ")\n",
        "\n",
        "print(\"\\n=== AGE BIAS ANALYSIS ===\")\n",
        "age_bias_analysis = claims_with_age.groupBy(\n",
        "    when(col(\"age\") < 30, \"Young\")\n",
        "    .when(col(\"age\") < 50, \"Middle\")\n",
        "    .when(col(\"age\") < 70, \"Older\")\n",
        "    .otherwise(\"Senior\")\n",
        ").agg(\n",
        "    count(\"*\").alias(\"claim_count\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"claim_count\"))\n",
        "\n",
        "print(\"Claims analysis by age groups:\")\n",
        "age_bias_analysis.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cognitive Bias Analysis\n",
        "\n",
        "Cognitive biases are systematic patterns of deviation from norm or rationality in judgment, often affecting actuarial decision-making.\n",
        "\n",
        "### 2.1 Anchoring Bias\n",
        "Anchoring bias occurs when individuals rely too heavily on the first piece of information encountered when making decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect cognitive biases in healthcare decision-making\n",
        "print(\"=== COGNITIVE BIAS ANALYSIS ===\")\n",
        "print(\"Analyzing decision-making patterns in claim processing...\")\n",
        "\n",
        "# Analyze anchoring bias in claim amounts\n",
        "print(\"\\n1. ANCHORING BIAS - Claim Amount Patterns:\")\n",
        "claim_amount_analysis = claims_with_demographics.groupBy(\"diagnosis_code\").agg(\n",
        "    count(\"*\").alias(\"claim_count\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    stddev(\"total_charge\").alias(\"std_charge\"),\n",
        "    min(\"total_charge\").alias(\"min_charge\"),\n",
        "    max(\"total_charge\").alias(\"max_charge\")\n",
        ").withColumn(\"cv\", col(\"std_charge\") / col(\"avg_charge\"))\n",
        "\n",
        "print(\"Coefficient of variation by diagnosis (low CV indicates anchoring bias):\")\n",
        "claim_amount_analysis.orderBy(\"cv\").show()\n",
        "\n",
        "# Analyze confirmation bias in claim approval patterns\n",
        "print(\"\\n2. CONFIRMATION BIAS - Approval Pattern Analysis:\")\n",
        "# Look for patterns where certain demographics consistently get different treatment\n",
        "confirmation_bias = claims_with_demographics.groupBy(\"race_ethnicity\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\")\n",
        ").withColumn(\"approval_rate\", col(\"approved\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Approval rates by race/ethnicity (potential confirmation bias):\")\n",
        "confirmation_bias.orderBy(\"approval_rate\").show()\n",
        "\n",
        "# Analyze provider specialty bias\n",
        "print(\"\\n3. PROVIDER SPECIALTY BIAS:\")\n",
        "provider_bias = claims_with_demographics.join(providers_df, \"provider_id\", \"left\").groupBy(\"specialty\").agg(\n",
        "    count(\"*\").alias(\"claim_count\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved\")\n",
        ").withColumn(\"approval_rate\", col(\"approved\") / col(\"claim_count\"))\n",
        "\n",
        "print(\"Claims by provider specialty:\")\n",
        "provider_bias.orderBy(\"avg_charge\", ascending=False).show()\n",
        "\n",
        "# Geographic bias analysis\n",
        "print(\"\\n4. GEOGRAPHIC BIAS - Urban vs Rural Healthcare Access:\")\n",
        "geo_bias = claims_with_demographics.groupBy(\"region\").agg(\n",
        "    count(\"*\").alias(\"claim_count\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved\")\n",
        ").withColumn(\"approval_rate\", col(\"approved\") / col(\"claim_count\"))\n",
        "\n",
        "print(\"Healthcare access by region:\")\n",
        "geo_bias.orderBy(\"approval_rate\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Social Bias Analysis\n",
        "\n",
        "Social biases occur when data collection, analysis, or decision-making processes systematically disadvantage certain demographic groups.\n",
        "\n",
        "### 3.1 Demographic Bias Detection\n",
        "Analyze potential bias across gender, age, and other demographic factors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect social biases in healthcare payer data\n",
        "print(\"=== SOCIAL BIAS ANALYSIS ===\")\n",
        "print(\"Analyzing potential social biases in healthcare access and treatment...\")\n",
        "\n",
        "# 1. Racial/Ethnic Bias Analysis\n",
        "print(\"\\n1. RACIAL/ETHNIC BIAS ANALYSIS:\")\n",
        "racial_bias = claims_with_demographics.groupBy(\"race_ethnicity\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\"),\n",
        "    sum(when(col(\"claim_status\") == \"DENIED\", 1).otherwise(0)).alias(\"denied_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\")).withColumn(\"denial_rate\", col(\"denied_claims\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Claims analysis by race/ethnicity:\")\n",
        "racial_bias.orderBy(\"approval_rate\", ascending=False).show()\n",
        "\n",
        "# 2. Gender Bias Analysis\n",
        "print(\"\\n2. GENDER BIAS ANALYSIS:\")\n",
        "gender_bias = claims_with_demographics.groupBy(\"gender\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Claims analysis by gender:\")\n",
        "gender_bias.orderBy(\"approval_rate\", ascending=False).show()\n",
        "\n",
        "# 3. Income Bias Analysis\n",
        "print(\"\\n3. INCOME BIAS ANALYSIS:\")\n",
        "income_bias = claims_with_demographics.groupBy(\"income_level\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Claims analysis by income level:\")\n",
        "income_bias.orderBy(\"approval_rate\", ascending=False).show()\n",
        "\n",
        "# 4. Intersectional Bias Analysis (Race + Gender + Income)\n",
        "print(\"\\n4. INTERSECTIONAL BIAS ANALYSIS:\")\n",
        "intersectional_bias = claims_with_demographics.groupBy(\"race_ethnicity\", \"gender\", \"income_level\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\")).filter(col(\"total_claims\") >= 5)  # Filter for meaningful sample sizes\n",
        "\n",
        "print(\"Intersectional analysis (race + gender + income):\")\n",
        "intersectional_bias.orderBy(\"approval_rate\", ascending=False).show(20)\n",
        "\n",
        "# 5. Geographic Disparities\n",
        "print(\"\\n5. GEOGRAPHIC DISPARITIES:\")\n",
        "geo_disparities = claims_with_demographics.groupBy(\"region\", \"race_ethnicity\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\")).filter(col(\"total_claims\") >= 3)\n",
        "\n",
        "print(\"Geographic disparities by race/ethnicity:\")\n",
        "geo_disparities.orderBy(\"region\", \"approval_rate\", ascending=False).show()\n",
        "\n",
        "# 6. Provider Bias Analysis\n",
        "print(\"\\n6. PROVIDER BIAS ANALYSIS:\")\n",
        "provider_demographics = claims_with_demographics.join(providers_df, \"provider_id\", \"left\")\n",
        "provider_bias_analysis = provider_demographics.groupBy(\"specialty\", \"race_ethnicity\").agg(\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\"),\n",
        "    sum(when(col(\"claim_status\") == \"PAID\", 1).otherwise(0)).alias(\"approved_claims\")\n",
        ").withColumn(\"approval_rate\", col(\"approved_claims\") / col(\"total_claims\")).filter(col(\"total_claims\") >= 3)\n",
        "\n",
        "print(\"Provider specialty bias by race/ethnicity:\")\n",
        "provider_bias_analysis.orderBy(\"specialty\", \"approval_rate\", ascending=False).show(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Realistic Healthcare Bias Scenarios\n",
        "\n",
        "This section demonstrates common bias patterns found in real healthcare payer data and their implications for actuarial analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realistic Healthcare Bias Scenarios Analysis\n",
        "print(\"=== REALISTIC HEALTHCARE BIAS SCENARIOS ===\")\n",
        "\n",
        "# Scenario 1: Prior Authorization Bias\n",
        "print(\"\\n1. PRIOR AUTHORIZATION BIAS SCENARIO:\")\n",
        "print(\"Analyzing if certain demographics face higher prior authorization requirements...\")\n",
        "\n",
        "# Simulate prior authorization data (in real data, this would come from PA systems)\n",
        "pa_bias_scenario = claims_with_demographics.withColumn(\n",
        "    \"requires_pa\", \n",
        "    when(col(\"total_charge\") > 500, True).otherwise(False)\n",
        ").withColumn(\n",
        "    \"pa_approved\",\n",
        "    when(col(\"requires_pa\") & (col(\"race_ethnicity\").isin([\"Black\", \"Hispanic\"])), \n",
        "         random.random() < 0.6)  # Lower approval rate for certain groups\n",
        "    .when(col(\"requires_pa\"), random.random() < 0.8)  # Higher approval rate for others\n",
        "    .otherwise(True)\n",
        ")\n",
        "\n",
        "pa_analysis = pa_bias_scenario.filter(col(\"requires_pa\")).groupBy(\"race_ethnicity\", \"income_level\").agg(\n",
        "    count(\"*\").alias(\"pa_requests\"),\n",
        "    sum(when(col(\"pa_approved\"), 1).otherwise(0)).alias(\"pa_approved\")\n",
        ").withColumn(\"pa_approval_rate\", col(\"pa_approved\") / col(\"pa_requests\"))\n",
        "\n",
        "print(\"Prior Authorization approval rates by demographics:\")\n",
        "pa_analysis.orderBy(\"pa_approval_rate\").show()\n",
        "\n",
        "# Scenario 2: Network Adequacy Bias\n",
        "print(\"\\n2. NETWORK ADEQUACY BIAS SCENARIO:\")\n",
        "print(\"Analyzing if certain areas have limited provider networks...\")\n",
        "\n",
        "network_adequacy = claims_with_demographics.join(providers_df, \"provider_id\", \"left\").groupBy(\"region\", \"race_ethnicity\").agg(\n",
        "    countDistinct(\"provider_id\").alias(\"unique_providers\"),\n",
        "    count(\"*\").alias(\"total_claims\"),\n",
        "    avg(\"total_charge\").alias(\"avg_charge\")\n",
        ").withColumn(\"provider_density\", col(\"unique_providers\") / col(\"total_claims\"))\n",
        "\n",
        "print(\"Provider network density by region and demographics:\")\n",
        "network_adequacy.orderBy(\"region\", \"provider_density\").show()\n",
        "\n",
        "# Scenario 3: Diagnostic Coding Bias\n",
        "print(\"\\n3. DIAGNOSTIC CODING BIAS SCENARIO:\")\n",
        "print(\"Analyzing if certain conditions are under/over-diagnosed by demographics...\")\n",
        "\n",
        "# Simulate diagnostic bias (certain conditions more likely to be coded for certain groups)\n",
        "diagnostic_bias = claims_with_demographics.withColumn(\n",
        "    \"bias_factor\",\n",
        "    when(col(\"race_ethnicity\") == \"Black\" & col(\"diagnosis_code\").isin([\"F32\", \"G47\"]), 1.3)  # Mental health over-diagnosis\n",
        "    .when(col(\"race_ethnicity\") == \"Hispanic\" & col(\"diagnosis_code\") == \"E11\", 0.8)  # Diabetes under-diagnosis\n",
        "    .when(col(\"gender\") == \"F\" & col(\"diagnosis_code\") == \"I10\", 1.2)  # Hypertension over-diagnosis in women\n",
        "    .otherwise(1.0)\n",
        ")\n",
        "\n",
        "diagnostic_analysis = diagnostic_bias.groupBy(\"race_ethnicity\", \"gender\", \"diagnosis_code\").agg(\n",
        "    count(\"*\").alias(\"diagnosis_count\"),\n",
        "    avg(\"bias_factor\").alias(\"avg_bias_factor\")\n",
        ").filter(col(\"diagnosis_count\") >= 5)\n",
        "\n",
        "print(\"Diagnostic coding patterns by demographics:\")\n",
        "diagnostic_analysis.orderBy(\"diagnosis_code\", \"avg_bias_factor\", ascending=False).show(20)\n",
        "\n",
        "# Scenario 4: Cost Prediction Bias\n",
        "print(\"\\n4. COST PREDICTION BIAS SCENARIO:\")\n",
        "print(\"Analyzing if cost prediction models show bias...\")\n",
        "\n",
        "# Simulate cost prediction with bias\n",
        "cost_prediction_bias = claims_with_demographics.withColumn(\n",
        "    \"predicted_cost\",\n",
        "    when(col(\"race_ethnicity\").isin([\"Black\", \"Hispanic\"]), col(\"total_charge\") * 0.9)  # Under-prediction\n",
        "    .when(col(\"income_level\") == \"Low\", col(\"total_charge\") * 0.85)  # Under-prediction for low income\n",
        "    .otherwise(col(\"total_charge\") * 1.0)\n",
        ").withColumn(\"prediction_error\", col(\"total_charge\") - col(\"predicted_cost\"))\n",
        "\n",
        "prediction_bias_analysis = cost_prediction_bias.groupBy(\"race_ethnicity\", \"income_level\").agg(\n",
        "    count(\"*\").alias(\"predictions\"),\n",
        "    avg(\"prediction_error\").alias(\"avg_error\"),\n",
        "    stddev(\"prediction_error\").alias(\"error_std\")\n",
        ")\n",
        "\n",
        "print(\"Cost prediction bias by demographics:\")\n",
        "prediction_bias_analysis.orderBy(\"avg_error\").show()\n",
        "\n",
        "# Scenario 5: Quality Score Bias\n",
        "print(\"\\n5. QUALITY SCORE BIAS SCENARIO:\")\n",
        "print(\"Analyzing if quality scores show demographic bias...\")\n",
        "\n",
        "# Simulate quality scores with potential bias\n",
        "quality_score_bias = claims_with_demographics.withColumn(\n",
        "    \"quality_score\",\n",
        "    when(col(\"claim_status\") == \"PAID\", \n",
        "         when(col(\"race_ethnicity\") == \"White\", random.uniform(0.7, 1.0))\n",
        "         .otherwise(random.uniform(0.5, 0.9)))  # Lower scores for non-white groups\n",
        "    .otherwise(random.uniform(0.3, 0.7))\n",
        ")\n",
        "\n",
        "quality_analysis = quality_score_bias.groupBy(\"race_ethnicity\", \"gender\").agg(\n",
        "    count(\"*\").alias(\"claims\"),\n",
        "    avg(\"quality_score\").alias(\"avg_quality_score\"),\n",
        "    stddev(\"quality_score\").alias(\"quality_std\")\n",
        ")\n",
        "\n",
        "print(\"Quality scores by demographics:\")\n",
        "quality_analysis.orderBy(\"avg_quality_score\", ascending=False).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comprehensive Bias Report\n",
        "\n",
        "Generate a comprehensive bias analysis report combining all detected biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive bias analysis summary\n",
        "print(\"=== COMPREHENSIVE HEALTHCARE BIAS ANALYSIS SUMMARY ===\")\n",
        "\n",
        "# Calculate key bias metrics\n",
        "total_claims = claims_with_demographics.count()\n",
        "total_members = members_df.count()\n",
        "\n",
        "# Overall approval rate\n",
        "overall_approval = claims_with_demographics.filter(col(\"claim_status\") == \"PAID\").count() / total_claims\n",
        "\n",
        "# Demographic distribution\n",
        "demo_dist = members_df.groupBy(\"race_ethnicity\").count().orderBy(\"count\", ascending=False)\n",
        "gender_dist = members_df.groupBy(\"gender\").count().orderBy(\"count\", ascending=False)\n",
        "income_dist = members_df.groupBy(\"income_level\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "print(f\"\\nDATASET OVERVIEW:\")\n",
        "print(f\"Total Claims: {total_claims:,}\")\n",
        "print(f\"Total Members: {total_members:,}\")\n",
        "print(f\"Overall Approval Rate: {overall_approval:.2%}\")\n",
        "\n",
        "print(f\"\\nDEMOGRAPHIC DISTRIBUTION:\")\n",
        "print(\"By Race/Ethnicity:\")\n",
        "demo_dist.show()\n",
        "print(\"By Gender:\")\n",
        "gender_dist.show()\n",
        "print(\"By Income Level:\")\n",
        "income_dist.show()\n",
        "\n",
        "# Key bias findings summary\n",
        "print(f\"\\nKEY BIAS FINDINGS:\")\n",
        "print(\"1. Statistical Bias:\")\n",
        "print(f\"   - Missing data rate: {claims_with_demographics.filter(col('total_charge').isNull()).count() / total_claims:.2%}\")\n",
        "print(\"   - Data quality issues detected in claim processing\")\n",
        "\n",
        "print(\"\\n2. Social Bias:\")\n",
        "print(\"   - Approval rates vary by demographic groups\")\n",
        "print(\"   - Geographic disparities in healthcare access\")\n",
        "print(\"   - Intersectional bias patterns identified\")\n",
        "\n",
        "print(\"\\n3. Cognitive Bias:\")\n",
        "print(\"   - Anchoring bias in claim amount patterns\")\n",
        "print(\"   - Confirmation bias in approval processes\")\n",
        "print(\"   - Provider specialty bias detected\")\n",
        "\n",
        "print(\"\\n4. Realistic Healthcare Scenarios:\")\n",
        "print(\"   - Prior authorization bias patterns\")\n",
        "print(\"   - Network adequacy disparities\")\n",
        "print(\"   - Diagnostic coding bias\")\n",
        "print(\"   - Cost prediction bias\")\n",
        "print(\"   - Quality score bias\")\n",
        "\n",
        "# Generate actionable recommendations\n",
        "print(f\"\\nACTIONABLE RECOMMENDATIONS:\")\n",
        "print(\"1. Implement bias monitoring dashboards\")\n",
        "print(\"2. Regular bias audits of claim processing algorithms\")\n",
        "print(\"3. Diversity and inclusion training for actuarial teams\")\n",
        "print(\"4. Bias mitigation strategies in ML models\")\n",
        "print(\"5. Regular review of approval rate disparities\")\n",
        "\n",
        "# Save detailed report\n",
        "from datetime import datetime\n",
        "bias_summary = f\"\"\"\n",
        "# Healthcare Payer Bias Analysis Report\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Executive Summary\n",
        "- Total Claims Analyzed: {total_claims:,}\n",
        "- Total Members: {total_members:,}\n",
        "- Overall Approval Rate: {overall_approval:.2%}\n",
        "\n",
        "## Key Findings\n",
        "1. Statistical bias detected in data quality and missing data patterns\n",
        "2. Social bias identified across racial, gender, and income demographics\n",
        "3. Cognitive bias found in decision-making processes\n",
        "4. Realistic healthcare scenarios show multiple bias patterns\n",
        "\n",
        "## Recommendations\n",
        "1. Implement continuous bias monitoring\n",
        "2. Regular bias audits and mitigation\n",
        "3. Diversity training for actuarial teams\n",
        "4. Bias-aware model development\n",
        "5. Regular review of demographic disparities\n",
        "\n",
        "## Actuarial Continuing Education\n",
        "This analysis qualifies for actuarial continuing education credit under:\n",
        "- Statistical bias identification and mitigation\n",
        "- Social bias analysis in healthcare data\n",
        "- Cognitive bias recognition in actuarial work\n",
        "- Modeling bias detection and fairness testing\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/tmp/healthcare_bias_analysis_report.md\", \"w\") as f:\n",
        "    f.write(bias_summary)\n",
        "print(f\"\\nDetailed report saved to /tmp/healthcare_bias_analysis_report.md\")\n",
        "print(\"This report can be used for actuarial continuing education documentation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Actuarial Continuing Education Summary\n",
        "\n",
        "This notebook demonstrates bias detection techniques that qualify for actuarial continuing education credit according to USQS and Humana guidance.\n",
        "\n",
        "### Key Learning Outcomes\n",
        "- ✅ Identified statistical biases in healthcare claims data\n",
        "- ✅ Recognized cognitive biases in actuarial decision-making\n",
        "- ✅ Understood social biases in healthcare data collection\n",
        "- ✅ Applied bias detection techniques to healthcare analytics\n",
        "- ✅ Implemented fairness testing methodologies\n",
        "\n",
        "### Next Steps\n",
        "1. Apply these techniques to your own healthcare datasets\n",
        "2. Integrate bias detection into your ETL pipelines\n",
        "3. Consider bias mitigation strategies in model development\n",
        "4. Document bias analysis for actuarial continuing education records\n",
        "\n",
        "### Resources\n",
        "- See `documentation/actuarial_ce_guidance.md` for detailed CE requirements\n",
        "- Professional webinars mentioned in Humana guidance\n",
        "- Additional bias detection tools and methodologies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
